---
layout:		post
title:  	BERT 压缩：蒸馏、量化和修剪
subtitle:   更快、更轻、更便宜
date:       2019-12-07
author:     一轩明月
header-img: img/post-bg-universe.jpg
catalog: 	 true
tags:
    - NLP
    - BERT
---

> 博客内含有数学公式，如果使用 Chrome 浏览器观看的话，可以添加这个[插件](https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima)

近两年诸多自然语言处理任务都遵循着基本套路——用迁移学习方法搞一个大规模预训练模型。虽然这些模型取得的效果很好，但因为都是学 [Transformer](https://libertydream.github.io/2019/11/02/图解-Transformer/) 架构，其基本特点是数据越多序列标识间内部特征掌握的越好，所以都是数据贪婪的。

所以预训练模型们都在变得越来越“臃肿”，喂养的数据集规模也越来越夸张。比如[英伟达](https://venturebeat.com/2019/08/13/nvidia-trains-worlds-largest-transformer-based-language-model/)最新的模型有 83 亿个参数，比 BERT-large 大 24 倍，是 GPT-2 的 5 倍。Facebook AI 提出的 [RoBERTa](https://arxiv.org/abs/1907.11692)，更是需要 160GB 的文本作为“口粮”。正如在早先[文章](https://libertydream.github.io/2019/08/31/Transformer-对-NLP-排行榜的破坏性/)中吐糟的，人们已经不知道这些模型到底是在秀算法模型，还是在秀家底。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-07_transformer_size_trend.png)

这种“风气“构成了新挑战——**怎么把这些模型应用于生产环境？低延迟环境下的应用可能？**以及，有没有必要为了同等规格的训练而花费大价钱弄一套 GPU 服务器？毕竟对于许多研究员和开发者来说，这真的是”贫穷限制了我的想象“。

同时，为了尊重隐私，对边缘机器学习系统的需求涨幅超过了云端调用（需要上传隐私数据到服务器）。想要在你的智能手机上跑这些模型同样需要**轻量、节能和高速响应**。

除此之外，对环境资源的损耗是随着这些模型计算资源需求的增加而指数级扩张的。

为了解决上述问题，人们目前形成了若干种解决方案：

1. **量化(quantization )**：牺牲一定的准确度，取计算网络权重的近似值
2. **修剪(pruning )**：删掉部分网络连接、神经元，取精去粗
3. **蒸馏(distillation)**：采用”师生模型“的一种压缩方法，大模型是老师，小模型是学生

本文先介绍蒸馏及其一种实现，DistilBERT，而后简单介绍量化方法和修剪法。

# 知识蒸馏-概括能力的迁移

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-07_Knowledge-distillation-2.jpg)

- **知识蒸馏**

_知识蒸馏（Knowledge distillation）_有时也叫_师生学习（teacher-student learning）_，是一种通过训练小模型复现大模型（或集成模型）行为实现压缩效果的压缩技术。

监督学习中，分类模型一般是通过最大化样本的概率估计值进行样本分类的。通行做法是最小化模型的预测分布与训练集经验分布间的交叉熵损失。训练效果好的模型体现为最终的概率分布中，正确类别的概率很高而其它的几乎为 0 。但这些”几乎为 0“的概率中，筷子里面拔旗杆，有一些仍是会比其它的大，这部分反映了模型的概括归纳能力并一定程度上预示了其在测试集上的表现效果。

比如，一句话里的”办公椅“可能会被误标为”扶手椅“，但通常不会被误标为”蘑菇“。这种不确定性有时被叫做**暗知识**。而另一种理解蒸馏的方式是，它防止了模型对于预测值过于自信。

下图是 BERT 在某电影台词预测任务中得到的前 20 个结果，可以看到模型识别出了高概率的两个标识（`day`和`life`），后面跟着一系列可行标识。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-07_top_20_guesses_of_movie.png)

- **损失定义**

师生学习，就是**训练学生网络去拟合教师网络的输出分布**，即尝试概括老师掌握的知识。

相较于用硬指标（金标的独热编码）求交叉熵，知识的传递是通过对软指标（老师的概率分布）求交叉熵完成的，于是损失函数变为

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-07_loss_teacher_student.png)

t 是老师的概率估计，s 是学生的。因为充分利用了教师网络的分布信息，此时的损失函数指示作用更好。扩展到各个特征当中，引入软最大烈度（softmax-temperature）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-07_softmax_temperature.png)

> 每次迭代时，软最大（softmax）函数会将候选值基于其指数值进行归一化，保证网络输出结果位于 0-1 之间。

T 是 `temperature` 参数。烈度（temperature），是 LSTM 类（和一般神经网络）模型的超参数，在 softmax 激活前对分布进行缩放 $$\frac{logists}{T}$$，以控制预测随机性。当烈度为 1，就是普通的逻辑斯蒂分布，不对之前的计算结果缩放。可见烈度越小，模型越自信（要激活的输入越少），趋近 0 时等价为独热向量。烈度越大，模型越”激动“（所有取值都变得可能），趋近 $$+\infty$$ 时变成均匀分布。

训练时，教师和学生都使用相同的烈度参数从而更深一步的学习样本具有的信息。期间 T 设置为 1 使用标准 Softmax 激活。

以压缩 BERT 为例，要蒸馏压缩模型，使用 [KL 损失](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)。优化问题等价为

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-07_KL_loss.png)

计算学生的梯度（q）时保持不变，这样就能借助 PyTorch 加速计算了

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Optimizer

KD_loss = nn.KLDivLoss(reduction='batchmean')

def kd_step(teacher: nn.Module,
            student: nn.Module,
            temperature: float,
            inputs: torch.tensor,
            optimizer: Optimizer):
    teacher.eval()
    student.train()
    
    with torch.no_grad():
        logits_t = teacher(inputs=inputs)
    logits_s = student(inputs=inputs)
    
    loss = KD_loss(input=F.log_softmax(logits_s/temperature, dim=-1),
                   target=F.softmax(logits_t/temperature, dim=-1))
    
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

这样跟着老师的步伐可以训练出一个以更小的语言模型，即 **DistilBERT**。教师可以选择英文 `bert-base-uncased` 版 BERT。

最终训练目标实际上是三类损失的线性组合。一是蒸馏损失，二是训练损失，这里是遮罩语言模型损失（BERT 的损失函数）。而最近[研究](https://arxiv.org/abs/1910.01108)发现，加上余弦嵌入损失有助于学生和老师二者隐态向量的方向对齐。

# DistilBERT 架构

- 学生架构

身为学生的 DistilBERT 和一般的 BERT 架构基本一致，但是拿掉了标识类型嵌入（token-type embeddings）和 pooler（用于语句分类任务），这样层数就削减了一半。

至于为什么不给隐藏层瘦身，主要是因为 Transformer 架构中的多数操作（线性层和层归一化）在当下线性代数计算库中都是高度优化过的，而[研究](https://arxiv.org/abs/1910.01108)也表示隐态维数对计算效率的影响是常数级的，不是很重要。而层数对训练时长的影响却是决定性的，更小不一定意味着更快。

- 学生初始化

训练子网络不只是架构的事，还得为了子网络的收敛找到合适的初始化方法。DistilBERT 的做法是直接从老师 BERT 的两层中抽一层出来，取学生和老师共有的隐态维度对 DistilBERT 初始化

- 蒸馏

[RoBERTa](https://arxiv.org/abs/1907.11692) 指出 BERT 的训练方式对最终表现至关重要。于是 DistilBERT 据此借助梯度累计进行大批量的蒸馏（每一批 4000 个样本），同时采用了动态遮罩，删掉了句子预测部分。

- 数据和算力

DistilBERT 在和 BERT 一样的语料集上进行训练（英文维基百科和多伦多图书语料库），8 块 16GB V100 GPU 跑了接近 90 个小时。

- 效果

[研究](https://arxiv.org/abs/1910.01108)表示基准测试中，规模缩小了 40% 的 DistilBERT 能保留 97% 的 BERT 性能，下游任务中损失掉的精度也很小，而训练速度提升了 60%。

# 量化

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-07_Quantization-1.jpg)

量化意味着降低模型权重的精度。一个方法是 k 均值量化：给定模型权重矩阵 W，权重值为浮点数。将权重值分为 N 组。然后将 W 转换成 [1...N] 的整型矩阵，每个元素指代 N 个聚类中心之一。这样，我们就把矩阵元素从 32 位浮点数压缩成 log(N) 位的整形值。计算机架构一般只会允许降到 8 位或是 1 位。但后者十分罕见，毕竟二值化一个矩阵意味着只能还有两种不同的值，这对模型的损伤是巨大的。

还要注意的一点是 k 均值量化并不会降低内存消耗或是带来加速效果，这是为什么呢？

训练时，在被使用前，每个权重矩阵都会经过重构，比如说重新用 32 位浮点数填充（聚类中心）。因此，这也被称为**伪量化**。**真量化**会永久地使用更少的位数编码。以 [TensorFlow](https://www.tensorflow.org/lite/performance/quantization_spec) 的一个实现为例。通过“聚拢缩放”变换，它将 32 位的浮点矩阵转换成 8 位整型矩阵。缩放比例和偏移量因权重矩阵而异。
$$
W_8 = \frac{W_{32}}{scale}+shift
$$
这样 8 位的 W 就能用于乘法计算了，但是后面要将结果反向“聚拢缩放”进行矫正。要进一步加速可以使用**全量化**，此时不仅是权重矩阵，所有数学指标都会被量化（尤其是激活函数）。两种办法都能将内存占用降低 4 倍以上。

但无论哪种加速，最终效果如何都取决于你的计算机做整数矩阵乘法的速度。此时机器学习的研究就撞上了算力现实

除了上面说到的训练后再量化的方法，还有一种训练中就量化的——[量化感知训练法](https://arxiv.org/abs/1712.05877)。此时的训练流程是：

1. 量化权重
2. 计算这部分量化过的网络的损失
3. 计算未量化权重的损失梯度
4. 更新未量化权重值

训练完成后，模型权重也就跟着量化好了，可以拿着量化后的矩阵进行样本估计了。

# 修剪

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-07_Pruning-1.png)

- 删连接

修剪通过删除模型的一部分来使其更小更快。一个流行做法是_权重修剪_，即删掉个别的连接权重。这项技术有时会和人类大脑早期发展情况作比较，有些连接被加强有些消亡。最简单的_权重值修剪_会删掉那些权重接近 0 的部分，以权值大小表征连接重要度。删除连接的方法是将矩阵中对应元素值设为 0。注意该方法并不会使权重矩阵更小或是加速计算。真要节省空间，你应该使用稀疏矩阵的方式存储。而任何加速方式终归依赖于具体的稀疏矩阵乘法的实现方式，目前起码在 TensorFlow 中稀疏矩阵乘法并没有比稠密形式快

- 删神经元

相较于权重修剪，_神经元修剪_将整个神经元去除，表现为将权重矩阵中的整行/列删除。神经元激活和误差梯度（在有代表性的数据集上计算）的组合是重要度的不错表征。不像权重修剪，这里权重矩阵是物理意义上的变小了，任何计算过程都变快了。

- 删权重矩阵

最近有[研究]( https://arxiv.org/abs/1905.10650)把目光看向了矩阵本身，研究员将整个注意力端从 Transformer 类模型中剔除，同时付出了极小的精度损失。这种方法自然很诱人。但是删掉整个权重矩阵的做法忽视了单个神经元的重要性。我们认为更细粒度的修剪可以进一步压缩模型。另一方法，高压缩率并不能保证更快的加速效果——神经元修剪可能导致权重矩阵尺寸不一，这使矩阵乘法的并行计算更加困难。