---
layout:		post
title:  	一文了解生成对抗网络
subtitle:   
date:       2020-04-19
author:     一轩明月
header-img: img/post-bg-unix-linux.jpg
catalog:    true
tags:
    - CV
excerpt:    GANs 绝对是过去十年 ML 最有趣的成果之一。本文从给定分布下随机变量的生成过程说起，通过一个例子说明 GANs 试图解决的问题也正是随机变量的生成问题，并进一步探讨了生成匹配网络和生成对抗网络两种解决思路。最后，本文给出了这类问题的一般数理形式和相应的损失函数
---

> 编译自：Understanding Generative Adversarial Networks (GANs)，[Joseph Rocca](https://towardsdatascience.com/@joseph.rocca?source=post_page-----cd6e4651a29----------------------)

# 引言

Yann LeCun 称其为为“过去 10 年间最有趣的机器学习思想”。当然，有这样享有盛誉的学者为我们的谈论对象站台无疑是一条绝佳的广告。确实，生成对抗网络（Generative Adversarial Networks，GANs）自 Ian J. Goodfellow 等人 2014 年发表[论文](https://arxiv.org/abs/1406.2661)提出以来，已取得了巨大成功。

那什么是生成对抗网络呢？是什么使他们这么“有趣”呢？通过本文你会了解到，对抗式训练是一种很有启发性的想法，简洁而优美，它表明了机器学习，更具体的讲是生成模型上人们取得了观念上的切实进步（类似于反向传播，一种很简单但十分聪明的技巧，使神经网络的基本思想传遍南北且富有效能）

在具体讲述之前，先来快速了解一下 GANs 的创作目的。生成对抗网络是生成模型家族里的一员，它们能生成（接下来将会看到）新的内容。为了形象化展示“生成模型”的概念，来看一组广为人知的通过 GANs 得到的成果示例。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-04-18_GANs_ability.png)

这是原论文中的示例，借助生成对抗网络基于 MNIST 和 TFD 数据集生成的一些样本。两张图片里最右边一列是与直接相邻的生成样本最近的真实数据。这些成果证实产出数据真的是生成的而不仅仅是被网络记住的。

这种生成新内容的能力自然使 GANs 看起来十分“神奇”，至少乍眼看去是这样。在接下来的部分，我们会揭露 GANs 的魔术手段，探究模型背后的思想、数学原理以及建模过程。我们不仅会谈到生成对抗网络所依附的基础概念，还会进而从头开始逐步分析建构过程以及为什么要这样思考。



# 大纲

第一部分我们会讨论给定分布下随机变量的生成过程。在之后的第二部分我们通过一个例子证明 GANs 试图解决的问题可以被表述成随机变量的生成问题。针对这个问题，在文章第三部分探讨了基于生成网络的匹配办法，在第四部分介绍了 GANs 的解决思路。进一步地，我们会给出这类问题的一般结构及其损失函数，以此串联起所有部分。



# 随机变量生成

在这一部分我们会探讨随机变量的生成过程：我们会提到一些既有方法，并具体介绍一下反转换法，通过它可以由简单均匀随机变量得到复杂随机变量。尽管这些内容可能看上去和我们的主题，GANs，关系没有那么密切，但在下一部分我们会看到其与生成模型间存在的深刻联系

- **伪随机生成均匀随机变量**

计算机的运算本质上是确定性的，所以理论上并不存在生成真正意义上的随机数的可能（即便是“什么是真正的随机”本身都是个难以回答的问题）。但是，设计算法来生成和理想情形十分接近的随机数序列确实可能的。实际上，计算机可以使用伪随机数生成器来生成近乎服从 0-1 均匀分布的数字序列。均匀分布是十分简单的情况，而在此之上可以通过多种不同的方式构建复杂随机变量。

- **随机变量是运算或者过程的结果**

有很多种不同的用来生成复杂随机变量的技术，比如说反转换法，拒绝采样法，梅特罗波利斯－黑斯廷斯算法等。这些技术依赖于不同的数学技巧，主要是把我们要生成的随机变量表示为某种（对简单随机变量的）运算或过程的结果。

[拒绝采样法](https://en.wikipedia.org/wiki/Rejection_sampling)认为随机变量并非从复杂分布中抽样得到的，而是一个从已知简单分布中在给定条件下接受或拒绝采样的结果，不断重复这一过程直到采样值被接受。可以证明在接受条件正确的情况下，可以做到基于正确分布高效采样。

[梅特罗波利斯－黑斯廷斯算法](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)则试图找到一种马尔可夫链（Markov Chain，MC），其静态分布对应着目标随机变量服从的分布。一旦找到了马尔可夫链，可以将其延展足够长假装认为已经达到了稳定状态，然后将最后一个值取出，视作是从目标分布下抽取得到的。实际是一种马尔科夫蒙特卡洛（MCMC）方法

这里我们不会深入到拒绝采样和 MCMC 的细节当中，因为这些方法和 GANs 背后的理念关系不大（感兴趣的读者可以点击链接进一步阅读了解更多内容）。这里重点要说的是反转换法。

- **反转换法**

反转换法的思路可以简单描述为，复杂随机变量是在我们知道怎样生成的均匀随机变量之上进行函数运算得到的。本文中“复杂”就是表示“不简单”而不是数学意义上的复杂。

举个一维的例子。令 $$X$$ 是我们要采样的复杂随机变量，而 $$U$$ 是我们知道从哪采样的 0-1 均匀随机变量。又因为一个随机变量完全绑定于它的[积累分布函数](https://en.wikipedia.org/wiki/Cumulative_distribution_function)（ Cumulative Distribution Function，CDF）。随机变量的 CDF 是从变量定义域到 $$[0,1]$$ 区间的函数，一维定义为


$$
C D F_{X}(x)=\mathbb{P}(X \leqslant x) \quad \in[0,1]
$$


对均匀随机变量 $$U$$ 来说，则有


$$
C D F_{U}(u)=\mathbb{P}(U \leqslant u)=u \quad \forall u \in[0,1]
$$


简单起见，假设函数 $$CDF_X$$ 可逆，反函数表示为 $$CDF_{X}^{-1}$$ _（该方法可以很容易地使用广义反函数扩展到不可逆的情况，但那不是我们要讲的重点）_。定义


$$
Y=C D F_{X}^{-1}(U)
$$


则有


$$
C D F_{Y}(y)=\mathbb{P}(Y \leqslant y)=\mathbb{P}\left(C D F_{X}^{-1}(U) \leqslant y\right)=\mathbb{P}\left(U \leqslant C D F_{X}(y)\right)=C D F_{X}(y)
$$


可见， $$Y$$ 和 $$X$$ 有着相同的 $$CDF$$ ，定义了一样的随机变量。所以通过像上面这样定义出一个 $$Y$$（均匀随机变量的函数），我们就能设法确定一个目标分布下的随机变量。

总结一下，反转换法就是用精心设计的“转换函数”（CDF 的反函数）处理均匀随机变量，以生成服从给定分布的随机变量。“反转换法”的概念实际可以拓展为“转换法”，更一般性的，生成的随机变量是一些更简单随机变量的函数（并不一定是均匀的，转换函数也不再是 CDF 的反函数）。一般认为“转换函数”的目的在于重塑原始概率分布：转换函数会从原始分布过高的地方（相较于目标分布）取值并将其放到很低的地方去。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-04-19_inverse_CDF.jpeg)

上图展示了反转换法，蓝色部分是 0-1 均匀分布，橘色部分是标准高斯分布，灰色部分就是从均匀分布到高斯分布的映射（反 CDF）



# 生成模型

- **尝试生成十分复杂的随机变量**

假设我们想生成一幅 $$n \times n$$ 像素大小的二维黑白宠物狗图像，可以将每条数据重塑为一个  $$N=n \times n$$ 维的向量（对各列进行堆叠），这样狗的图像就能向量化表示了。但是这并不意味着所有“狗”向量都能反向变回二维形态。由此，我们可以说这个潜藏了宠物狗信息的 $$N$$ 维向量服从于 $$N$$ 维向量空间下的某个特定概率分布（空间中的某些点很可能表示的是狗，而其他点则不太可能）。同理，在 $$N$$ 维向量空间下还存在着猫、鸟等其他图像的概率分布。

所以，生成一幅小狗图像就等价于从 $$N$$ 维向量空间下的“小狗概率分布”中生成一个新向量。也就是说我们实际要解决的就是特定概率分布下的随机变量生成问题。

在此需要强调两点。一是我们讲的“小狗概率分布”是一个在广袤空间下的十分复杂的分布。其次，即使我们假定存在着这样的潜在分布（存在着像狗而不像其它对象的图像），显然也不知道怎么精确表述该分布。这两点使得要想从该分布中获取随机变量困难重重。下面我们来试着解决这两个问题。

- **以神经网络作为函数的转换法**

尝试生成新小狗图像时，首先遇到的问题就是 $$N$$ 维空间下的“小狗概率分布”太复杂了，不知道怎么直接生成这种复杂随机变量。但是，我们很清楚怎么生成 $$N$$ 个无关的均匀随机变量——使用转换法，为此我们要把 $$N$$ 维随机变量表示成对简单 $$N$$ 维随机变量经某个复杂函数处理后的结果。

这里要说一下，寻找转换函数并不是像介绍反转换法时取积累分布函数（显然我们并不知道）的逆那样直截了当。转换函数不一定能被清晰表述出来，所以我们需要从数据中学习。

对绝大多数类似情况来讲，十分复杂的函数自然暗指神经网络建模。所以这里就是要通过神经网络对转换函数建模，训练完成后如果输入 $$N$$ 维均匀随机变量，应该返回一个服从正确“小狗概率分布”的 $$N$$ 维随机变量。设计完了网络结构还得训练它，接下来的两个部分，我们会探讨训练这些生成网络的两种方式，其中就包括 GANs 的对抗训练思想。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-04-19_DNN_generative_model.png)

上图展示了使用神经网络的生成模型运作流程。显然我们实际讨论的维度比图中展示的高得多



# 生成匹配网络

> **声明**：“生成匹配网络”并非标准叫法。但是在文献中我们能找到像“生成矩匹配网络”或“生成特征匹配网络”等说法，这里只是想给下文提供一个更一般的叫法

- **训练生成模型**

目前为止，我们知道小狗的图像生成问题可以重新定义为 $$N$$ 维向量空间中“小狗概率分布”下的随机向量生成问题。同时提出使用转换法，用神经网络作为转换函数。

现在，我们要训练（优化）网络来正确表达转换函数。为此有两种不同的方法可选：一种直接的，一种间接的。直接训练就是比较真实和生成概率分布，并通过网络反向传播误差，这是生成匹配网络（ Generative Matching Networks，GMNs）的指导思想。而对间接训练法来说，不直接比较真实和生成的概率分布，相反地，通过对两个分布指定一个下游任务来训练生成网络，针对下游任务的生成网络优化过程会逼着生成分布靠近真实分布，而这正是生成对抗网络 GANs 背后的思想。我们先讲直接法和 GMNs，在下一部分介绍 GANs。

- **比较样本上的两个概率分布**

如前文所述，GMNs 的出发点是通过直接比较生成分布与真实分布来训练生成网络。但是我们并不知道怎么准确表示真实的“小狗概率分布”，我们也可以说生成分布太复杂所以无法精确表示。所以，精确比较是不靠谱的，但是如果能找到用样本概率分布进行比较的方法，也就能训练网络了。确实，我们有一个真实数据样本，可以在训练时的每轮迭代中生成一个生成数据样本。

尽管理论上任意距离（或者说相似度度量方法）都能用来高效比较可用样本上的两个分布，这里还是特别提一下最大平均误差法（Maximum Mean Discrepancy ，MMD）。MMD 定义了两个概率分布间的距离，给出分布样本就能计算（估计）。更多 MMD 内容可以参看[资料1](http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf)，[文章1](http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchSmo07.pdf)或[文章2](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf)

- **匹配误差的反向传播过程**

确定样本上两种分布的比较方法后，就要决定 GMNs 下生成网络的训练过程了。给定服从均匀分布的随机变量作输入，我们希望生成结果的概率分布是“小狗概率分布”。GMNs 就是重复下列步骤来优化网络

1. 生成一些随机均匀输入
2. 输入通过网络计算得到生成结果
3. 结合样本比对真实“小狗概率分布”和生成概率分布（比如计算真实小狗图像样本和生成图像样本间的 MMD 距离）
4. 使用反向传播更新梯度，进一步缩短真实和生成分布间的距离

按上述步骤操作，也就是网络的梯度下降过程，损失函数就是真实分布与当前轮次生成分布间的距离。下图展示了 GMNs 运作过程。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-04-19_GMNs_process.png)



# 生成对抗网络

- **“间接”训练法**

上文讲的“直接”法是直接将生成分布与真实分布在训练生成网络的时候进行对比。而 GANs 的指导思想就妙在将这种直接比对换成了一种间接方式，给这两个分布选择一项下游任务。就这项任务而言，随着生成网络训练逐渐完成，生成分布也被逼着和真实分布靠得越来越近。

GANs 的下游任务是分辨真样本和生成样本，也可以说要做到“不可分辨”，因为我们希望辨析尽量失败。所以在 GANs 结构中有一个判别器，接收真实和生成数据样本并尽力区分二者。还有一个生成器，专门训练用来欺骗判别器。通过一个简单的例子来看一下为什么直接法和间接法理论上会得到相同的最佳生成器。

- **理想情况：完美生成器与判别器**

为了更好的理解为什么训练一个生成器来蒙骗判别器会得到和直接训练生成器来匹配目标分布一样的结果，不妨来看一个一维情况下的例子。先暂时忘记生成器和判别器的表示方式，只当它们是些抽象概念（下一部分会指明）。进一步地，假设二者都是“完美”（能力无限）的，不会受任何形式（参数化）模型的约束。

假定手头有一份真实分布，比如一维高斯分布，我们希望生成器从这个概率分布中进行采样。我们讲“直接”训练法会逐步调整生成器（迭代梯度下降）来矫正真实与生成分布间的测量误差。最后，假设优化过程完美，我们最终应该得到一个和真实分布精确匹配的生成分布。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-04-19_direct_matching_method.jpeg)

上图展示了直接匹配法，蓝色线是真实分布，橘色为生成分布。逐步迭代中，比较两分布并通过梯度下降来调整网络权重。这里进行的比较是针对均值和方差来说的。注意（显然）这个例子太简单了所以不需要怎么迭代，只是想形象化展示一下前文思想

对于“间接”法，必须还得考虑判别器。暂且认定这个判别器是位先知，清楚地知晓哪个是真实分布，哪个是生成分布，并且能基于此预测任一给定点的类别（“真实”或“生成”）。如果两个分布相距甚远，判别器能轻易且高度自信地对多数给定点进行分类。如果想要蒙骗判别器，就必须使生成分布靠近真实分布。当两分布的所有点都完全一致时，分类难度最高：此时每个点取得“真实”和“生成”的概率是一样的，判别器也就无法做出比等概率判别为真更好的选择了。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-04-19_adversarial_method.jpeg)

上图展示了对抗法的思路。蓝色是真实分布，橘色是生成分布。灰线对应于右侧 y 轴刻度，表示判别器将各点归向更高密度类别时的正确率（假设“真实”和“生成”数据是等比例的）。两个分布越接近，判别器犯的错就越多。训练目标就是使绿色区域（生成分布太高）向红色区域（生成分布太低）移动。

在这点上，难免质疑这种方法是否真的是个好想法。确实，它似乎要更复杂（必须基于下游任务而非基于分布来对生成器进行优化），而且还需要一个先知样的判别器，但实际上既不完美又不全知。要辩驳的话，对于第一点，直接比较两个概率分布的困难度抵消了间接法显然更高的复杂性。至于第二点，很明显是不知道判别器是啥样的，但是是可以学到的。

- **近似：对抗神经网络**

现在让我们来看一下 GANs 体系中生成器和判别器采用的具体形式。生成器是一个神经网络负责对转换函数建模，如果训练好了，输入一个简单随机变量后其必须返回一个服从目标分布的随机变量。鉴于这非常复杂且充满未知，所以要用另一个神经网络对判别器进行建模，接收一个样本点（小狗例子中就是一个 $$N$$ 维向量）并返回该点是“真实”样本的概率。

注意，事实上我们现在是在用一个参数化模型来表示生成器和判别器（而非前一部分中的理想情况），但这对上面给出的论述逻辑并无太大影响：我们只是要在参数化空间而非整个空间中进行寻找，所以理想条件下的最优点可以被视为被许多参数化模型“包围”了起来。

确认了结构，就可以带着相反的目标一同联合训练两个神经网络了。

1. 生成器的任务是蒙骗判别器，所以生成神经网络就是要最大化最终分类误差（真实和生成数据之间）
2. 判别器的任务是侦测假冒的生成数据，所以判别神经网络就是要最小化最终分类误差

故而在每轮迭代过程中，生成网络不断更新参数来增大分类误差（生成器参数误差梯度上升）而判别网络则不断更新参数降低误差（判别器参数误差梯度下降）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-04-19_GANs.png)

上图展示了生成对抗网络运作过程。生成器接收简单随机变量作输入并生成新数据。判别器接收“真实”和“生成”数据，构建分类器尝试辨别二者。生成器就是要欺骗判别器（通过尽力混淆真实数据和生成数据增大分类误差），判别器的目标就是明辨真实数据与生成数据。

相反的目标和两网络对抗训练背后的思想诠释了什么叫“对抗网络”。竞争使得两网络不断向各自目标“前进”。从游戏视角来看，可以看作是一个双人最小最大游戏，取得的均衡态意味着生成器能准确地从目标分布中生成数据，同时判别器对任意给定点给出的“真实”或“生成”概率都是 $$\frac{1}{2}$$

# GANs 的数学原理

> **声明**：下面的等式并非原作中的式子，这里出于两方面思考使用了另一种数学形式。首先是为了和前文直观感受更契合，其次原始论文里的等式足够清晰了，不用重写一遍。这里不探讨不同可能的损失函数的实践问题（梯度消融等）。本文和原作差异主要在后者使用的是交叉熵，我们使用的是绝对误差。此外，下文情景中我们假定生成器和判别器都是能力无限的

神经网络建模本质上是要定义两件事：结构和损失。我们已经介绍了生成对抗网络的体系结构，包含两部分：

1. 生成网络 $$G(.)$$ 接收概率密度为 $$p_z$$ 的随机输入 $$z$$，返回（训练后）服从目标概率分布的结果 $$x_g = G(z)$$
2. 判别网络 $$D(.)$$ 的输入 $$x$$ 可以是“真实”样本（$$x_t$$，概率密度 $$p_t$$）或者是“生成”样本（$$x_g$$，其概率密度 $$p_g$$ 受 $$p_z$$ 影响），返回 $$x$$ 是“真实”数据的概率 $$D(x)$$

接着看一下 GANs  理论损失函数。如果等比例的“真实”与“生成”数据传入判别器，判别器绝对误差可以表示为


$$
\begin{aligned}
E(G, D) &=\frac{1}{2} \mathbb{E}_{x \sim p_{t}}[1-D(x)]+\frac{1}{2} \mathbb{E}_{z \sim p_{z}}[D(G(z))] \\
&=\frac{1}{2}\left(\mathbb{E}_{x \sim p_{t}}[1-D(x)]+\mathbb{E}_{x \sim p_{g}}[D(x)]\right)
\end{aligned}
$$


生成器的目标是蒙骗判别器，后者的任务是分辨真实和生成数据。所以训练生成器的时候，我们希望最小化判别器误差的同时使生成误差最大化。可以表示为


$$
\max _{G}\left(\min _{D} E(G, D)\right)
$$


对任意给定生成器 $$G$$ （连同受诱导的概率密度 $$p_g$$），最佳判别器要能最小化下式


$$
\mathbb{E}_{x \sim p_{t}}[1-D(x)]+\mathbb{E}_{x \sim p_{g}}[D(x)]=\int_{\mathbb{R}}(1-D(x)) p_{t}(x)+D(x) p_{g}(x) d x
$$


为了最小化该间隔，可以选择对数据集中的每一个 $$x$$ 求函数最小值。这样就得到了给定生成器对应的最佳判别器


$$
\mathbb{I}_{\left(p_{t}(x) \geqslant p_{g}(x)\right)}
$$


接着检索 $$G$$ 使下式最大化


$$
\int_{\mathbb{R}}\left(1-D_{G}^{*}(x)\right) p_{t}(x)+D_{G}^{*}(x) p_{g}(x) d x=\int_{\mathbb{R}} \min \left(p_{t}(x), p_{g}(x)\right) d x
$$


同样地，为了最大化该间隔，可以对每个 $$x$$ 求函数最大值。因为密度 $$p_t$$ 独立于生成器 $$G$$ ，所以最好将 $$G$$ 设为


$$
p_{g}(x) \geqslant p_{t}(x)
$$


当然，因为 $$p_g$$ 作为概率密度总和要为 1，我们必然可以得到最佳 $$G$$ 


$$
p_{g}(x)=p_{t}(x)
$$


这样就得到了理想中能力无限的生成器和判别器，对抗场景的最优解就是生成器所得密度和真实密度一致，而判别器无法断言一个比另一个更真实。最后来看一下 $$G$$ 的最大化


$$
\frac{1}{2} \int_{\mathbb{R}} \min \left(p_{t}(x), p_{g}(x)\right) d x=\int_{\mathbb{R}} \frac{\min \left(p_{t}(x), p_{g}(x)\right)}{p_{t}(x)+p_{g}(x)} \frac{p_{t}(x)+p_{g}(x)}{2} d x
$$

从该式可以看出 $$G$$ 想使判别器犯错的期望最大化



# 干货

- 计算机大体上可以生成简单的伪随机变量（比如非常接近均匀分布的变量）
- 有多种方式生成更复杂的随机变量，其中“转换法”是将随机变量表示为更简单随机变量（们）的函数
- 机器学习中，生成模型是为了从给定（复杂）概率分布中生成数据
- 深度学习生成模型通过神经网络（非常复杂的函数）建模，传入简单随机变量返回服从目标分布的随机变量（像“转换法”）
- 生成网络可以“直接”训练（通过比较生成数据分布和真实分布），这是生成匹配网络的思路
- 生成网络还可以“间接”训练（通过蒙骗另一个同时训练的网络，其旨在从”真实“数据中分辨出”生成“数据），这是生成对抗网络的思路

即便有关 GANs 的炒作有些夸张，我们还是要说 Ian Goodfellow 和其他共同作者们提出的对抗训练确实是一个很棒的思想。这种将直接比对转换为间接的方式一定会在未来深度学习领域中大放异彩。总的来说，我们不知道 GANs 思想是不是”过去十年机器学习领域中最有趣的思想“，但它绝对，至少是，最有趣的思想之一。

