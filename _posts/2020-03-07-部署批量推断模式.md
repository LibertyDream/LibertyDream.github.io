---
layout:		post
title:  	部署批量推断模式
subtitle:   模型部署系列之三
date:       2020-03-07
author:     一轩明月
header-img: img/post-bg-os-metro.jpg
catalog: 	 true
tags:
    - opinions
---

[上一篇](https://libertydream.github.io/2020/03/01/模型部署的软件接口/)部署文章中我们设计了软件接口来降低模型上线难度。本文会探讨如何用这些接口和工作调度机制部署模型，实现批量推断。批量推断多用于周期性执行的计划方案中，每次对一批样本进行估计。

我们还会讲到什么时候合适，什么时候又不适合将模型设置成批量推断形态，如何使用 Python 和 cron 实现批量推断以及怎么用产品级工作流管理工具完成同样的任务。

## 什么时候用批量推断

本系列[第一篇文章](https://libertydream.github.io/2020/02/23/模型部署到底是在说什么/)中提到了若干终端用户与模型预测结果交互的场景。其中有一例到要搭建一个潜在顾客评分模型，一些懂技术的分析师们会使用模型结果来发掘用户。他们会使用 SQL 在关系数据库中查询数据，希望每天早上都能看到根据前日得分计算出的新潜在客户分数。

这就是一个典型的批量推断场景。其一，每次计算结果都是针对一批样本，也就是前天生成的客户评分，其二，新的计算结果每天都要生成一次。所以此处部署模型就是指一项每天都要执行的批处理任务。大体流程是检索新潜在客户得分，反序列化训练好的模型，生成新得分，再将预测结果存储到分析师们用到的数据库内。

文章中的另一个场景是电商公司给他的客户们发电子邮件推荐产品。这些营销邮件在当地时间每周一下午和每周五早上发送到用户端，每封邮件有五个推荐项。

这同样是可以使用批量推断的场景。因为推荐结果是针对所有消费者进行计算的，所有现存用户都是样本集的一员。可以一周进行两次预测计算，一次是在周一下午发送邮件之前，另一次则是周五清晨邮件送出之前。鉴于邮件都是根据当地时间来判断发送时机的，只要确保运算任务在最早的那封邮件开始编辑前能够完成即可。

要知道构建成批样本的方式多种多样。上面的例子我们是通过时区和用户群来分隔批量样本的。如果是在基于用户群构建多个协同过滤模型这会显得更有意义些。

什么时候执行批量推断也是视情况而定。除了一周两次生成预测结果，一般我们会一周跑一次批量推断模型，并将 10 个推荐结果分到两封电子邮件当中。这里做出的牺牲是对于那些十分活跃的购物者来讲推荐可能会显得过时。

每当要基于给定样本异步生成预测内容，特别是任务计划间间隔超过一小时以上时，把模型部署成批量推断模式是合适的。

## 不适合批量推断的场景

还是以前面推荐模型的场景为例，想象一个不适于批量推断的情形。比如说电商公司希望无论是网页端还是移动端的用户都能看到产品推荐展示。

产品团队想在双端不同位置加上推荐内容，推送商品基于用户近期行为，如浏览页面、搜索关键词等进行计算。这一需求无疑对我们预测生成方式添加了限制，进而影响到我们的部署过程。

首先，推荐内容要能被不同的客户端获取。这意味着我们的部署过程不能和网页或移动中的任意一个进行绑定。和其中一方捆绑在一起意味着其他客户端的用户就看不到推荐信息了。[重新训练](https://mlinproduction.com/model-retraining/)以后更新模型也会平添不少麻烦。

其次，推荐内容要用到用户最新的行为信息。这杜绝了我们采用周期性计划任务，将预测内容缓存备用的推荐方式。一种方案是调高预训练频率，比方说每小时而非每天进行一次训练，但这还是会漏掉最近一小时内活跃用户的活动信息。而且这是在假定每批推断任务都会成功的前提下，实际中绝无可能。执行过批处理任务的人都知道任务失败是常事。

最后，该推荐场景下对预测延迟的要求也很高。用户浏览应用移动和网页页面都要看到推荐的产品，所以预测结果必须在亚秒级时间范围内计算出来，不然页面加载速度过慢会损害用户体验。延迟需求通常是模型不能采取批量推断方案的首要原因。

综上，（近乎）实时推荐的场景下的模型部署不适于批量推断。

## 批量推断的实现

最简单的批量推断实现包含两个部分。一个是应用程序，另一个是编排计划和调用模型的组件。这里我们会用伪代码实现一版简单的批量推断，用 Python 脚本实现应用逻辑，用 [cron](https://en.wikipedia.org/wiki/Cron) 作工作调度器。新建 run_batch_inference.py 实现推断逻辑

```python
import argparse
import logging

logging.basicConfig(level=logging.INFO)

def run_batch_inference(remote_model_path):
    '''
    计算并存储预测结果 \
    
    参数:\
    --------
    remote_model_path:序列化模型在远端的访问路径
    '''
    logging.info('Running batch inference')
    raw_data = get_raw_inference_data()
    
    logging.info('Retrieve serialized model from {}.'.format(remote_model_path))
    model = Model.from_remote(remote_model_path)
    X = model.preprocess(raw_data)
    predictions = model.predict_batch(X)
    
    logging.info('Writing predictions to database')
    write_to_db(raw_data, predictions)
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description = 'Retrieve arguments for batch inference')
    parser.add_argument('--remote_model_path', type=str, required=True, help='Remote path to serialized model.')
    args = parser.parse_args()
    run_batch_inference(remote_model_path=args.remote_model_path)
```

这段伪代码基本包含了所有实现批量推断所需内容。

批量推断的实现被封装进 `run_batch_inference()` 方法。方法首先会去获取预测用的原始数据，通常是从数据库里拉去并按需参数化。比如，在潜在客户评分模型中该方法可能会接收一个日期，返回当日所有的预测评分。而在推荐系统的例子中，`get_raw_inference_data`可能会接收时区并按时区查询用户。

接着我们获取所部署的模型并将其加载入内存。这要怎么做呢？一种方案是写一个程序负责将调教好的模型持久化到分布式文件系统上，比如 S3。

上一篇部署系列文章中，有讲到要怎么用 `Model` 类定义接口来实现 ML 工作流任务。其中一个方法 `to_remote()`，会将训练好的模型序列化并上传至远程文件系统，像 S3 或者谷歌云。方法返回序列化模型的访问路径，将该路径传入 run_batch_inference.py 模块就能加载模型到内存了。

接下来的两行处理原始数据和生成预测结果，一样的，这里还是依靠既有接口方法来完成这两项任务。最后 `write_to_db()` 方法负责将结果写入数据库。这里`raw_data` 和 `predictions` 都被传入函数是考虑到原始数据里包含着 ID 字段这样的必要元信息。ID 既可以是我们潜在客户样本标号，也可能是推荐实例面向的若干用户 ID。

## 批量推断的计划与执行

有了执行批量推断的 Python 代码，接着就要考虑代码周期性执行的问题。对此有许多方案可选，比如使用工作调度器，Kubernetes CronJobs 等等。这里用 [cron](https://en.wikipedia.org/wiki/Cron) 可能是部署工作计划最简单的方式。Cron 是按时间规划任务的调度器，可以按固定时间间隔周期性的执行任务。

虽然用 cron 编排计划很方便，但并不支持自动重启失败任务，发送通知等功能。稍后我们会介绍一些提供这些功能的 cron 替代品。

使用 cron 首先要确定批量推断的频率与时机。然后将计划方案写成 [cron 表达式](https://crontab.guru/)，再把表达式和运行指令添加到  [crontab](https://mlinproduction.com/batch-inference-for-machine-learning-deployment-deployment-series-03/Overview) 文件里。比方说我们打算每天早上 3 点执行一次潜在客户推断任务，对应 cron 表达式为 `0 3 * * *`。所以追加到 crontab 文件内的命令行会是这样：

```
0 3 * * * python3 run_batch_inference.py
```

而要每周一、周五早上 5 点获取推荐产品，需要向 crontab 追加下面一条指令

```
 0 5 * * 1,5 python3 run_batch_inference.py
```

这样就完成了批量推断模型的部署。 run_batch_inferency.py 脚本会准时运行并输出预测结果，以此向组织贡献商业价值。

## 批量推断任务调度工具

通过 Python 脚本和 cron 我们实现了批量推断与部署，它们会周期性的被触发完成批处理任务，但还缺少很多像监控、自动重试和失败通知这样的功能。对新手小白来讲这些功能还是十分必要的。如果你曾有过执行批处理任务的经历，也会理解这些额外功能的价值所在。这里介绍几个能高效、容错地部署批量推断模型的工具。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-06_prefect_logo.png)

[Prefect](https://docs.prefect.io/core/) 是一个工作流程管理系统，能接收代码并将其转换为分布式的鲁棒管道。它有的 UI 界面很全面，比如任务、远程执行集群和自动规划等。Prefect 使添加[通知和警告](https://docs.prefect.io/core/tutorials/slack-notifications.html#installation-instructions)变得异常简单，官方文档内还有丰富的[案例样本](https://docs.prefect.io/core/examples/)。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-06_Airflow_logo.png)

[Apache Airflow](https://airflow.apache.org/docs/stable/) 一个编程式编写、规划和监控工作流程的平台。Airflow 的特色在于提供了丰富的用户接口，方便用户可视化生产环境中的管道运行情况，监控进程，并在必要时处理故障。Airflow 于 2014 年 10 月开始为 Airbnb 使用，是一个有着庞大用户基础的成熟工具。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-06_kubernetes_logo.png)

Kubernetes Cronjobs 允许用户在 Kubernetes 集群上周期性执行批处理任务。必须要说的是 Cronjobs 并不像其他工作流管理工具一样提供了 UI 界面，但可以通过附件加上这一功能。如果你所在的组织已经在用 Kubernetes 集群并有专门的工程师团队进行维护，Cronjobs 无疑是数据科学家们的首选。

这里有一篇文章介绍了如何[使用 Cronjobs 部署机器学习模型](http://mlinproduction.com/k8s-cronjobs/)。注意默认情况下 Kubernetes 不支持多步骤工作流，但如果用的是 [Argo Workflows](https://argoproj.github.io/docs/argo/examples/README.html) 就不存在这个问题。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-06_jenkins_logo.png)

传统的自动化工具，像 [Jenkins](https://jenkins.io/)，也能用于规划批量训练和批量推断任务。这些工具通常提供了错误处理，通知和自动重试等功能，但并不是专为机器学习配置的。但如果公司里已经在使用这样的工具了，从这里入手也是不错的选择。

最后值得一提的是，现如今有大量机器学习平台方案可供选择，平台会提供工作流程管理和其他 ML 核心功能。开源工具可以选择 [Kubeflow](https://www.kubeflow.org/) 和 [mlflow](https://mlflow.org/)。商业方案包括 [Amazon Sagemaker](https://aws.amazon.com/sagemaker/)，[cnvrg](https://cnvrg.io/) 以及众多[其他选项](https://github.com/alirezadir/Production-Level-Deep-Learning#46-all-in-one-solutions)。