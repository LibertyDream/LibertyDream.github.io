---
layout:		post
title:  	BERT 实战
subtitle:   句子分类小试牛刀
date:       2019-11-30
author:     一轩明月
header-img: img/post-bg-space.jpg
catalog: 	 true
tags:
    - NLP
    - BERT
---

> 文章编译自：
>
> http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/

语言处理类的机器学习模型近些年发展势头迅猛，已经离开实验室开始与工业应用结合。一个很棒的例证就是近期谷歌声称”[BERT 已经是 Google 搜索背后的主力](https://www.blog.google/products/search/search-language-understanding-bert/)“。谷歌认为这代表着“过去五年最大的飞跃，也是研究史上的新里程碑”。

本文是 BERT 实践的简明教程，选用其变种之一进行语句分类。这个小例子入门足以，同时也充分展示了个中关键概念。关注代码的话，可以和本文相配的 [notebook](https://nbviewer.jupyter.org/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb) 一起看。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-distilbert-sentence-classification.png)

# 数据集：SST2

我们选用的数据集是 [SST2](https://nlp.stanford.edu/sentiment/index.html)。这是一个电影评论的集合，评论中的每句话都打上了积极（值为 1）或消极（值为 0）标签。

| 句子                                                         | 标签 |
| :----------------------------------------------------------- | ---- |
| a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films | 1    |
| apparently reassembled from the cutting room floor of any given daytime soap | 0    |
| they presume their audience won't sit still for a sociology lesson | 0    |
| this is a visually stunning rumination on love , memory , history and the war between art and commerce | 1    |
| jonathan parker 's bartleby should have been the be all end all of the modern office anomie films | 1    |

# 模型：句子情感分类

我们的目标是构建一个模型，输入一个句子（向数据集里的那样），输出 1（表示积极情感）或是 0（表示消极情感）。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_sentiment-classifier-1.png)

模型实际分两部分：

- [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5) 负责处理句子并将其中的内部特征一并送给下一阶段的模型。DistilBERT 是 BERT 模型的简化版，更轻，更快，效果稍差。
- 二阶段模型就是 scikit-learn 里基本的逻辑回归模型，接收 DistilBERT 的处理结果完成情感二分类

两个模型间传递的数据是一个 768 维的向量，是句子的向量化表示（嵌入表示），可以用于分类。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_distilbert-bert-sentiment-classifier.png)

如果你看过我的[图解 BERT](https://libertydream.github.io/2019/11/16/NLP-迁移学习演化之路/)，这个向量就是序列中第一个位置（`[CLS]`标识）的处理结果

# 模型训练

使用的时候，只用训练逻辑回归模型。DistilBERT 选用的是在英语数据集上预训练过得，“懂得”英文。虽说这个模型没针对句子分类场景微调过，但依旧具备句子分类能力，因为当初是为了通用目的构建的。BERT 的目标有两个，一是获取更好的嵌入表示，二是分类，后者使得模型对整个句子有一种整体感知并将其封装到第一个位置的处理结果当中。[transformers](https://github.com/huggingface/transformers) 库同时提供了预训练好的模型和 DistilBERT 的实现源码。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_model-training.png)

# 教程概述

下图是本教程的整体流程计划。首先用训练好的 DistilBERT 模型获取 2000 个句子的嵌入表示（得到2000 个向量）。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-distilbert-tutorial-sentence-embedding.png)

之后就都是 scikit-learn 的活了。先照例分割下训练集和测试集

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-distilbert-train-test-split-sentence-embedding.png)

要注意，实际上 scikit-learn 中的分割方法会先对样本进行混淆（搅和在一起），然后再采样，不是简单的取数据集里的前 75% 的样本。

然后再训练集上训练我们的逻辑回归模型

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-training-logistic-regression.png)

# 预测计算

写代码和介绍模型训练方法前，先说一下训练好的模型是怎么进行分类预测的。

以对 “a visually stunning rumination on love” 分类为例。首先要用 DistilBERT 的标识器把句子中的单词分成一个个标识。然后给这组标识加上任务指示标识，表示要进行句子分类（这里就是句首加上`[CLS]`，句尾加上`[SEP]`)

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-distilbert-tokenization-1.png)

第三步标识器会把每个标识替换成嵌入表中各自对应的 id 值，嵌入表是 DistilBERT 模型自带的。如果不了解嵌入可以看一下我之前的文章，[图解 Word2Vec](https://libertydream.github.io/2019/11/09/图解-word2vec/)

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-distilbert-tokenization-2-token-ids.png)

所有这些任务只用一行代码

```python
tokenizer.encode("a visually stunning rumination on love", add_special_tokens=True)
```

现在句子转换成了合适的输入格式，可以送入 DistilBERT 处理了。如果你已经看过 [图解 BERT](https://libertydream.github.io/2019/11/16/NLP-迁移学习演化之路/)，那这一步可以画成这个样子：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-distilbert-input-tokenization.png)

# DistilBERT 处理

DistilBERT 对输入向量的处理和 [BERT](https://libertydream.github.io/2019/11/16/NLP-迁移学习演化之路/) 是一致的。每个向量由 768 个浮点数构成。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-model-input-output-1.png)

因为是句子分类任务，只用看第一个向量（与`[CLS]` 标识对应）。把这个向量作为输入传入逻辑回归模型

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-model-calssification-output-vector-cls.png)

这里开始就是逻辑回归的任务了，按照训练阶段习得的特征对这个向量进行分类

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-distilbert-sentence-classification-example.png)

训练是后面要说的，并会附上整个流程代码

# 代码

这一部分就是训练语句分类模型的代码了。也可以看 [notebook](https://nbviewer.jupyter.org/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)

首先导入相关库和方法

```python
import numpy as np
import pandas as pd
import torch
import transformers as ppb # pytorch transformers
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
```

数据集在 GitHub 上有[提供](https://github.com/clairett/pytorch-sentiment-classification/)，直接导入为 pandas 的数据表

```python
df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\t', header=None)
```

可以使用 `df.head()` 查看数据头五行，形式如下：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_sst2-df-head.png)

- 导入预训练好的 DistilBERT 模型和标识器

```python
model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')

## 用 BERT 的话就取消下行注释
#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')

# 加载预训练模型/标识器
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)
```

有了标识器就可以对数据集进行标识化了。要留意一点，上面的例子中只标识化和处理的一条语句。这里我们会将所有句子作为一批一并处理（出于资源限制，notebook 中处理的是样本的一个子集，比如说 2000 个样本）

- 标识化

```python
tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))
```

这样句子就都变成了 id 列表

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_sst2-text-to-tokenized-ids-bert-example.png)

数据集现在就是一个列表数组了，其中存放的是 pandas 的 Series 或 DataFrame。而在 DistilBERT 处理前，还得保证所有向量都是一般大，因此就要对比较短的标识填充 0 标识占位。你可以看一下 notebook，就是基本的 Python 字符串和数组操作。

填充后我们的矩阵/张量数据就能送给 BERT 处理了

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-input-tensor.png)

- 使用 DistilBERT 进行处理

现在就可以从标识矩阵中抽出张量作为输入送入 DistilBERT 了

```python
input_ids = torch.tensor(np.array(padded))

with torch.no_grad():
    last_hidden_states = model(input_ids)
```

这一步完成后，变量 `last_hidden_states` 内存储的就是 DistilBERT 的输出了。它是形如 `(样本数，序列最大标识数，DistilBERT 隐态单元数)` 的元组。本实例中，就是 `(2000, 66, 768)`

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-distilbert-output-tensor-predictions.png)

- BERT 输出解包

对这个三维输出张量解包。可以先来检查下维度

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-output-tensor.png)

每一行对应着数据集中的一句话。回顾整体流程，是这个样子

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-input-to-output-tensor-recap.png)

- 抽出主要部分

针对句子分类，我们只关心 `[CLS]` 标识的 BERT 输出结果。所以选出这一部分，其它的丢掉

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-output-tensor-selection.png)

```python
# 抽出所有序列的第一个位置的数据，取得该位置所有的隐态输出结果
features = last_hidden_states[0][:,0,:].numpy()
```

这样 `features` 就是一个二维 numpy 矩阵了，存储着数据集中所有语句的嵌入结果。

- 逻辑回归

有了 BERT 的处理结果，也就构建好了训练逻辑回归模型的数据集。一共 768 维特征，标签数据原始数据集中有

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_logistic-regression-dataset-features-labels.png)

照例完成训练集、测试集的划分

```python
labels = df[1]
train_features, test_features, train_labels, test_labels = train_test_split(features, labels)
```

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-30_bert-distilbert-train-test-split-sentence-embedding.png)

接着，在训练集上训练逻辑回归模型

```python
lr_clf = LogisticRegression()
lr_clf.fit(train_features, train_labels)
```

有了训练好的模型，就能在测试集上打个分，看看模型表现如何了

```python
lr_clf.score(test_features, test_labels)
```

这里我们的分类准确率达到了 81%

# 基准得分

仅作参考，该数据集上目前最高准确度是 **96.8**。DistilBERT 如果经过微调，调整 BERT 权重值使其更适合于语句分类（我们称为_下游任务_），得分可以更高一些。微调后的 DistilBERT 最终得分是 **90.7**，完全体 BERT 得分是 **94.9**。

接下来，你可以回顾一下文章并动手尝试一下[微调](https://huggingface.co/transformers/examples.html#glue)，或是将模型从 DistilBERT 换成 BERT 看看异同。