---
layout:     post
title:      论 Transformer 对 NLP 排行榜的破坏作用
subtitle:   
date:       2019-08-31
author:     一轩明月
header-img: img/post-bg-space.jpg
catalog: 	 true
tags:
    - Transformer
excerpt:    BERT 为代表的大规模预训练模型取得了辉煌成就，众多研究告诫着我们对 BERT 的理解是十分浅薄，而它们却遍布排行榜，而且一个趋势是模型越来越大，不再是展示思想更像是秀资源家底。要取得确实的进步，就要保证新模型是经过严苛证明的——包括对同一批数据多轮训练后得到的基准性能，消融研究，计算损耗与稳定性估计等等。
---

> 文章编译自：
>
> https://hackingsemantics.xyz/2019/leaderboards/

NLP 领域当下如此活跃，排行榜要记一大功，他们是众多共享任务的核心，比如 GLUE 为代表的基准体系，SQUAD 为代表的个体数据集以及 AllenAI 数据集。排行榜刺激不同工程团队间相互比拼，不断在攻克人类语言的道路上寻找更好的方法、更好的模型

但，果真如此吗？

## 排行榜的问题何在？

NLP 某一任务 X 的排行榜通常是这样的：

| System   | Citation           | Performance |
| -------- | ------------------ | ----------- |
| System A | Smith et al. 2018  | **76.05**   |
| System B | Li et al. 2018     | 75.85       |
| System C | Petrov et al. 2018 | 75.62       |

在线排行榜（比如 GLUE 基准）和学术论文（在将提出模型和基准比较时）都采用这种形式。

当下，考量一个模型是否新颖、有趣远不止性能表现这一个指标，但它却是排行榜上的唯一指标。DL 家族十分庞杂，有各种各样的架构，以至于没有一个标准方法能够呈现模型参数、训练数据集等附加信息。表现在论文里就是，这些细节一会出现在技术选型中，一会被放在了附录里，又一会是在 GitHub 仓库讨论区里，亦或根本不存在。而在在线排行榜中，模型细节只能跟着链接回论文里检索（如果能访问到论文的话），或者检索库中代码。

世界节奏在加快，有几个人真会去深究这些细节呢？除非我们在审稿或是复现吧？简洁的排行榜已经给出了我们最关心的信息：哪个模型是最好的。《思考，快与慢》一书中指出，一般我们都很懒，倾向于不加批判的接受信息，忽视任何警告，即使立刻出现的那种。即便我们真的积极追踪警告......没机会的，赢家已经得到广泛地宣传和推广，已经在盲审时获得了某种不公平的竞争优势。

对于一切向最优看（SOTA-centric）的危害已经有很多人讲过了。如果读者的关注点在于上榜，那么会助长只有能挑战最优才值得出版的不良风气。这种觉知导致海量论文只是具有边际收益且通常不可复现。赢家之外的人们会认为他们的工作甚至不值得花时间写论文，这对于共享任务的危害无疑是巨大的。

本文要说的还有近期排行榜上的另一问题。原因很简单：通常情况下，**在数据集上构建更好的表示更有机会击败其他模型——或者只是单纯用更多的数据，更深的网络就行**。如果一篇论文提出一个更好的新模型，相较竞争者使用了更多的数据/算力，我们很难确认这种胜利到底归功于哪一方面。

当下大多数 NLP 排行榜都被基于 Transformer 架构的模型统治着。[BERT](https://www.aclweb.org/anthology/N19-1423/) 获得 NAACL 2019 最佳论文奖，其在诸多排行上都达到了最优水平（SOTA，state-of-the-art）并保持了几个月。当下热点话题是据说 [XLNet](https://arxiv.org/abs/1906.08237) 在 GLUE 和一些其他基准测试中超越了 BERT。其他 Transformer 模型还有 [GPT-2](https://openai.com/blog/better-language-models/)，[ERINE](https://arxiv.org/abs/1905.07129) 等等，模型列表仍在变长。

而我们开始要面对的问题是，这些模型太大了。尽管源代码是开放的，可实际上普通实验室根本没有能力重现这些结果，或是做一些比对的事情。比如， XLNet 使用了超 3200 万个标识符（token），500 块 TPU 跑了 2 天，花费超过 25 万美金。即使微调这一模型都是十分昂贵的。

## 且慢，本应如此才对

另一方面，有种看的到且几乎无法避免的趋势：资源更多的人使用更多的资源取得了更好的结果。你甚至可以说，这般巨大的模型以此证明了自身的可扩展性，并兑现了深度学习诞生时的承诺——能从更多的信息中学到更加复杂的模式。没人知道面对特定的 NLP 任务我们实际需要多少数据，但能多一些总该是好的，而且限制住数据似乎适得其反。

从这点来看，从今往后顶尖 NLP 研究可能只是工业界才玩得起的游戏了。相对的，无论是尝试获取更多资助，还是和高性能计算中心合作，学界必须想办法提升自己的游戏水准。当然也可以转型作分析，在工业界提供的大模型上做点什么，或是构建数据集。

但是，这对于 NLP 总体发展而言可能不是什么好事。

## 为什么 ”大模型 + 排行榜 = 灾难“

大模型的主要问题是：

> “更多的数据 + 算力 = SOTA” 不是研究体例

如果排行榜强调的是实际进步，我们要做的是寻找新架构而不是团队间斗争损耗。显然，大规模的预训练模型是有价值的，可除非作者们能证明他们的模型在基于可比对数据和计算的斗争中始终表现一致且确有差异，我们无法确认他们是在展示模型还是在展示资源。

此外，这些研究多数无法复现。没人会花 25 万美金只是重新跑一下 XLNet 模型。况且，消融研究显示它只在 3 个数据集（共 4 个）上相比 BERT 取得了 1-2 个百分点的收益，实际上，我们不能保证其遮挡策略比 BERT 更成功。

同时，因为难度通常更高、向排行榜看齐的社区只奖励表现最好的模型，精简模型的开发备受冷落。反过来讲，超越学术团队竞争水平的价格无法让学生毕业时成为更好的工程师。

最后，有[文章](https://openreview.net/forum?id=rJl-b3RcF7)指出大规模深度学习模型经常过度参数化。比如，一项[研究](https://arxiv.org/abs/1901.05287)中，缩小版 BERT 在语法测试中得分比大规模版更高。有[文章](https://arxiv.org/abs/1906.02243)不无尴尬的指出了现实，深度学习模型需要大量算力本身不一定是坏事，但是浪费算力对环境而言就不是什么好事了。

> 消融研究（ablation study）：为验证特征有效性、必要性而设计的实验，通常是将删掉特征后和保留特征时的结果进行比对

## 可能的解决方案

NLP 排行榜当下面临着巨大危机，我们放弃复现只是每隔几个月看着一个 Google 模型超越另一个 Google 模型。为了避免这种情况发生，排行榜需要做出改变。

原则上有两个可能的解决方案：

1. 对某个具体的任务，应该**提供标准训练语料库，将算力限制在强基线水平**。如果是 BERT 这样的基线，这会促使模型向精细化使用资源的方向发展。如果模型使用预训练表示（词嵌入，BERT 等等），预训练数据集大小应该计入最终成绩。
2. 对于像 GLUE 这样的系列任务，我们可以**不限制候选者对训练数据规模和算力的选择，但会计入最终成绩**。排行榜应该立刻澄清某一模型相对其使用的资源量，性能超过了基线多少

这两种方法都要求有一种稳妥的方式估计计算损耗。至少，可以选择任务组织者估计的推理时间。亚历山大·德洛斯指出最佳方式是上报单位时间浮点运算次数， 似乎 PyTorch 和 TensorFlow 都已经对此提供了支持。或许还可以对共享任务构建一个通用服务，接收一个深度学习模型，在一批数据上训练一轮，而后反馈给研究员损耗估计值。

训练集也不能直接评估：纯文本语料库的价值应该低于注释过的语料库。但应该保证可以量化。比如，非结构化数据可以为原始计数 $$N$$，增强/解析数据为 $$aN$$，结构化数据比如字典记为 $$N^{2}$$。

从另一方面看，一些模型可能本身确实是需要比其他模型更多的数据，且只能在大规模实验上评估。即便如此，令人信服的论文应该证明新模型相较对手确实值得“拥有”更多数据，所以仍应该使用所有模型针对同一批数据多轮训练而后加以比较。

## 总结

BERT 为代表的大规模预训练模型取得了辉煌成就，且在众多任务上取得了最佳效果。当然从技术上讲，用任何 `<muppetName>` 作为预训练表示都是可以的，只要论文讲的是其他事且并不依赖于任何未经充分验证的`<muppetName>`的任何属性。分析 `<muppetName>` 也无可厚非：BERT 流派论文不断地告示着我们对 BERT 的理解是多么浅薄，而它们却遍布排行榜。

假如一篇论文引入新的 `<muppetName>` 却未考虑其稳定性和训练、竞赛所需花费，我们会实实在在地遇到技术难题。而后所有人都将排行榜上的表现视为突破性架构的指标。

试想一下明天醒来，有篇论文提出了“别想试”网络架构，该架构使用全北美计算机训练了一年，在所有 NLP 任务上都达到了超人水平。即使这时手握源码，我们也无法证实这一论断。我们可以使用预训练权重，但不经多轮消融研究与稳定性评估，架构作者依旧无法证明其方法的优越性。某种意义上讲，他们秀的是资源而非模型本身。

如果我们想要取得确实进步，我们要保证赢得赞誉与声望的新模型是经过严苛证明的——包括对同一批数据多轮训练后得到的基准性能，消融研究，计算损耗与稳定性估计等等。这会极大地鼓舞假设驱动型研究。
