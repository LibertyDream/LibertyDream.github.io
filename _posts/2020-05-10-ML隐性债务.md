---
layout:		post
title:  	机器学习系统的隐性债务
subtitle:   
date:       2020-05-10
author:     一轩明月
header-img: img/post-bg-blue.jpg
catalog:    true
tags:
    - opinions
excerpt:    机器学习为快速构建实用、复杂的预测系统提供了一套强力工具。但如果认为这些成果是天上掉下来的就危险了。借用软件工程技术债务框架，会发现现实 ML 系统通常有着高昂的维护成本。本文探究了几个系统设计时要考虑到的 ML 相关风险因素，包括边界侵蚀，缠结，隐式反馈循环，未知消费者，数据依赖，配置问题，外部世界变动以及各种系统级反范式设计
---

> 编译自：Hidden Technical Debt in Machine Learning Systems，[Google team](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)

# 引言

随着机器学习（machine learning，ML）社区经年累月的积攒实时系统经验，一个广泛存在而又令人不安的趋势开始显现：开发、部署 ML 系统相对容易便捷，但长久维护这些系统却很难，很贵。

这种二分现象借助 _技术债务_ 的透镜可以更好地理解，这是一种比喻，1992 年由 Ward Cunningham 提出，用来帮助分析软件工程中快速行动带来的长期成本。和财务负债一样，通常有充足的理由背上技术债务。不是所有的债务都是坏的，但欠下的债都是要还的。技术债务或许可以通过重构代码，改善单元测试，删除僵尸代码，减少依赖，精简 API 和改良文档说明进行结算。其目的不在于添加新功能，而是为了将来的提升，减少错误，提高可维护性。延期偿还只会加重负担，隐性债务之所以危险是因为它是无声累积的。

本文提出 ML 系统自身有着促发技术负债的特性，因为这些系统不仅有着传统的代码维护问题，还有一些额外的 ML 专属问题。这一债务很难察觉，因为它是系统层面的而非代码层面。数据本身会影响 ML 系统的行为，这导致传统抽象和边界会被破坏或失效。同时，传统偿还代码层面技术债务的方法不足以消除系统层面 ML 特定的技术隐患。

本文不会提出什么新 ML 算法，而是想引起社区警觉，实践中必须着眼长期进行艰难取舍。本文重点关注系统层面的交互和接口设计，这也是 ML 技术债务可能会快速积累的领域。系统层面上，ML 模型可能会无声侵蚀抽象边界。重用和输入信号链可能会无意间耦合原本分离的两个系统。ML 包被当作黑盒看待，导致大量的“胶水代码”或是停留在猜想中的校准层。外部世界的变动会出乎意料地影响到系统行为。即使是要监控 ML 系统行为，如果不精心设计也会是困难重重。

# 复杂模型侵蚀边界

传统软件工程实践已经证明，使用封装和模块化设计的强抽象边界有助于编写可维护代码，即易于孤立地进行变更和改进。严格的抽象边界有助于呈现给定组件的不变性和信息输入、输出的逻辑一致性。

不幸的是，很难通过指定期望行为的方式给机器学习系统施加严格抽象边界。诚然，在 _不依赖外部数据就无法用软件逻辑高效呈现期望行为_ 的场景下我们需要 ML，但现实世界不是整洁利落的封装起来的。这里研究了几种导致边界侵蚀的方法，这些方法可能导致 ML 系统技术负债显著增加。

- **缠结**

机器学习会将信号混合在一起，使其缠结，无法做到隔离出来进行改进。比如，考虑一个有着若干特征 $$x_1,\dots,x_n$$ 的模型，如果改变 $$x_1$$ 的输入分布，剩下的 $$n-1$$ 个特征的重要性，权重或者说使用情况都会发生改变。无论模型是离线批量更新还是在线推断这都是真实要面对的场景。添加一个新特征 $$x_{n+1}$$ 会导致相似变化，亦如删掉任意特征 $$x_j$$ 。没有输入是真正独立的，这里我们称其为 CACE 规则：Changing Anything Changes Everything（牵一发而动全身）。CACE 不仅适用于输入信号，超参数，学习环境，采样方法，收敛门槛，数据选择，本质上所有可能的微调都适用。

一个可能的缓和策略是隔离模型，集成服务。该方法很适合那些能自然拆解成子问题的情景，比如彼此没有交集的多分类场景。但很多情况下集成效果良好是因为组件模型的误差是不相关的，对组合的依赖会导致强缠结：改良单个模型反而可能导致系统准确率变糟，如果剩余误差和其他组件相关性更高的话。

另一个策略是当他们发生时密切监测预测行为的变化。比如 [H. B. McMahan 等，2013](https://research.google.com/pubs/archive/41159.pdf)就提出了这样一种方法，用一个高维可视化工具帮助研究员快速看到多维度和切片的效果。逐片进行度量可能也是很有效的方式。

- **级联校正**

通常存在这样一种情况，针对问题 $$A$$ 设计了模型 $$m_a$$，但需要的是与之稍有不同的问题 $$A^{\prime}$$ 的解决方案。这时，一个自然的想法是以 $$m_a$$ 作输入并学习点修正训练模型 $$m_a^{\prime}$$ 出来快速解决问题。

但这种修正模型创建了一个依赖于 $$m_a$$ 的新系统，使得将来分析改进模型要费很大的力气。当修正模型彼此级联，相应代价也随之上升，比如在 $$m_a^{\prime}$$ 之上为了问题 $$A^{\prime \prime}$$ ，为了几个彼此间有些许差异的测试版等等场景构建模型。一旦时机成熟，级联修正可能会造成改进的死锁，改进任何单个组件的准确率实际会造成系统层面的损失。

弥补策略是强化 $$m_a$$，添加特征辨析样本，在相同模型上直接学习修正内容。或者付出成本为 $$A^{\prime}$$ 单独创建一个模型。

- **未申明消费者**

通常某个机器学习模型 $$m_a$$ 的预测会被广泛使用，或是运行时，或是写到文件/日志里以供后续其他系统消费使用。而如果缺乏访问控制，某些消费者会是未申明的。悄悄地将模型结果输入给别的系统。传统软件工程里将这类问题称为可见度债务。

未申明消费者往好了讲是成本高，往坏了说则很危险。因为他们会造成模型 $$m_a$$ 与栈内其他部件间的强耦合，对 $$m_a$$ 的变更很可能会影响到其他部分，以一种潜在的无意识、难以理解而又有害的方式。实际上，这种强耦合会巨幅增加成本以及对 $$m_a$$ 做出丝毫改动的难度，即使是改良也不例外。此外，未申明消费者还可能造成隐性反馈循环，这在后面部分细说。

除非专门进行过设计防止相关情况发生，比如访问限制或严格的服务层协议（ service-level agreements，SLAs），未申明消费者可能很难被发现。没有阻拦，工程师们自然会用手头最便利的信号，尤其是在工作截止日期临近的时候。

# 数据依赖成本高于代码依赖

在 [J. D. Morgenthaler 等，Google](https://research.google.com/pubs/archive/37755.pdf) 中，_依赖债务_ 在传统软件工程环境中为代码复杂度和技术债务做出了突出贡献。我们发现 ML 系统里 _数据依赖_ 在搭建债台上有着相似能力，只是可能更难察觉。代码依赖可以通过编译器和链接器静态分析出来，但对数据依赖缺乏相似工具，所以很难绘制出大规模数据依赖链，也就很难进行解耦了。

- **不稳定数据依赖**

为了求快，经常拿其他系统产生的特征作输入信号，但一些输入信号并不稳定，意味着它们会随时间定量或定性的变更行为。这可能会暗中发生，当输入信号来自另一个机器学习模型而其自身会随时间更新，或者来自相关数据查询表，比如 TF/IDF 值或语义映射的计算。如果输入信号所有权和消费它的模型的所有权相互独立，那不稳定数据依赖也可以显式发生。此时，输入信号可能随时产生，而这很危险。因为即使是对输入信号的“改良”都可能对那些诊断处理成本高昂的消费系统造成随机有害影响。比如，试想某个输入信号先前校验出错了，消费信号的模型很可能会去拟合这些校验误差，纠正信号的无声更新可能会对模型造成突发影响。

一个通行应对策略是对给定信号创建版本副本。例如，相较于允许词语与主题聚类间的语义映射能随时间改变，对该映射创建一个静态版本可能更合理，直到更新版经过了充分审查前就一直使用该版本。版本控制自身有开销，比如可能会过时，随时间推移对相同信号保存了多个版本等。

- **未利用的数据依赖**

编程时未利用依赖多半是并不需要那些包。相似的，未利用数据依赖是指输入信号只给模型带来了微不足道的收益。这可能导致即便可以没影响的删掉它们，模型还是会没必要的频繁变动，有时还是灾难性的。

比如，为了减少麻烦，在新旧产品编码方案转换时将二者都作为特征保存在系统中。新产品只有一个新编码，但旧产品可能两个都有，同时模型对某些产品会继续用旧编码。一年过后，旧编码停用要从库中删除，这对 ML 系统的维护人员来说可不会是什么好日子。

未利用的数据依赖可能以几种方式潜入模型之中：

1. **遗留特征**。最常见的情况是特征 $$F$$ 是早先开发模型时的产物，随着时间推移，新特征使 $$F$$ 变得多余但也检测不出来了。
2. **捆绑特征**。有时要对组合特征进行评估且被证实有效，因为截止日期的压力或类似作用下，所有捆绑特征都被一并加入模型，其中可能包含少值或无值特征。
3. **$$\epsilon$$ 特征**。作为研究员，即使在精度收益很小或复杂度开销可能很高的情况下，依旧有修改模型的冲动
4. **关联特征**。通常两个特征强相关，而其中一个更像成因。很多 ML 方法很难发现这一点并认为两个特征是一样的，甚至可能会去选那个非成因的。如果稍后现实行为改变了相关关系，会导致模型十分脆弱

未利用依赖可以费点力气用特征留一评估法进行检测。这可以定期做来识别并删掉不必要特征

- **数据依赖的静态分析**

传统编程时，编译器和构建系统会进行依赖图的静态分析。数据依赖的静态分析工具太过少见，但对错误检查，跟踪消费者，强制移动和更新来说又至关重要。 [H. B. McMahan 等，2013](https://research.google.com/pubs/archive/41159.pdf) 提出的自动特征管理系统就是这样一个工具，其可以注解数据源和特征。之后可以进行自动检查保证所有依赖都有恰当的注释，依赖树也完全不是问题。实际上这类工具使迁移和删除变得更加安全

# 反馈循环

在线 ML 系统的一个关键特征在于如果他们随时间更新，通常最终会影响到自身行为。这导致了某种形式的分析债务，也就是说在模型发布前很难预测其行为。这些反馈循环形式多样，但如果他们随时间渐次发生，就更难以监测和处理了，就像不经常更新模型一样。

- **直接反馈循环**

模型可能直接影响到它将来训练数据的选择。尽管理论上该使用 bandit 算法，更常见的还是会用标准监督算法。这里的问题在于 bandit 算法（比如[环境感知 bandit](https://hunch.net/~jl/projects/interactive/sidebandits/bandit.pdf)）并不一定保证能适配现实问题所需的行为空间大小。

采用一定程度的随机化可能有助于缓解这些影响，或者可以将受到给定模型影响的数据部分隔离开

- **隐性反馈循环**

直接反馈循环分析起来费时费力，但至少他们提出了一项统计挑战，ML 研究员们可能会觉得理应进行调研。更麻烦的情况是隐性反馈循环，两个系统穿越世界间接影响着彼此。

举个例子，试想有两个系统独立决定网页的各个方面，例如一个挑选产品进行展示而另一个选择了相关评论。改善其中一个系统可能导致另一个的行为变动，因为另一个组件上人们会更多或更少的点击以回应变化。注意这些隐性循环可能存在于完全独立的两个系统之间。试想两家投机公司下的两个股市预测模型，其中一个的改进（更怕的是 bug）可能会影响到另一个模型的出价和买入行为。

# ML 系统的反范式设计

可能会让学术圈有些惊讶，ML 系统中只有很小一部分的代码是用来学习或预测的——见下图。按某些人的话，剩下大部分可以成为“管道设施（plumbing）”。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-10_real_world_ML_system.png)

很不幸，使用了机器学习方法的系统多数会在设计模式上落得债务累累。这一部分，我们会探究机器学习系统中可能出现的几种反范式系统设计，应当尽可能的避免或重构。

- **胶水代码**

ML 研究员喜欢将一般性问题的解决方案封装成独立的包，在像 mloss.org 上，内部代码中，专有软件包和云平台上有各式各样的这类开源包。

使用通用包会形成一种系统设计模式，胶水代码。大量支持性代码是为了从通用包中读取或写入数据。长期来看胶水代码成本很高，因为它会将系统绑定在特定包的特质上，测试替代品的成本高到令人咂舌。这样，使用通用包会抑制改善创新，因为它很难有效利用领域知识或者微调目标函数达成领域目标。鉴于一个成熟系统可能最终（最多）有 5% 的机器学习代码，（至少）有 95% 的胶水代码，建一个干净利落的本地解决方案而不是复用通用包可能代价更小些。

和胶水代码抗争的一条重要策略是把黑盒子样的包封装进通用 API 当中。这样支持性基础设施的复用率更高，变更包的成本也更低。

- **管道密林**

作为胶水代码的一种特殊情况，管道密林通常出现在数据准备环节。每当新信号被识别，新数据源被加入，“密林”都能有机演化。不加打理的话，ML 格式友好的数据准备系统最终会变成刮擦，连接和一般会生成中间文件的采样步骤的杂乱混合体。要管理这些管道，检测错误，或是错误恢复都是难度大，成本高。测试这些管道通常需要进行昂贵的端到端集成测试，所有这些都给系统背上了更沉重的技术债务，使得进一步革新的代价越来越大。

管道密林只能通过对数据采集和特征抽取的整体性思考加以规避。清理管道密林的利落方法和从头重新设计确实是工程实践里的大部头，但这能巨幅降低后续成本，加速进一步变革。

胶水代码和管道密林是集成问题的征兆，其根源可能在于过度分割了“研究”和“工程”角色。当 ML 包在象牙塔环境里开发出来，最终对于实际使用它们的团队而言可能就是个黑盒子。工程师和研究员同处一队（确实，经常是同样的人）的混合研究方法能显著减少此类摩擦。

- **无效实验代码路径**

胶水代码或管道密林带来的常见结果之一，就是人们愈发喜欢对可替代方案进行短期实验，在主体产品代码中拉个分支搭条实验代码路径出来。对任意单个改变而言，这种实验方式的成本都是相对比较低的——没有基础设施需要重做。但随着时间推移，这些积攒下来的代码路径会造成日渐增长的债务负担，因为要维护向后兼容性的难度在变大，环复杂度也在呈指数增长，想要测试代码路径间所有可能的交互变得困难重重甚至根本不可能。

一个知名风险[案例](https://www.sec.gov/news/press-release/2013-222)莫过于骑士资本的系统在 45 分钟内损失了 4.65 亿美金，显然是由过时实验代码路径造成的。

和传统软件中的无效标志一样，定期检查一下每个实验分支，看看哪些可以删除，这总是有好处的。通常仅有一小部分分支在实际使用，许多其他的都是测试了一次就被丢弃了。

- **抽象债务**

上述问题清楚地指出支持 ML 系统进行的强抽象存在显著缺陷。[A. Zheng 2014](https://www.slideshare.net/AliceZheng3/the-challenges-of-bringing-machine-learning-to-the-masses) 对 ML 抽象状况与数据库技术的进行了比较研究，发现如果将关系数据库的抽象做基准，没有哪篇机器学习文献能够得上的。描述数据流，模型或预测的正确接口会是什么样的呢？

特别是对分布式学习而言，仍缺乏广为接受的抽象。可以说机器学习中 MapReduce 的广泛使用是由于强分布式学习抽象的缺失造成的。确实，近年来少数取得的广泛共识之一便是 MapReduce 对于迭代型 ML 算法来说是个糟糕的抽象。

参数服务器的抽象似乎更健壮些，但有很多基于该思想的规范在竞争。缺乏标准抽象使得极易混淆组件间的边界。

- **常见异味**

软件工程领域中，[设计异味](https://en.wikipedia.org/wiki/Design_smell)可能会给组件或系统带来潜在问题。我们发现了一些 ML 系统的异味，不是严格的规则，只是些主观指标。

1. **原始数据类异味**

ML系统产生和使用的丰富信息全都经常是用原始浮点数和整型编码的。对一个鲁棒系统而言，该能辨明模型参数算是对数概率乘数还是决策边界，有关生产预测和预测如何使用的各式各类的信息它也应该清楚

2. **多语言异味**

用给定语言写一个特定类型的系统通常很有诱惑力，特别是当这种语言对手头任务有着开包即用的库或是便捷语法的时候。但是，多语言也意味着测试成本上升以及让他人接手的难度上升。

3. **原型异味**

借助原型测试下新想法无疑是很便捷的。但总依赖于原型环境可能导致系统整体的脆弱，难于修改，也很难从抽象与接口改良中获益。维护原型环境自有其代价，一个显著危害在于时间紧迫的时候可能直接把原型当解决方案使。此外，小规模信息总结出的经验只反映了全部真相极少的一部分

# 配置债务

另一个可能潜在积累债务的点有些令人吃惊，它就在机器学习系统的配置当中。任何大型系统都会有宽泛的可配置项，比如所用特征，数据选择方法，各类算法相关的学习设置，预/后处理，验证方法等等。我们发现无论是研究员还是工程师都将配置（与配置拓展）视作后续任务，也确实，配置的验证或测试可能没被太重视。在积极开发地成熟系统中，配置行数可能会很快超过一般代码的数量。每项配置都可能出错。

考虑以下场景。特征 $$A$$ 错记 $$9/14$$ 为 $$9/17$$。特征 $$B$$ 对于 $$10/7$$ 之前的数据禁止访问。因为记录格式的变化，计算特征 $$C$$ 的代码必须在 $$11/1$$ 前后根据数据作调整。特征 $$D$$ 生产环境不可访问，所以子特征 $$D^{\prime}$$ 和 $$D^{\prime \prime}$$ 必须是在在线查询时才能使用。如果用到了特征 $$Z$$，那任务要额外分配点内存否则训练无效。鉴于延迟约束特征 $$Q$$ 限制了特征 $$R$$ 的使用。

所有这些乱七八糟的情况导致适当调整配置十分艰难，也很难分析出原因。但是配置项出错的代价是高昂的，会损失大量的时间，浪费海量的算力，造成生产事故。这也促使我们拟定了一些关于良好配置系统的原则：

1. 与先前配置相比所作出的小改动要很容易
2. 人为失误，缺漏，或是越界概率要低
3. 对模型间的配置差异要易于可见，可视化
4. 要易于断言与验证配置基本内容：所用特征数，数据依赖的传递闭包等等
5. 要能检测未使用或冗余设置
6. 配置要经过全面的代码审查，核对入库

# 应对外部世界变化

ML 系统如此令人着迷的一点在于它们通常会直接和外界打交道。经验证明外部世界绝少处于稳定状态，频繁变化的背景下造成持续维护成本升高。

- **动态系统中的固定阈值**

为了要做某些动作免不了给模型选个 _决策边界_：预测真假，标记是否为垃圾邮件，是否展示广告。传统机器学习做法是在一组可能的阈值当中挑选，这样对于特定标准拿捏地比较到位，比如精确度和召回率。但是这些阈值经常要手工操作。所以如果模型根据新数据更新了那旧有人为设置可能会失效。多模型上手动更新多个值费时且脆弱。[D. Sculley 等](https://research.google.com/pubs/archive/37195.pdf) 对该类问题给出了一种应对方式， 在有限验证数据集上通过简单的选举来学习阈值

- **监控和测试**

单个组件的单元测试和系统运行时的端到端测试都是很值得的，但面对快速变化的外部世界这样的测试并不足以证实系统是在如期运行。要想系统长期稳定可靠，对系统行为的全面实时监控以及自动化反馈必不可少是。

关键在于：监控什么？考虑到许多 ML 系统都是随时间调整变化的，可测变量并不总是那么明显。不妨从以下几点开始。

1. **预测偏差**

对一个如期运作的系统而言，预测标签的分布应当和观测标签的想吻合。这当然不是全面测试，因为只需要一个空模型，不顾输入特征仅是就按标签出现的平均值预测也能有这个效果。但是，这也是出奇有用的诊断法，而且像这类指标的变化通常代表着需要关注的问题出现了。比如，该方法能帮助检查世界突变的情况，依照历史数据所画训练分布不再反应当下现实。在各个维度对预测误差进行分离能快速定位问题，还能用于自动警报。

2. **行为限制**

对于会采取现实行动的系统，像物品叫价或垃圾信息标注，设置乃至强行限制行为都对完整性保障有裨益。限制要宽别成虚假警报。当系统出发行为限制，要能自动报警并引入人为干预和调查。

3. **上游生产商**

数据通常经由各种上游生产商喂给学习系统。这些上游生产方应该被全程监控，详尽测试，还得例行满足服务级目标，期间要考虑到下游 ML 系统的需求。此外，任何上有警报必须反馈给 ML 系统的控制台以确保准确性。相似的，在 ML 系统实现既定服务目标时，出现的任何错误也要分发给所有下游消费者，如果可能的话直接传给他们的控制台。

外部变化实时都在发生，反馈也要实时跟上。依靠人为干预对警告页进行干预是一种策略，但对时限敏感的问题来说就不够看了。专门构建系统以完成自动化反馈，摆脱直接人为操作影响通常是笔划算的买卖

# 其他机器学习相关债务

下面简短强调一些其他可能出现的机器学习相关债务

- 数据测试债务

如果 ML 系统中代码换成数据，代码应当接受测试，显然一些输入数据的测试对于完善系统同样至关重要。基本的健全检测就很用，可以看作是监控输入分布变化的更复杂的测试