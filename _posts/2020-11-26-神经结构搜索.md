---
layout:		post
title:  	神经结构搜索
subtitle:   
date:       2020-11-26
author:     一轩明月
header-img: img/post-bg-blue.jpg
catalog:    true
tags:
    - reinforcement learning
    - auto-ML
excerpt:    神经结构搜素（NAS）能使网络结构工程自动化，目的在于学会一种网络拓扑，该拓扑对特定任务有绝佳效果。NAS 技术可以分成三块：搜索空间，搜索算法和子模型演化策略。本文回顾了多个很有意思的想法，都旨在实现更好、更快、更经济的自动化神经结构搜索。
---

> 编译自：Neural Architecture Search， [Lilian Weng](https://lilianweng.github.io/lil-log/)

尽管多数成功且流行的模型结构是由人类专家设计的，但不意味着我们已经探索完了整个网络结构空间并找到了最佳选择。如果以一种系统且自动的方式学习高性能模型结构，找到最佳结果的胜算会更大些。

自动学习和演化网络拓扑并不是什么新思想（[Stanley 和 Miikkulainen, 2002](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)）。近些年，[Zoph 和 Le 2017](https://arxiv.org/abs/1611.01578) 和 [Baker 等, 2017](https://arxiv.org/abs/1611.02167) 在神经结构搜索（Neural Architecture Search，NAS）领域所做的开创性工作引起了人们的广泛关注，推动了许多有趣思想的诞生，旨在得到更好、更快且更经济的 NAS 方法。

刚开始接触 NAS 的时候，[Elsken 等, 2019](https://arxiv.org/abs/1808.05377) 这篇综述给了我很大帮助，他们将 NAS 描述成由三个主要部分组成的系统，简单明了且常为其他 NAS 论文引用。

1. **搜索空间**：NAS 搜索空间定义了一系列操作（比如卷积、全连接，池化），以及怎么拼接这些操作组建有效网络结构。搜索空间的设计往往会用到人类经验，也就不可避免的会带有人类偏见。
2. **搜索算法**：NAS 搜索算法会在诸多网络结构候选项中进行采样。它以子模型性能标准为回报（比如高准确度，低延迟），并尽力生成高性能候选结构。
3. **评价策略**：要测算、估计或预测一众被举荐子模型的性能，这样才能得到搜索算法学习所需的反馈。评估候选项的过程开销可能会很大，人们也已经设计了很多新方法来节省时间或计算资源。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NAS-high-level.png)

*图 1  神经结构搜索（NAS）的三个主要部分（图片来源：[Elsken 等，2019](https://arxiv.org/abs/1808.05377))*

### 搜索空间

NAS 搜索空间定义了一系列网络基础操作，以及怎样连接操作构建有效网络结构。

#### 顺次层向操作

给神经网络结构设计搜索空间，最简单的方式是描绘网络拓扑，要么 CNN 要么 RNN，有一系列 *顺次层向操作（sequential layer-wise operations）*，就像早期工作 [Zoph 与 Le 2017](https://arxiv.org/abs/1611.01578) 和 [Baker 等, 2017](https://arxiv.org/abs/1611.02167) 所做的那样。网络表示的序列化需要大量专业知识，因为每个操作都与不同的层参数关联，这样的关联关系又需要硬编码。比如，预测过 `conv` 操作之后模型应该输出核大小，步长等信息；或者预测过 `FC` 操作之后我们需要知道下个预测的单元数量。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NAS-search-space.png)

*图 2  （上）CNN 的序列表示（下）循环单元的树结构的序列表示（图片来源：[Zoph 和 Le，2017](https://arxiv.org/abs/1611.01578)）*

要保证得到的结构有效，还得加点规则（[Zoph 和 Le，2017](https://arxiv.org/abs/1611.01578)）：

- 如果某层没与任何输入层相连，那它就是输入层；
- 在最后一层，把所有没被连接的层输出连起来；
- 如果某一层有多个输入层，那所有输入层要按深度连接起来
- 如果待连接的输入层尺寸不一，那对较小的层用 0 填充使连好的层大小一致

借助类似注意力的机制，跳跃连接关系也能预测。在 $$i$$ 层为锚点上加上 $$i-1$$ 个基于内容的 sigmoid 信息，以此表示要与前面哪些层连接。每个 sigmoid 以当前节点 $$h_i$$ 的隐态和之前 $$i-1$$ 个节点 $$h_j,j=1,\dots,i-1$$ 作为输入。



$$
P(\text{j 层是 i 层的输入}) = \text{sigmoid}(v^\top \tanh(\mathbf{W}_\text{prev} h_j + \mathbf{W}_\text{curr} h_i))
$$



顺次搜索空间的表示能力很强，但它太大了，而且要周密探索搜索空间得耗费巨量计算资源。[Zoph 和 Le，2017](https://arxiv.org/abs/1611.01578) 的实验用了 800 块 GPU 并行跑了 28 天，而 [Baker 等，2017](https://arxiv.org/abs/1611.02167) 对搜索空间加了点限制，最多不超过 2 个 `FC` 层。

#### 基于单元的表示

受视觉模型结构（如 Inception，ResNet）的成功启发，特别是复用组件这一设计思路影响，*NASNet 搜索空间*（[Zoph 等，2018](https://arxiv.org/abs/1707.07012)）设计了一种卷积网络结构，网络就是相同单元的多次重复，并且每个单元里面都有几个 NAS 算法预测的操作。精心设计的单元组件可在数据集间的迁移。通过调整单元重复次数也很容易减少或增大模型规模。

准确地讲，NASNet 搜索空间要学两类单元来构建网络：

1. *一般单元（Normal Cell）*：输入、输出的特征映射有着相同维度
2. *缩小单元（Reduction Cell）*：输出特征映射的宽高减半

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NASNet-search-space.png)

*图 3  NASNet 搜索空间约束结构成一叠重复单元的样子。单元结构通过 NAS 算法优化（图片来源：[Zoph 等 2018](https://arxiv.org/abs/1707.07012)）*

每个单元的预测分成 $$B$$  个块（NASNet 论文里 $$B=5$$），每个块里有 5 个预测步骤，预测由 5 个不相同的 softmax 分类器给出，代表块内元素的不同选择。注意 NASNet 搜索空间在单元间不存在残差连接，模型仅在块内部学习跳跃连接关系。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_cell-prediction-steps.png)

*图 4  （a）每个单元包括 $$B$$ 个块，每个区块由 5 个不同决策决定（b）举例说明每步决策都能选什么操作*

实验中人们发现了一个 [*DropPath*](https://arxiv.org/abs/1605.07648) 变种，起名叫 *ScheduledDropPath*，能显著改善 NASNet 实验的最终效果。DropPath 会以固定概率随机删除一些路径（也就是 NASNet 里带操作的边）。ScheduledDropPath 是那种在训练时线性增大路径删除概率的 DropPath。

[Elsken 等，2019](https://arxiv.org/abs/1808.05377) 指出 NASNet 搜索空间主要有三大优势：

1. 大幅减小了搜索空间规模
2. 基于[样式](https://en.wikipedia.org/wiki/Network_motif)的结构很容易应用到不同数据集上
3. 证明结构工程中重复堆叠组件这一设计模式确实有用。比如，可以在 CNN 里堆叠残差块，在 Transformer 里堆叠多头注意力块来构建强力模型

#### 分级结构

为了利用已发现的精心设计过的网络[样式](https://en.wikipedia.org/wiki/Network_motif)，NAS 搜索空间可以转为分级结构，就像在*分级 NAS（Hierarchical NAS，HNAS）*（[Liu 等，2017](https://arxiv.org/abs/1711.00436)） 中一样。起初是一小撮基础运算，包括像卷积，池化，同一这些单个操作，然后由基础运算构成的小子图（或“样式”）递归地构建更高级的计算图。

在 $$\ell=1, \dots, L$$  级的计算样式可以表示为 $$(G^{(\ell)}, \mathcal{O}^{(\ell)})$$，其中：

- $$\mathcal{O}^{(\ell)}$$ 是一组操作，$$\mathcal{O}^{(\ell)} = \{ o^{(\ell)}_1, o^{(\ell)}_2, \dots \}$$
- $$G^{(\ell)}$$ 是邻接矩阵，其中 $$G_{ij}=k$$ 表示结点 $$i$$ 和 $$j$$ 间的操作为 $$o^{(\ell)}_k$$，结点索引遵循 DAG 中的[拓扑顺序](https://en.wikipedia.org/wiki/Topological_sorting)，索引 $$1$$ 是起点，最大的索引对应汇点（sink node）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_hierarchical-NAS-search-space.png)

*图 5  （上）三个 1 级基础运算构成一个 2 级样式（下）将三个 2 级样式插入基础网络集成为一个 3 级样式（图片来源：[Liu 等，2017](https://arxiv.org/abs/1711.00436)）*

要想按分级结构搭建网络，要从最低的 $$\ell=1$$ 级开始，递归地定义 $$\ell$$ 级的第 $$m$$ 个样式：


$$
o^{(\ell)}_m = \text{assemble}\Big( G_m^{(\ell)}, \mathcal{O}^{(\ell-1)} \Big)
$$


分级表示就变成 $$\Big( \big\{ \{ G_m^{(\ell)} \}_{m=1}^{M_\ell} \big\}_{\ell=2}^L, \mathcal{O}^{(1)} \Big), \forall \ell=2, \dots, L$$，其中 $$\mathcal{O}^{(1)}$$ 里是一系列基础运算。

$$\text{assemble}()$$ 过程等价于顺次计算结点 $$i$$ 的特征映射，按拓扑顺序把它的前继结点 $$j$$ 的所有特征映射聚集起来：


$$
x_i = \text{merge} \big[ \{ o^{(\ell)}_{G^{(\ell)}_{ij}}(x_j) \}_{j < i} \big], i = 2, \dots, \vert G^{(\ell)} \vert
$$

[论文](https://arxiv.org/abs/1711.00436)里 $$\text{merge}[]$$ 是按深度连接的。

和 NASNet 一样，[Liu 等，2017](https://arxiv.org/abs/1711.00436)的实验目的在于在预先定义好的带重复组件的”宏“结构中发掘优质单元结构，实验显示就算用简单的搜索方法（比如随机搜索或演化算法），精心设计过的搜索空间也能显著增强其性能。

[Cai 等，2018b](https://arxiv.org/abs/1806.02639)提出了一种树结构的搜索空间，用上了路径级网络转换。树中每个结点都设有给子结点划分输入的 *分配（allocation）* 计划，以及组合子结点结果的 *整合（merge）* 计划。如果要增加或合并整合方案，路径级网络转换可将单层换成一个多路样式。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_path-level-network-transformations.png)

*图 6  用路径级转换操作将单层转换为树结构样式（图片来源：[Cai 等，2018b](https://arxiv.org/abs/1806.02639)）*

#### 记忆库表示

前馈网络的记忆库表示方法是 [Brock 等，2017](https://arxiv.org/abs/1708.05344) 在 [SMASH](# 从预测出发) 中提出的。相较于运算图，作者将神经网络视作有多个可读写记忆块的一个系统。每层操作被设计为：（1）从记忆块子集中读取；（2）计算结果；（3）将结果写入另一子集。比如在序列模型中，就是不断从单个记忆块中读取与覆盖。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NAS-memory-bank-view-representation.png)

*图 7  几个流行网络结构块的记忆库表示（图片来源： [Brock 等，2017](https://arxiv.org/abs/1708.05344)）*

### 搜索算法

NAS 搜索算法会在一众子网络中采样，以子模型性能度量结果为回报，学着去得到高性能候选结构。与超参数搜索里的做法如出一辙。

#### 随机搜索

随机搜索是最基础的了，*随机* 从搜索空间中抽取有效候选结构，并不管学习模型。已知随机搜索在超参数搜索上十分有用（[Bergstra 与 Bengio，2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)）。精心设计的搜索空间下，随机搜索定下的性能底线可是很能打的。

#### 强化学习

最开始的 **NAS**（[Zoph 和 Le 2017](https://arxiv.org/abs/1611.01578)） 设计中有一个基于 RL 的控制器，决定要评估哪个子模型结构。控制器是个 RNN，输出一个用于配置网络结构的变长标识序列。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NAS.png)

*图 8  NAS 的高层视角，有一个 RNN 控制器和一条评估子模型的管道（图片来源：[Zoph 和 Le，2017](https://arxiv.org/abs/1611.01578)）*

控制器的训练是当作一个强化学习任务来看的，用 [REINFORCE](https://libertydream.github.io/2020/07/19/策略梯度算法/#reinforce)。

- **行动空间**：行动空间就是一个标识列表，这些标识定义了控制器所预测的子网络（见[上文](# 顺次层向操作)）是个什么样子。控制器会输出*行动（action）* $$a_{1:T}$$，其中 $$T$$ 是标识总数
- **回报**：收敛得到的子网络准确率就是训练控制器时的回报，$$R$$
-  **损失**：NAS 用 REINFORCE 损失来优化控制器参数 $$\theta$$ 。我们希望用下面给出的梯度使期望收益最大化（高准确率）。这里用策略梯度的好处在于即使回报不可微算法仍旧有效。


$$
\nabla_{\theta} J(\theta) = \sum_{t=1}^T \mathbb{E}[\nabla_{\theta} \log P(a_t \vert a_{1:(t-1)}; \theta) R ]
$$



**MetaQNN**（[Baker 等，2017](https://arxiv.org/abs/1611.02167)）训练了一个代理来顺次选取 CNN 层，训练使用的是结合了 ε-greedy 探索策略和经历回放的 [Q 学习](https://libertydream.github.io/2020/07/05/强化学习综述/#-q-学习非策略-td-控制)。回报也是验证准确率。



$$
Q^{(t+1)}(s_t,  a_t) = (1 - \alpha)Q^{(t)}(s_t, a_t) + \alpha (R_t + \gamma \max_{a \in \mathcal{A}} Q^{(t)}(s_{t+1},  a'))
$$



其中状态 $$s_t$$ 是个层运算和相关参数构成的元组。行动 $$a$$ 决定了运算间的连接性。Q 值大小与我们对关联起来的两个操作能带来高准确率的信心成正比。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_MetaQNN.png)

*图 9  MetaQNN 概览——用 Q 学习设计 CNN 模型（图片来源：[Baker 等，2017](https://arxiv.org/abs/1611.02167)）*

#### 演化算法

**增强拓扑神经演化**（**NEAT**，*NeuroEvolution of Augmenting Topologies*）是种用[遗传算法(GA)](https://en.wikipedia.org/wiki/Genetic_algorithm)演化神经网络拓扑的方法，在 2002 年由 [Stanley 和 Miikkulainen](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf) 提出。NEAT 会一并推演连接权重和网络拓扑。每段基因内都会携带着配置网络的全部信息，包括结点权重和边。目标总体会随着权重和连接的突变、双亲基因的交叉而增多。更多有关神经演化的内容可以看这篇深度综述 [Stanley 等，2019](https://www.nature.com/articles/s42256-018-0006-z)。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NEAT-mutations.png)

*图 10  NEAT 算法中的突变（图片来源：[Stanley 和 Miikkulainen，2002](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)）*

[Real 等，2018](https://arxiv.org/abs/1802.01548) 用演化算法（EA）来搜索高性能网络结构，称为 **AmoebaNet**。作者采用了[锦标赛选拔](https://en.wikipedia.org/wiki/Tournament_selection)法，该方法每轮会从一组随机样本中选择最好的那一项并将其突变后代放回目标总体。若锦标赛规模为 $$1$$，就等价于随机选择。

AmoebaNet 改进了锦标赛选举规则，使其对较年轻的基因型更友好，同时每轮都要丢掉最老的模型。这种方法叫 *陈化演进（aging evolution）*，使 AmobebaNet 可以覆盖和探索更多的搜索空间，而不是过早陷于到性能好的模型上。

更准确地讲，有陈化演进的每轮锦标赛选拔中（见图 11）：

1.  从目标总体中采样 $$S$$ 个模型，准确率最高的当作*亲代（parent）*
2.  通过亲代突变获得 *后代（child）* 模型
3.  接着训练、评估子模型并将其加入到目标总体中
4.  从目标总体中删除最老的模型

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_aging-evolution-algorithm.png)

*图 11  陈化演进算法（图片来源：[Real 等，2018](https://arxiv.org/abs/1802.01548)）*

用到的突变有两类：

1. *隐态突变（Hidden state mutation）*：随机选一对组合并重设随机一端使图中无环
2. *操作突变（Operation mutation）*：随机将一个既有操作随机换成另一个

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_AmoebaNet-mutations.png)

*图 12  AmoebaNet 中的两类突变（图片来源：[Real 等，2018](https://arxiv.org/abs/1802.01548)）*

实验中 EA 和 RL 在最终验证准确率上打平，但 EA 在随机时刻上的性能会更好而且能找到更小的模型。这里在 NAS 中用 EA 计算开销还是很大，每个实验要用 450 个 GPU 花 7 天来做。

**HNAS**（[Liu 等，2017](https://arxiv.org/abs/1711.00436)） 也是用演化算法（原始的锦标赛选拔）当作他们的搜索策略。[分级结构](# 分级结构)搜索空间中，每条边都是一个操作，所以他们实验中的基因型突变就是用一个不同的操作随机替换掉一条边。替换集中包括 `none` 选项，所以它能修改、删除和添加一条边。他们通过在“平凡”样式（所有单位映射，即输入=输出）上进行大量随机突变构建初始基因型组。

#### 渐进决策过程

构建一个模型结构是个有序过程，额外的运算或层都会带来额外的复杂度。如果我们引导搜索模型从简单模型开始，逐步演化到更复杂的结构上，很像是在搜索模型的学习过程中加入了“课程”。

渐进 NAS（Progressive NAS，**PNAS**； [Liu 等，2018](https://arxiv.org/abs/1712.00559)）将 NAS 问题看作是一个搜索模型复杂度逐步增加的过程。相较于 RL 或 EA，PNAS 用基于序列模型的贝叶斯优化（Sequential Model-based Bayesian Optimization，SMBO) 法来当搜索策略。PNAS 的工作模式类似 A* 搜索，因为它是由易到难的搜索模型，同时还在学习一个指导搜索的代理函数。

> [A* 搜索算法](https://en.wikipedia.org/wiki/A*_search_algorithm)（“最佳优先搜索”）是个很流行的寻路算法。寻路问题就是在一张带权图中，找到从特定起点到给定终点开销最小的路径。每轮迭代，A* 都会通过最小化 $$f(n)=g(n)+h(n)$$ 来扩张路径，其中 $$n$$ 是下一个结点，$$g(n)$$ 是从起点到 $$n$$ 的开销，而 $$h(n)$$ 是负责估计从 $$n$$ 点到目标点最小开销的启发式函数。

PNAS 采用 [NASNet](# 基于单元的表示) 搜索空间。每个块被设计成一个包含 5 元素的元组，并且 PNAS 在步骤 5 的组合运算中只考虑元素间加法，不包括连接。不同的是，相较于设定固定 $$B$$ 个块，PNAS 从 $$B=1$$ 开始，模型单元中只有一个块，逐步增大 $$B$$。

验证集上的表现就是 *代理（surrogate）*模型训练过程中的反馈，代理模型要负责*预测（predict）*新结构的性能。有这个预测器，我们可以决定接下来应该先评估哪个模型。因为性能预测器应该能处理各种尺寸的输入，准确率和样本效率，作者最终用了个 RNN 模型。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_progressive-NAS-algorithm.png)

*图 13  渐进 NAS 算法（图片来源：[Liu 等，2018](https://arxiv.org/abs/1712.00559)）*

#### 梯度下降

要用梯度下降更新结构搜索模型，需要使选择不同运算的过程可微。这些方法一般会把结构参数和网络权重的学习结合在一起。更多内容见下面[“单尝试”方法](# 单尝试法：搜索+评估)一节

### 评估策略

我们需要测量，估计或者说预测每个子模型的性能，以此来获取反馈优化搜索算法。候选项的评估过程开销可能会非常大，人们也设计了很多新评估方法来节约算力，节省时间。评估子模型的时候，我们通常关心的是它在验证集上的准确率。近期研究则开始关注模型的其他要素，比如模型大小和延迟，因为特定设备可能内存有限或需要快速响应。

#### 从头训练

最简单的做法是从头独立训练每个子网络直到 *收敛（convergence）*，然后算下它在验证集上的准确率（[Zoph 和 Le，2017](https://arxiv.org/abs/1611.01578)）。这确实能得到可靠的性能数字，但训练——收敛——评估一圈下来只给 RL 控制器的训练贡献了一个数据样本（更别说 RL 是出了名的样本效率低）。所以就计算开销而言这么做代价太大了。

#### 代理任务性能

有些方法用代理任务上的性能表现作为子网络的性能的估计量，这一般更经济，统计起来也更快：

- 在更小的数据集上训练
- 训练轮次更少
- 在搜索阶段训练并评估一个降档模型。比如，一旦学完了一个单元结构，我们可以大量复制单元或增大过滤器数量（[Zoph 等，2018](https://arxiv.org/abs/1707.07012)）
- 预测学习曲线。[Baker 等，2018](https://arxiv.org/abs/1705.10823) 对验证准确率的预测进行了建模，将其看作是一个时序回归问题。回归模型（$$\nu$$ 支持向量机回归；$$\nu$$-SVR）的特征包括各轮早期准确率序列，结构参数和超参数

#### 参数共享

相较于分别从头训练子模型，你可能会想，在他们中间构造依赖关系再找个方法复用权重怎么样？一些研究者还真这么做并成功了。

受 [Net2Net](https://arxiv.org/abs/1511.05641) 转换启发， [Cai 等，2017](https://arxiv.org/abs/1707.04873) 提出了 *高效结构搜索（Efficient Architecture Search，**EAS**）*。EAS 有个 RL 代理，称为元控制器，负责预测想保留功能需要进行的网络转换以便于增加网络深度或扩大层的宽度。因为网络是逐步增大的，之前验证过的网络的权重可以在后续探索中*复用*。有了继承来的权重，新建网络只需做些轻度训练即可。

元控制器会结合当前网络结构学习生成 *网络转换行为（network transformation action）*，行为用一个变长字符串表示。为了处理长度不定的结构配置，元控制器采用了双向循环网络的实现方式。几个演员网络会输出不同的转换决策：

1. *Net2WiderNet* 操作会换用更宽的层，也就意味着全连接层内单元变多，亦或卷积层里会有更多的过滤器，与此同时功用性不减。
2. *Net2DeeperNet* 操作会新插入一层，为保留功用性新层会被初始化成两层间的一个单位映射。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_EAS-meta-controller.png)

*图 14  高效结构搜索（EAS）中基于 RL 的元控制器一览。对结构配置编码后，它会通过两个独立演员网络输出 net2net 转换行为（图片来源：[Cai 等，2017](https://arxiv.org/abs/1707.04873)）*

受到类似的启发，高效 NAS（**ENAS**；[Pham 等，2018](https://arxiv.org/abs/1802.03268)）通过在子模型间大量共享参数来对 NAS 进行加速（小 1000 倍）。ENAS 核心洞见是它发现所有采样得到的结构图都能看作是一张更大的*超图（supergraph）*下的*子图（sub-graph）*。所有子网络都在共享该超图的权重。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_ENAS-example.png)

*图 15  （左）4 结点循环单元构成的完整搜索空间，但只有红色连接被激活（中）举例说明左侧激活子图怎么变成子模型结构（右）RNN 控制器给中间网络结构生成的网络参数（图片来源：[Pham 等，2018](https://arxiv.org/abs/1802.03268)）*

ENAS 会交替训练共享模型权重 $$\omega$$ 和控制器 $$\theta$$:

1.  LSTM 控制器参数 $$\theta$$ 用 [REINFORCE](https://libertydream.github.io/2020/07/19/策略梯度算法/#reinforce) 训练，在验证集上计算回报 $$R(\mathbf{m}, \omega)$$ 
2.  子模型的共享参数 $$\omega$$ 用标准监督学习损失训练。注意超图里相同结点的不同运算符都有自己的参数

#### 从预测出发

常规子模型的评价套路是用标准梯度下降更新模型权重。SMASH ([Brock 等，2017](https://arxiv.org/abs/1708.05344)) 有个不同的有趣想法：*能不能直接靠网络结构参数预测模型权重？*

他们结合模型结构配置的编码信息用 [HyperNet](https://blog.otoro.net/2016/09/28/hyper-networks/) ([Ha 等，2016](https://arxiv.org/abs/1609.09106)) 直接生成模型权重，然后直接带着 HyperNet 给的参数去验证模型。注意我们不必对每个子模型额外做训练，但得训练 HyperNet。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_SMASH-algorithm.png)

*图 16  SMASH 算法（图片来源：[Brock 等，2017](https://arxiv.org/abs/1708.05344)）*

带 SMASH 给定权重的模型性能和真实验证误差间的相关性，一定程度上可以用预测权重进行模型比较。我们需要一个容量足够大的 HyperNet，因为如果 HyperNet 模型尺寸比子模型小很多的话相关性就没了。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_SMASH-error-correlation.png)

*图 17  （左）误差相关性（右）HypterNet 模型大小与相关性的关系（图片来源：[Brock 等，2017](https://arxiv.org/abs/1708.05344)）*

SMASH 可以看作是实现参数共享的另一种方式。[Pham 等，2018](https://arxiv.org/abs/1802.03268) 指出 SMASH 有个问题是：因为权重是通过张量求积得到的，HyperNet 的使用会限制 SMASH 子模型给*低阶空间（low-rank space）*的权重。相对而言，ENAS 就没这个限制。

### 单尝试法：搜索+评估

为一众子模型分别进行搜索和评估代价太高，我们已经见识了像 [Brock 等，2017](https://arxiv.org/abs/1708.05344) 或 [Pham 等，2018](https://arxiv.org/abs/1802.03268) 提出的方法，单训练一个模型就足以模拟搜索空间中的任意子模型。

**单尝试（one-shot）**结构搜索进一步发展了共享权重的思想，进一步将结构生成的学习与权重参数结合在一起。下列方法全都将子结构的视作一张超图下的不同子图，超图里公共边的权重共享。

[Bender 等，2018](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf) 构建了一个很大的过参数化网络，称为**单尝试模型（One-Shot model）**，它包括搜索空间内所有可能的操作。借助 ScheduledDropPath（丢弃率随时间增长，训练终了达到 $$r^{1/k}$$，其中 $$0 < r < 1$$ 是个超参数而 $$k$$ 是抵达路径数）和一些精巧设计（比如 ghost 批归一化，只对激活结构做 L2 正则），这么大一个模型的训练会很稳定，可用于评估从超图中得来的任何子模型。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_one-shot-model-architecture.png)

*图 18  单尝试模型结构。每个单元有 $$N$$ 个选择块，每个选择块最多选 2 个运算。实线边每个结构都用，虚线边可选（图片来源：[Bender 等，2018](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf)）*

一旦单尝试模型训练好了，通过清空或删出某些运算，随机采样许多不同的结构喂给它进行性能评估。采样过程可以用 RL 或演化代替。

作者们观察到用单尝试模型测得的准确率，和对相同结做些微调后得到的准确率之间差异可能会非常大。他们猜单尝试模型会自动学会关注网络里 *最有用* 的操作并且变得只要这些操作可用就很 *依赖* 这些操作。所以清空有用操作会大幅降低模型准确率，而删掉些无关紧要的部分只会带来很小的影响——所以在用单尝试模型进行评估的时候得分方差会更大些。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_one-shot-model-accuracy-correlation.png)

*图 19  模型分级样例，不同的单尝试模型准确率与他们作为独立模型时的真实验证准确率（图片来源：[Bender 等，2018](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf)）*

很明显设计这样一个搜索图不是个普通活，但它确实展现了单尝试法的巨大潜力。只用梯度下降它的效果就很好，不需要像 RL 和 EA 这样的额外算法。

有些人认为 NAS 低效的一个主要原因是将结构搜索看成是一个 *黑箱优化* 问题，所以我们会陷入像 RL，演化，SMBO等方法泥潭之中。如果转而依靠标准梯度下降，有可能使搜索过程更高效。于是乎， [Liu 等，2019](https://arxiv.org/abs/1806.09055) 提出了可微分结构搜索（Differentiable Architecture Search，**DARTS**）。DARTS 对检索超图中的每条路都进行连续松弛（continuous relaxation），从而能通过梯度下降一并训练结构参数与权重。

这里用有向无环图（directed acyclic graph，DAG）进行表示。一个单元就是一个 DAG，由 $$N$$ 个结点构成的拓扑有序序列组成，每个结点都有一个隐表示 $$x_i$$ 要学，每条边 $$(i, j)$$ 都和某个操作 $$o^{(i,j)} \in \mathcal{O}$$ 挂钩，将 $$x_j$$ 转变为 $$x_i$$：


$$
x_i = \sum_{j < i} o^{(i,j)}(x_j)
$$


为保证搜索空间连续，DARTS 将特定操作的类别选择松弛为对所有操作进行 softmax，同时结构搜索的任务简化成学习一系列混合概率 $$\alpha = \{ \alpha^{(i,j)} \}$$。


$$
\bar{o}^{(i,j)}(x) = \sum_{o\in\mathcal{O}} \frac{\exp(\alpha_{ij}^o)}{\sum_{o'\in\mathcal{O}} \exp(\alpha^{o'}_{ij})} o(x)
$$


其中 $$\alpha_{ij}$$ 是个 $$\vert \mathcal{O} \vert$$ 维向量，记录着结点 $$i$$ 和 $$j$$ 间不同运算的权重。

因为我们既想优化网络权重 $$w$$ 又想使结构表示 $$\alpha$$ 最优，于是就面临一个双层最优化问题：


$$
\begin{aligned}
\min_\alpha & \mathcal{L}_\text{validate} (w^*(\alpha), \alpha) \\
\text{s.t.} & w^*(\alpha) = \arg\min_w \mathcal{L}_\text{train} (w, \alpha)
\end{aligned}
$$


在第 $$k$$ 步，给定当前结构参数 $$\alpha_{k−1}$$，先优化权重 $$w_k$$，以学习率 $$\xi$$ 将 $$w_{k−1}$$ 沿着训练损失最小的方向 $$\mathcal{L}_\text{train}(w_{k−1}, \alpha_{k−1})$$ 移动。接着固定刚更新完的权重 $$w_k$$，*对权重进行单步梯度下降计算后*，更新混合概率使验证损失最小化。


$$
J_\alpha = \mathcal{L}_\text{val}(w_k - \xi \nabla_w \mathcal{L}_\text{train}(w_k, \alpha_{k-1}), \alpha_{k-1})
$$


这么做源于我们想找一个低验证损失的结构，其权重经梯度下降取得最优，同时用单步展开权重*代替* $$w^∗(\alpha)$$。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_DARTS-illustration.png)

*图 20  DARTS 对 DAG 超图中的边进行连续松弛并确认最终模型（图片来源： [Liu 等，2019](https://arxiv.org/abs/1806.09055)）*


$$
\begin{aligned}
\text{Let }w'_k &= w_k - \xi \nabla_w \mathcal{L}_\text{train}(w_k, \alpha_{k-1}) & \\
J_\alpha &= \mathcal{L}_\text{val}(w_k - \xi \nabla_w \mathcal{L}_\text{train}(w_k, \alpha_{k-1}), \alpha_{k-1}) = \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) & \\
\nabla_\alpha J_\alpha 
&= \nabla_{\alpha_{k-1}} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) \nabla_\alpha \alpha_{k-1} + \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1})\nabla_\alpha w'_k & \\& \text{; 多变量链式法则}\\
&= \nabla_{\alpha_{k-1}} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) + \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) \big( - \xi \color{red}{\nabla^2_{\alpha, w} \mathcal{L}_\text{train}(w_k, \alpha_{k-1})} \big) & \\
&\approx \nabla_{\alpha_{k-1}} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) - \xi \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) \color{red}{\frac{\nabla_\alpha \mathcal{L}_\text{train}(w_k^+, \alpha_{k-1}) - \nabla_\alpha \mathcal{L}_\text{train}(w_k^-, \alpha_{k-1}) }{2\epsilon}} & \\
& \text{; 做数值微分近似}
\end{aligned}
$$


红字部分是在做数值微分近似，其中



$$w_k^+ = w_k + \epsilon \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1})$$  $$w_k^- = w_k - \epsilon \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1})$$.

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_DARTS-algorithm.png)

*图 21  DARTS 算法概览（图片来源： [Liu 等，2019](https://arxiv.org/abs/1806.09055)）*

还有一个想法和 DARTS 类似的，随机 NAS（Stochastic NAS；[Xie 等，2019](https://arxiv.org/abs/1812.09926)）用具象分布（具象指对离散随机变量的连续松弛；[Maddison 等，2017](https://arxiv.org/abs/1611.00712)）和再参数化技巧进行连续松弛，它的目标和 DARTS 相同，都是使离散分布可微从而可以用梯度下降做最优化。

DARTS 能大幅减少 GPU 耗时，作者做了 CNN 单元搜索实验，其中 $$N=7$$，用一块 GPU 只花了 1.5 天。但因为网络结构的连续性表示，它也有高 GPU 内存消耗的问题，为了将模型塞入单块 GPU 中，作者选了个小 $$N$$。

为了限制 GPU 内存开销，**ProxylessNAS** ([Cai 等，2019](https://arxiv.org/abs/1812.00332)) 将 NAS 看作是 DAG 中的一个路径剪枝过程，并使结构参数二值化，迫使每次在两结点间只有一条路被激活。一条边被遮蔽与否的概率会稍后学习，少采样些二值结构并用 *BinaryConnect* ([Courbariaux 等，2015](https://arxiv.org/abs/1511.00363)) 更新相应概率。 ProxylessNAS 表明 NAS 和模型压缩间关系密切，通过路径压缩它能将内存开销降低一个数量级。

接着回到图表示。在 DAG 邻接矩阵 $$G$$ 中，$$G_{ij}$$ 是结点 $$i$$ 和 $$j$$ 间的边，其值可从 $$\vert \mathcal{O} \vert$$ 个基本操作里挑，$$\mathcal{O} = \{ o_1, \dots \}$$。无论是单尝试模型， DARTS 还是 ProxylessNAS 都把各边看作是操作的混合体 $$m_\mathcal{O}(x)$$，只是略有不同。

单尝试模型中 $$m_\mathcal{O}(x)$$ 就是所有操作的和。DARTS 里则是加权和，对长为 $$\vert \mathcal{O} \vert$$ 的实值结构权重向量 $$\alpha$$ 求 softmax 获取权重。ProxylessNAS 把 $$\alpha$$ 的 softmax 概率变成二值门，并用二值门保证每次只激活一个操作。


$$
\begin{aligned}
m^\text{one-shot}_\mathcal{O}(x) &= \sum_{i=1}^{\vert \mathcal{O} \vert} o_i(x) \\
m^\text{DARTS}_\mathcal{O}(x) &= \sum_{i=1}^{\vert \mathcal{O} \vert} p_i o_i(x) = \sum_{i=1}^{\vert \mathcal{O} \vert} \frac{\exp(\alpha_i)}{\sum_j \exp(\alpha_j)} o_i(x) \\
m^\text{binary}_\mathcal{O}(x) &= \sum_{i=1}^{\vert \mathcal{O} \vert} g_i o_i(x) = \begin{cases}
o_1(x) & \text{概率 }p_1, \\
\dots &\\
o_{\vert \mathcal{O} \vert}(x) & \text{概率 }p_{\vert \mathcal{O} \vert}
\end{cases} \\
\text{ 其中 } g &= \text{binarize}(p_1, \dots, p_N) = \begin{cases}
[1, 0, \dots, 0] & \text{概率 }p_1, \\
\dots & \\
[0, 0, \dots, 1] & \text{概率 }p_N. \\
\end{cases}
\end{aligned}
$$



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_proxylessNAS-training.png)

*图 22  ProxylessNAS 两步训练交替进行（图片来源： [Cai 等，2019](https://arxiv.org/abs/1812.00332)）*

ProxylessNAS 交替进行两步训练：

1.  训练权重参数 $$w$$ 的时候，固定结构参数 $$\alpha$$ 并按上面的 $$m^\text{binary}_\mathcal{O}(x)$$ 对二值门 $$g$$ 随机采样，通过标准梯度下降更新权重参数。
2.  训练结构参数 $$\alpha$$ 的时候，固定 $$w$$，重置二值门然后在验证集上更新 $$\alpha$$。照着 BinaryConnect 的思路，结构参数的梯度可以近似用 $$\partial \mathcal{L} / \partial g_i$$  代替 $$\partial \mathcal{L} / \partial g_i$$ 进行估计：


$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \alpha_i} 
&= \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial p_j} \frac{\partial p_j}{\partial \alpha_i} 
\approx \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial g_j} \frac{\partial p_j}{\partial \alpha_i} 
= \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial g_j} \frac{\partial \frac{e^{\alpha_j}}{\sum_k e^{\alpha_k}}}{\partial \alpha_i} \\
&= \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial g_j} \frac{\sum_k e^{\alpha_k} (\mathbf{1}_{i=j} e^{\alpha_j}) - e^{\alpha_j} e^{\alpha_i} }{(\sum_k e^{\alpha_k})^2}
= \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial g_j} p_j (\mathbf{1}_{i=j} -p_i)
\end{aligned}
$$



不用 BinaryConnext 的话，REINFORCE 也能用来更新参数使回报最大化，而且也用不到 RNN 元控制器。

要算 $$\partial \mathcal{L} / \partial g_i$$ 得先计算并存好 $$o_i(x)$$，这要 $$\vert \mathcal{O} \vert$$ 倍的 GPU 内存。为解决这一难题，作者将从 $$N$$ 中选一拆解为多个二项选择问题（思路：“如果某条路是最好的，它应该比其他任意一条路都好”）。每次更新只采样两条路，其他的都遮蔽。选出来的两条路按上面的方程更新然后适度缩放保证其他路的权重不变。如此一来，其中一条路被增强（权重增加）而另一条被削弱（权重降低），其他路权重不变。

除开准确率，ProxylessNAS 将 *延迟* 也作为一项重要优化指标，毕竟不同设备对推理延迟的要求可能差别很大（如 CPU，GPU，移动设备等）。为使延迟可微，作者将延迟表示成网络维度的一个连续函数。混合操作的期望延迟可以写成 $$\mathbb{E}[\text{latency}] = \sum_j p_j F(o_j)$$，其中 $$F(.)$$ 是一个延迟预测模型：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_ProxylessNAS-latency.png)

*图 23  ProxylessNAS 在训练中加入可微分延迟损失（图片来源：[Cai 等，2019](https://arxiv.org/abs/1812.00332)）*

### 未来方向

目前为止我们讨论了很多有趣的想法，它们通过神经结构搜索使网络结构工程自动化，而且其中很多都已取得了十分不错的效果。但很难推断 *为什么* 某个结构效果好，我们怎么能开发出跨任务的通用组件而不是依赖特定数据集。

正如 [Elsken 等，2019](https://arxiv.org/abs/1808.05377) 中所言：

> “……，目前为止它很少告诉我们为什么特定结构效果很好，独立得出的结构为什么那么相似。找到通用样式，理清为什么那些样式对高性能很重要，并研究下这些样式能否泛化到不同问题上，这些都值得深思。”

同时，只关心验证准确率可能不太够了（[Cai 等，2019](https://arxiv.org/abs/1812.00332)）。像日常使用的移动电话这样的设备一般内存、算力都很有限。尽管 AI 应用对我们日常生活的影响越来越大，但也不可避免的越来越*吃设备（device-specific）*。

另一个有趣的方向是给 NAS 考虑下*无标注数据* 和自监督学习。有标注数据规模总是有限的，同时很难说数据集是否存在偏见，或者是否严重背离了真实数据分布。

[Liu 等，2020](https://arxiv.org/abs/2003.12056) 在思考这样一个问题，“*我们能不能不用人工注释标签找到高质量神经结构？*”，并提出了一套新的名为*无监督神经结构搜索*（*Unsupervised Neural Architecture Search*，**UnNAS**）的结构。结构效果需要在搜索阶段以无监督的方式进行估算。论文中用三个无监督代理任务（pretext task）做了实验：图像旋转预测，上色和拼图游戏。

作者在系列 UnNAS 实验中发现：

1. *相同数据集* 上有监督准确率和代理准确率的等级相关性很高。通常等级相关系数大于 0.8，无论数据集、搜索空间和代理任务如何
2. *跨数据集* 上有监督准确率和代理准确率间的等级相关性高企
3. 代理准确率越好意味着有监督准确率越好
4. UnNAS 结构的性能可以比肩有监督的对照组，尽管并没有更好

一个猜测是结构效果和图像统计信息有关。因为 CIFAR-10 和 ImageNet 都是自然图像，可比较且结果可迁移。UnNAS 可能无形中在搜索阶段引入了更多的无标注数据，更好的捕捉到了图像统计信息。

超参数搜索是 ML 社区永恒的话题。 NAS 实现了结构工程的自动化，我们可以逐步尝试将那些需要大量人工的 ML 流程自动化。更进一步讲，有没有可能自动发现 ML 算法呢？**AutoML-Zero** ([Real 等，2020](https://arxiv.org/abs/2003.03384)) 对此作了研究，通过陈化演进算法（aging evolutionary algorithms），AutoML-Zero 可以自动搜索全部 ML 算法，限制也很少，比如只用简单数学运算作构造块。

它要学的组件函数有三个，每个函数只用到了很基础的操作：

- `Setup`：初始化内存变量（权重）
- `Learn`：修改内存变量
- `Predict`：对输入 $$x$$ 做预测

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_AutoML-zero-evaluation.png)

*图 23  某任务的算法评估（图片来源：[Real 等，2020](https://arxiv.org/abs/2003.03384)）*

突变亲代基因型时考虑三类操作：

1. 在组件函数内的随机位置上随机插入或删除一条指令
2. 将组件函数内的指令全部随机化
3. 用随机选项替换某指令中的一个参数（比如“交换输出地址”或“修改常量值”）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_AutoML-zero-progress.png)

*图 24  CIFAR-10二分预估的演化过程（图片来源：[Real 等，2020](https://arxiv.org/abs/2003.03384)）*

### 附录：NAS 论文总结

|                           模型名称                           |          搜索空间          |                 搜索算法                  |            子模型评估             |
| :----------------------------------------------------------: | :------------------------: | :---------------------------------------: | :-------------------------------: |
| [NEAT (2002)](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf) |             -              |             演化（遗传算法）              |                 -                 |
|        [NAS (2017)](https://arxiv.org/abs/1611.01578)        |        顺次层向操作        |              RL (REINFORCE)               |         从头训练直到收敛          |
|      [MetaQNN (2017)](https://arxiv.org/abs/1611.02167)      |        顺次层向操作        | RL （结合 $$\epsilon$$-greedy 的 Q 学习） |            训练 20 轮             |
|       [HNAS (2017)](https://arxiv.org/abs/1711.00436)        |          分级结构          |            演化（锦标赛选拔）             |           迭代固定轮数            |
|      [NASNet (2018)](https://arxiv.org/abs/1707.07012)       |          基于单元          |                 RL (PPO)                  |            训练 20 轮             |
|     [AmoebaNet (2018)](https://arxiv.org/abs/1802.01548)     |      NASNet 搜索空间       |      演化（带陈化正则的锦标赛选拔）       |            训练 25 轮             |
|       [EAS (2018a)](https://arxiv.org/abs/1707.04873)        |          网络转换          |              RL (REINFORCE)               |             2 段训练              |
|       [PNAS (2018)](https://arxiv.org/abs/1712.00559)        |   NASNet 搜索空间删减版    |    SMBO; 逐步搜索复杂度越来越高的结构     |            训练 20 轮             |
|       [ENAS (2018)](https://arxiv.org/abs/1802.03268)        | 既顺次又基于单元的搜索空间 |              RL (REINFORCE)               |      用共享参数训练一个模型       |
|       [SMASH (2017)](https://arxiv.org/abs/1708.05344)       |         记忆库表示         |                 随机搜索                  | 由 HyperNet  预测被评定结构的权重 |
| [One-Shot (2018)](http://proceedings.mlr.press/v80/bender18a.html) |  一个过参数化的单尝试模型  |        随机搜索（随机消除些路径）         |          训练单尝试模型           |

| 模型名称                                                | 搜索空间        | 搜索算法                               |
| ------------------------------------------------------- | --------------- | -------------------------------------- |
| [DARTS (2019)](https://arxiv.org/abs/1806.09055)        | NASNet 搜索空间 | 梯度下降（取操作的 softmax 权重）      |
| [ProxylessNAS (2019)](https://arxiv.org/abs/1812.00332) | 树状结构        | 梯度下降（BinaryConnect）或  REINFORCE |
| [SNAS (2019)](https://arxiv.org/abs/1812.09926)         | NASNet 搜索空间 | 梯度下降 (具象分布)                    |


