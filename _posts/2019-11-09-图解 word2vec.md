---
layout:     post
title:      图解 Word2vec
subtitle:   计算机认识字词的方式
date:       2019-11-09
author:     一轩明月
header-img: img/post-bg-os-metro.jpg
catalog: 	 true
tags:
    - NLP
    - embedding
excerpt:    Word2vec 是 2013 年提出的一种高效创建词嵌入的方法，向量化效果很好。本文主要讲述何为嵌入以及使用 word2vec 获取嵌入的机制原理
---

> 编译自:
>
>  http://jalammar.github.io/illustrated-word2vec/ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec.png)

私以为嵌入（embedding）是机器学习领域最迷人的概念之一。如果你用过 Siri，谷歌翻译，MIUI，甚至是输入法里的输入提示，那你或许已经受益于这一思想了，而嵌入本身也已经是自然语言处理模型的核心了。近几十年嵌入在神经模型里的探索与应用有了长足进步，最新进展是前沿模型如 BERT 和 GPT-2 中的环境感知词嵌入。

Word2vec 是 2013 年提出的一种高效创建词嵌入的方法。但该方法除了用于构建词的嵌入表示，推荐引擎和一些非语言任务也借鉴了其思想，商业上的序列数据同样靠它变得更有意义。

本文主要讲述何为嵌入和使用 word2vec 获取嵌入的机制原理。在此之前，我们先熟悉一下用向量来描述事物的方式

### 个性嵌入化：你是个怎样的人？

你是内向还是外向？如果在 0 - 100 分中描述这一倾向，0 是完全内向，100 完全外向，你的分数是多少呢？不知道你是否做过类似 MBTI 这样的性格测试——进一步说，[五维职业性格测评](https://en.wikipedia.org/wiki/Big_Five_personality_traits )呢？没做过也不要紧，这些测试就是一系列问题，然后对你在各个维度上进行打分，内向/外向性是其中之一。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_big-five-personality-traits-score.png)

假设我在内向/外向性这一栏得了 38 分，那可以像这样画出来：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_introversion-extraversion-100.png)

转换成 -1 到 1 的区间里：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_introversion-extraversion-1.png)

只凭借这一条信息你能评定一个人怎么样吗？不行，人比这要复杂。让我们加一维——测试中另一栏的得分。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_two-traits-vector.png)

图中任何二维信息都可以表示为一个点，再好点可以用起点到该点的向量进行描述。处理向量我们有大量简单便捷的工具。

我有意去掉了所画特性的描述信息，只是为了让你习惯”不知道每个维度表示什么“这种状态——尽管从个性的向量化表示中依旧得到了大量信息。

现在这个向量部分表达了我的个性特质，如果你想将别人和我对比，这个向量就很有用了。比方说，我不小心被车撞了，需要换一个和我性格相似的人上来。下图中的两个人哪个和我更相似呢？

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_personality-two-persons.png)

计算向量间相似性的一个常用方法是余弦相似度：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_cosine-similarity.png)

第一个人和我性格更相近。指向相同方向的向量（长度也有用）间相似度更高。但两个维度依旧不足以描述人们之间的差异，数十年的心理学研究最终导向了五个特征（和众多子特征）。我们来看一下五个特征视角下的对比情况：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_big-five-vectors.png)

五维特征不再能够用二维的箭头形象化表示了，这也是机器学习领域的小障碍，我们通常都是在高维空间当中思考与寻找答案。好在余弦相似度在任意维度下都是有效的。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_embeddings-cosine-personality.png)

这一部分我们得到了两个核心观念：

1. 自然界中的事物（包括人）都可以用数值构成的特征向量来描述（利于机器处理）
2. 向量间的相似性是十分容易度量的

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_section-1-takeaway-vectors-cosine.png)

### 词嵌入

有了铺垫，就可以来看看训练用到的词向量（也叫词嵌入）和它们身上一些有趣的属性了。下面是在 Wikipedia 语料上训练得到的单词 “king” 的词嵌入

` [ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ] `

这是 50 个数构成的列表，直接看数字看不出什么来。可视化地和其他词比较一下，先将这些数字整理成一行：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_king-white-embedding.png)

再按照取值不同对其进行着色（接近 2 的为红，接近 0 的为白，接近 -2 的为蓝）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_king-colored-embedding.png)

接着省去数字，只保留上色效果，然后看一下 “king” 和其他单词间的对比：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_king-man-woman-embedding.png)

看到男人、女人间的相似性和二者与国王间的差异性了吗？你应该也感觉到了吧，这些向量捕捉到了一些词语间的微妙联系。再来看一组样本（垂直比较，看看那些颜色相同的列）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_queen-woman-girl-embeddings.png)

从中可以看出几点：

1. 有一列上所有词语的特征色都是红色。说明该维度下这些词都是一致的（我们并不知道每一个维度都意味着什么）
2. 可以看到“女人”和“女孩”是高度相似的。“男人”和“男孩”同样如此
3. “男孩”和“女孩”有些地方很相似却又和“男人”、“女人”不同，也许这些部分代表着“年轻”？说不准
4. 除了最后一个单词“水”其他单词都代表着人，于是你可以看到类别间的差异。尤其是那条垂直向下到“水”为止的蓝色线条
5. 有些地方“国王”与“王后”很像但和其他单词不同，这些维度是不是代表着“皇族”特性？

### 类比

词嵌入最知名的特性莫过于类比。我们可以对这些向量加加减减，得到的结果有时很有意思。声名显赫的例子就是：`”国王“-”男人“+”女人“≈”王后“`

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_king-man+woman-gensim.png)

上图是用 Python 调用  [Gensim](https://radimrehurek.com/gensim/) 库寻找相近单词时的情况，每一行代表一个相似单词和对应的余弦相似度。可视化一下：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_king-analogy-viz.png)

了解了词嵌入结果，就该看一下获取词向量的过程了。但在此之前，要聚焦于词嵌入的上一级概念：神经语言模型

### 语言模型

如果要举一个 NLP 应用的例子，输入法智能提示是一个很不错的选择。数十亿人每天都会用到这个功能成百上千次。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_swiftkey-keyboard.png)

这种输入预测的功能可以通过一个_语言模型_ 搞定。语言模型接收一串单词（比方说两个词），并试着预测下一个词会是什么。

截图里我们可以认为当前模型收到了两个标绿的词（thou shalt）并给出了一组输入提示（”not“的概率最高）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_thou-shalt-_.png)

可以将模型看作黑箱

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_language_model_blackbox.png)

但实际上模型并不只是输出一个单词，它真正返回的是对它所知道的所有单词（模型的”词表“，大小从几千到百万不等）的概率预测值。输入法要做的就是找到这些词中概率最大的那几个并呈现给用户。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_language_model_blackbox_output_vector.png)

早期模型经过训练后，会通过以下三步得到预测值：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_neural-language-model-prediction.png)

第一步就是我们在谈论的嵌入。训练得到的成果之一就是这个包含词表内所有单词嵌入的一个矩阵。到了预测阶段只需要从中查阅输入词的嵌入然后进行预测运算即可：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_neural-language-model-embedding.png)

接下来就来看看到底是经过怎样的步骤得到这个矩阵的吧

### 训练语言模型

语言模型相较于其他机器学习模型的一大优势在于，喂养模型的口粮——文字资料在网络上是随处可见的，是最肥沃的数据土壤。而其他模型往往是特定任务收集特定数据，或者是人工构建的特征。

>  *“You shall know a word by the company it keeps” J.R. Firth* 
>
> 不知其人视其友

词向量的获取是通过观察它经常出现在哪些词附近得到的。其中的机理是

1. 手头有大量文本数据
2. 当前文本上构建一个滑动的窗口
3. 滑动窗口为我们的模型创建训练样本

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_wikipedia-sliding-window.png)

随着窗口的滑动，模型的训练集也就构建好了。为明确个中道理，假设滑动窗口要处理下面这句话：

> ” Thou shalt not make a machine in the likeness of a human mind” ~Dune

一开始窗口位于句子头三个词的位置：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_lm-sliding-window.png)

将前两个词作为输入，第三个词作为输出标签

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_lm-sliding-window-2.png)

之后移动窗口到下一个词并创建第二个样本：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_lm-sliding-window-3.png)

很快我们就能获得更大的数据集，每个词都和不同的词对保持着共现关系。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_lm-sliding-window-4.png)

实际动手操作的时候，通常选择一边滑动窗口一边训练模型。只是个人认为把“数据集的构建”单拎出来逻辑会更清晰些。除了基于神经网络的方法，还有一种叫做 N-grams 的方法常被用于构建语言模型，其缺陷在于不能学到词语背后的潜藏信息，同时只能预测已经见过且语序相同的文本。

### 双向参考

凭你从上面了解到的知识来填个空：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_jay_was_hit_by_a_.png)

我（Jay）这里给你的是填空前的五个词（和之前提到的“bus”）。很多人都会猜空白处应该填入“公交车（bus）“。但如果我额外再给一条信息——空格后面的一个词，你又会填什么呢（看看前面的图）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_jay_was_hit_by_a_bus.png)

这一信息彻底改变了空白处的填充内容。目前看来，”红色的（red）“才是最有可能的选项。从这个例子可以看出一个单词的前后”邻居“们都蕴含着有价值的信息。结果证明参考两个方向（猜测位置左右两边的单词）可以得到更好的词嵌入效果。据此来看一下我们该怎么调整训练过程。

### 跨位（skipgram）

相较于只看前面两个单词，也可以同时参考下预测位置后面的两个单词是什么。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_continuous-bag-of-words-example.png)

这时，所构建的数据集会是这个样子：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_continuous-bag-of-words-dataset.png)

这被称为**连续词袋(Continous Bag of Words)** 结构，由论文  [one of the word2vec papers](https://arxiv.org/pdf/1301.3781.pdf) 提出。另一种结构效果不错但做法不太一样。

相较于通过上下文（左右单词）猜测当前位置的单词，另一种架构试图用当前单词猜测邻近词会是什么。试想滑动窗口某一时刻的状态如下：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-sliding-window.png)

绿色区域是输入词，粉色格子则是可能的输出。粉色格子颜色深浅不同是因为滑动窗口实际创建了四个独立样本：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-sliding-window-samples.png)

这被称为**跨位（skipgram)** 结构。可视化滑动窗口的工作流程会是这样子：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-sliding-window-1.png)

添加四个样本到训练集中

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-sliding-window-2.png)

移动窗口到下一个位置：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-sliding-window-3.png)

这样就又得到了四个样本

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-sliding-window-4.png)

就这样滑过若干位置后，会获得大量样本：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-sliding-window-5.png)

### 训练过程回顾

现在已经从文本上通过跨位采集到了数据集，来看一下怎么使用其训练基本的神经语言模型以预测邻近单词。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-language-model-training.png)

从第一个样本开始。抓取特征送入未训练的模型得到初始预测单词。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-language-model-training-2.png)

模型走完三步流程输出预测向量（词表中的每个词都有相应概率值）。因为还没训练过，自然此时预测效果不会好。不过还好，我们知道它猜哪个词才是正确的——打标单元所在行

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-language-model-training-3.png)

模型偏差了多少呢？可以将两个向量作差得到偏差向量：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-language-model-training-4.png)

偏差向量用于模型更新，下一次输入 `not` 的时候最可能的输出就是 `thou` 了。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-language-model-training-5.png)

这样子第一步就结束了。之后就是不断从数据集中取出样本进行处理，直到样本集为空。这叫做完成了**一轮（epoch）**训练。重复训练几轮后就得到了训练好的模型，可以从中抽取出嵌入矩阵并应用在其他任务上。

但尽管上述内容已经扩宽了对处理过程了理解，仍不是当下 word2vec 实际训练的形式。还有些关键概念。

### 负采样

回顾神经模型计算预测值走过的三步：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_language-model-expensive.png)

第三步的计算代价太过高昂——数据集中的每个样本都要全算一边。需要做点什么改善这一过程。一个思路是将我们的目标拆成两步：

1. 生成高质量词嵌入
2. 使用词向量训练语言模型

这里重点看下第一步。为了通过高质量模型得到高质量的词嵌入，可以将原来直接预测近邻：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_predict-neighboring-word.png)

转换成接收一组输入输出，然后预测输入单词和输出词间是近邻的概率（0 代表”非近邻“，1 代表”近邻“）。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_are-the-words-neighbors.png)

这一转换直接将模型从神经网络改成了逻辑回归，从而大幅简化计算任务，提高了计算速度。

而这种转换要求数据集结构也得变。标签列此时就是一列 0，1值了，因为我们添加的词都是近邻所以取值都是1。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-training-dataset.png)

现在计算速度就很可观了，但有个漏洞需要补上。如果所有样本都是正样本（标记为1），那得到的只是自大的只会返回 1 的模型——100% 准确率可什么也没学到，只会生成垃圾嵌入。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-smartass-model.png)

要矫正这点，需要引入_负样本（negative samples）_ 到数据集中，这些样本中的词语间不是近邻。模型对这些样本的返回值为 0。这无疑会对模型构成挑战但仍然速度很快。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-negative-sampling.png)

那该填入哪些词作为输出词呢？可以从词表中随机挑选出单词填入其中。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-negative-sampling-2.png)

这一想法为论文 [Noise-contrastive estimation](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf) 所支持。将实际信号（近邻的正样本）和噪声（非近邻单词中随机挑选）进行比对。这是对计算和统计效率的权衡。

### 负采样跨位

至此我们就讲完了 word2vec 中的两个核心概念，作为一体，称其为负采样跨位（SGNS，skipgram with negative sampling)。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_skipgram-with-negative-sampling.png)

### word2vec 训练过程

有了跨位和负采样的认知基础，就可以进一步贴近实际中的 word2vec 训练过程。

训练开始前，对模型训练用到的文本进行预训练。这一步，要定好词表大小（称为 vocab_size，比如 10000个）和其中会有哪些词。

训练开始阶段，创建两个矩阵——嵌入矩阵和上下文矩阵。两个矩阵对词表中的每个词都有对应词向量（vocab_size 是维度之一）。另一个维度是词向量的长度（embedding_size，300 是常见值）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-embedding-context-matrix.png)

将这些矩阵初始化为随机值后就可以开始训练了。每次训练选择一个正样本和相关负样本。来看第一组：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-training-example.png)

现在手头有四个词：`word`是输入，输出/上下文：`thou`(真实信号)，aaron 和 taco（负样本）。之后查找它们的嵌入——对输入单词来说从嵌入矩阵中找。上下文单词从上下文矩阵中选取。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-lookup-embeddings.png)

接着就要将输入嵌入和每个上下文嵌入作点积。本例中输出结果是一个数字，表示输入嵌入和上下文嵌入间的相似性。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-training-dot-product.png)

现在我们需要某种方法将这些分数转换成概率——正数且介于 0，1之间。这很适合于 [sigmoid 运算](https://jalammar.github.io/feedforward-neural-networks-visual-interactive/#sigmoid-visualization)。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-training-dot-product-sigmoid.png)

sigmoid 的运算结果就是模型对这些样本的处理结果。可以看到 `tacao`分值最高而`aaron`运算前后得分都是最低的。

有了未训练模型的预测值，手头又有实际目标值做对比，可以算一下模型预测偏差。将 sigmoid 分值与真实标签值作差。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-training-error.png)

由此来到了”机器学习“中的”学习“部分。可以使用偏差值调整4个词的嵌入，保证下次计算时结果更接近于目标值。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-training-update.png)

一步训练（training step）就完成了，对其中涉及到的词得到了更好的嵌入。开始下一步计算（下一个正样本和相关负样本），不断重复。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-training-example-2.png)

在数据集上反复跑几次，嵌入效果会越来越好。可以停止训练，丢掉上下文矩阵，将嵌入矩阵作为下一个任务中的预训练嵌入。

### 窗口大小和负样本数

word2vec 中的两个关键超参数是窗口大小和负样本数。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-window-size.png)

任务不同，窗口大小也不同。一点[启示](https://youtu.be/tAxrlAVw-Tk?t=648)是较小的窗口（2-15）得到的高相似得分嵌入，通常表示词语间可以替换（如果只看周围单词情况，反义词经常是能互换的，比如好和坏的语句往往很相似）。更大的窗口（15-50，乃至更多）得到的嵌入更多表现的是词语间的相关性。实际上，你通常得提供注释来指导嵌入过程以保证嵌入对当前任务有效。Gensim 库默认窗口大小为 5（前面两个，后面两个）。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-09_word2vec-negative-samples.png)

负样本采样数量是另一个要素。原始论文认为 5-20 个是不错的选择，也说如果你的数据集足够大 2-5 个效果就很好了。Gensim 默认是 5 个负样本。