---
layout:		post
title:  	深度学习模型没有过拟合的秘密
subtitle:   
date:       2020-06-28
author:     一轩明月
header-img: img/post-bg-universe.jpg
catalog:    true
tags:
    - opinions
excerpt:    如果你也曾好奇，为什么深度神经网络能泛化应用到外部数据上，却没有出现过度拟合，读下去。
---

> 编译自：Are Deep Neural Networks Dramatically Overfitted，[Lilog](https://lilianweng.github.io/lil-log/)

如果你像我一样，带着传统机器学习的经验进入深度学习领域，可能会经常沉思这一问题：既然经典深度神经网络有这么多参数并且训练误差很容易臻至完美，它应该不无意外地出现严重的过拟合才对，它又怎么能对样本外的数据点具备泛化能力的呢？

思考为什么深度神经网络具备泛化能力的过程不知怎得使我想起 System Biology 上一篇有趣的论文——[“生物学家能修收音机吗？”](https://www.cell.com/cancer-cell/pdf/S1535-6108(02)00133-2.pdf)。如果一名生物学家打算像她研究生物系统一样去修理无线电机器，日子可能会很难过。因为整个无线电系统的机理没有显露出来，外露的局部小功能可能会给点提示但没什么可能呈现所有系统内部的交互与连接，更别提整体工作流了。无论你是否认为它和 DL 相关，都值得一读。

本文会介绍一些深度学习模型泛化能力和复杂度测定相关的研究工作。希望这有助于你理解为什么 DNN 能泛化。

# 压缩和模型选择的经典定理

假定我们手头有个分类问题和一个数据集，自然可以写很多模型来进行解决，从用线性回归的简单拟合到占用磁盘空间记忆整个数据集。哪个更好？如果只关心训练数据上的准确率（尤其是当测试数据大概率未知的时候），记忆法似乎最好——好吧，听起来有点不对。

实际在这种情境下，一个好模型要具备哪些类型的品质有很多经典定理可以给予指导。

## 奥卡姆剃刀

[奥卡姆剃刀](https://zh.wikipedia.org/wiki/奥卡姆剃刀)是一个寻求答案时的非正式原则，由[奥卡姆](https://zh.wikipedia.org/wiki/奥卡姆的威廉)在 14 世纪提出：

> 简洁的解决方案大概率比复杂的更好

当我们有众多解释世界的候选底层理论而必须从中选取一个时，该表述强而有力。对某个问题而言太多假设可能看似合理，但很难泛化到其它复杂情景，乃至最终通向宇宙基本法则。

想想看，人们花了数百年时间才明白白天天空是蓝色，落日时却是红色完全是因为同样的原因（[瑞利散射](https://en.wikipedia.org/wiki/Rayleigh_scattering)），尽管直观上两个现象看起来是那么不同。人们为此独立提出了很多其他解释，简洁版最终胜出。

## 最短描述长度原则

相应的奥卡姆剃刀同样能用在机器学习模型上，该理念正式化地说法是 _最短描述长度（Minimum Description Length，MDL）_ 原则，用来在给定观测数据的时候对模型/解释进行比较。

>理解就是压缩

MDL 的基本思想是 _把学习看作是数据压缩过程_ 。通过压缩数据，我们需要从数据中找到规律性或者模式，而且大概率能将他们用到未知数据上。[信息瓶颈理论](https://lilianweng.github.io/lil-log/2017/09/28/anatomize-deep-learning-with-information-theory.html)认为深度神经网络先是通过最小化泛化误差进行学习表示数据，然后通过去除噪声凝练这种表示。

与此同时，MDL 认为模型描述也是压缩交付的一部分，所以模型不能太大。

二分版 MDL 原则描述为： $$\mathcal{H}^{(1)}, \mathcal{H}^{(2)}, \dots$$ 是一组能解释数据集 $$\mathcal{D}$$ 的模型，其中最好的假设应当最小化求和值：
$$
\mathcal{H}^\text{best} = \arg\min_\mathcal{H} [L(\mathcal{H}) + L(\mathcal{D}\vert\mathcal{H})]
$$

- $$L(\mathcal{H})$$ 是描述模型 $$\mathcal{H}$$ 的比特长度
- $$L(\mathcal{D}\vert\mathcal{H})$$ 是用 $$\mathcal{H}$$ 对数据 $$\mathcal{D}$$  进行编码描述时的比特长度

简而言之，*最好* 的模型是包括编码数据和其自身的 *最小* 模型。照此来讲，不管在训练集上达到的效果如何，章节开头提到的记忆法听起来都有点恐怖了。

有人可能会说奥卡姆剃刀不对，毕竟真实世界的复杂度飘忽不定，为什么必须找到简洁模型呢？ MDL 的一个有趣视角是将模型看作是**“语言”**而不是基础性的生成原理。我们是想找到好的压缩策略来描述小样本集中的规律性，同时为了解释现象**他们并不非得是“真正”的生成模型**。模型可能是错的但仍旧有用（想想贝叶斯先验）。

### 柯氏复杂性

[柯氏复杂性](https://zh.wikipedia.org/wiki/柯氏复杂性)借助现代计算机的概念来定义一个对象的算法（描述性的）复杂度：*描述对象的二进制计算机程序的最短长度*。按照 MDL 的观点，计算机本质上就是数据解码器最一般的形式。

柯氏复杂度的正式定义表述为：给定通用计算机 $$\mathcal{U}$$ 和程序  $$p$$ ，令 $$\mathcal{U}(p)$$ 为计算机处理程序的输出，而 $$L(p)$$ 是程序的描述长度。那么就计算机 $$\mathcal{U}$$ 来说，字符串 $$s$$ 的柯氏复杂性  $$K_\mathcal{U}$$ 为：
$$
K_\mathcal{U}(s) = \min_{p: \mathcal{U}(p)=s} L(p)
$$
注意通用计算机是说能模仿任意其他计算机行为的计算设备。现代计算机都是共同的，因为它们全都可以叫做图灵机。无论用的是哪台计算机，该定义都是适用的，因为另一台通用计算机总能编程克隆 $$\mathcal{U}$$ 的行为，而克隆程序的编码是静态不变的。

柯氏复杂性和香农信息论之间有很多关联，因为他们都和通用编码相关。令人惊讶的是一个随机变量的柯氏复杂性近乎等于它的信息熵（见[报告](https://homepages.cwi.nl/~paulv/papers/info.pdf) 2.3 节），有关于此的更多内容超出了本文范围，但网上有很多有趣的内容，请自便。

 $$L(\mathcal{H})$$ 



 $$\mathbb{R}^n$$ 



 $$C$$  $$n$$ in $$d$$  $$S \subseteq \mathbb{R}^d$$ with $$\vert S \vert = n$$  $$f: S \mapsto \mathbb{R}$$  $$C$$ so that $$C(\boldsymbol{x}) = f(\boldsymbol{x}), \forall \boldsymbol{x} \in S$$

>  $$2n + d$$  $$n$$ in $$d$$ 

 $$C: \mathbb{R}^d \mapsto \mathbb{R}$$.   $$d$$  $$\boldsymbol{x} \in \mathbb{R}^d$$  $$h$$   $$\mathbf{W} \in \mathbb{R}^{d\times h}$$,$$-\mathbf{b} \in \mathbb{R}^h$$   $$\boldsymbol{v} \in \mathbb{R}^h$$ 

 $$C$$  $$\boldsymbol{x}$$ 
$$
C(\boldsymbol{x}) 
= \boldsymbol{v} \max\{ \boldsymbol{x}\mathbf{W} - \boldsymbol{b}, 0\}^\top
= \sum_{i=1}^h v_i \max\{\boldsymbol{x}\boldsymbol{W}_{(:,i)} - b_i, 0\}
$$
$$\boldsymbol{W}_{(:,i)}$$  $$i$$  $$d \times h$$ 

t $$S = \{\boldsymbol{x}_1, \dots, \boldsymbol{x}_n\}$$   $$\boldsymbol{y} = \{y_1, \dots, y_n \}$$,  $$\mathbf{W} \in \mathbb{R}^{d\times h}$$, $$\boldsymbol{b}, \boldsymbol{v} \in \mathbb{R}^h$$  $$C(\boldsymbol{x}_i) = y_i, \forall i=1,\dots,n$$

 $$\mathbf{X} \in \mathbb{R}^{n \times d}$$.  $$h=n$$, $$\mathbf{X}\mathbf{W} - \boldsymbol{b}$$  $$n \times n$$.
$$
\mathbf{M}_\text{ReLU} 
= \max\{\mathbf{X}\mathbf{W} - \boldsymbol{b}, 0 \} 
= \begin{bmatrix}
\boldsymbol{x}_1\mathbf{W} - \boldsymbol{b} \\
\dots \\
\boldsymbol{x}_n\mathbf{W} - \boldsymbol{b} \\
\end{bmatrix}
= [\boldsymbol{x}_i\boldsymbol{W}_{(:,j)} - b_j]_{i \times j}
$$
 $$\mathbf{W}$$ 
$$
\mathbf{W}_{(:,j)} = \boldsymbol{w} \in \mathbb{R}^{d}, \forall j = 1, \dots, n
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/20200804_nn-expressivity-proof.png)

 $$a_i = \boldsymbol{x}_i \boldsymbol{w}$$,  $$\boldsymbol{w}$$ and $$\boldsymbol{b}$$  $$b_1 < a_1 < b_2 < a_2 < \dots < b_n < a_n$$  $$n+d$$  $$\boldsymbol{x}_i$$  $$\boldsymbol{w}$$, sort $$\boldsymbol{x}_i \boldsymbol{w}$$ $$b_j$$ $$\mathbf{M}_\text{ReLU}$$
$$
\mathbf{M}_\text{ReLU} = [a_i - b_j]_{i \times j}
= \begin{bmatrix}
a_1 - b_1 & 0        & 0  & \dots & 0 \\
\vdots &  \ddots  & &  & \vdots \\
a_i - b_1 & \dots & a_i - b_i & \dots & 0\\
\vdots &    & & \ddots & \vdots \\
a_n - b_1 & a_n - b_2 & \dots & \dots & a_n - b_n \\
\end{bmatrix}
$$


![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_fit-random-labels.png)

$$\det(\mathbf{M}_\text{ReLU}) \neq 0$$  $$\boldsymbol{v}$$  $$\boldsymbol{v}\mathbf{M}_\text{ReLU}=\boldsymbol{y}$$   $$\mathbf{M}_\text{ReLU}$$  $$\mathbb{R}^n$$  $$\boldsymbol{y}$$)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_bias-variance-risk-curve.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_new-bias-variance-risk-curve.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_new-risk-curve-mnist.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_regularization-generalization-test.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_optimization-landscape-shape.png)

$$D$$  $$\theta^{(D)}$$.$$d$$ $$\theta^{(d)}$$, $$d < D$$.  $$D$$  $$\theta^{(d)}$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_intrinsic-dimension-illustration.png)

_ $$D=3$$ _
$$
\theta^{(D)} = \theta_0^{(D)} + \mathbf{P} \theta^{(d)}
$$
 $$\theta_0^{(D)}$$  $$\mathbf{P}$$ is a $$D \times d$$   $$\theta_0^{(D)}$$ and $$\mathbf{P}$$  $$\theta^{(d)}$$ 

 $$d = 1, 2, \dots, D$$,  $$d$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_intrinsic-dimension.png)

_ $$d$$  _

 $$\ell$$ , $$\ell = 1, \dots, L$$,  $$t$$, $$\theta^{(\ell)}_t$$ 

-  $$\theta^{(\ell)}_t \leftarrow \theta^{(\ell)}_0$$. $$\ell$$.
- $$\theta^{(\ell)}_t \leftarrow \tilde{\theta}^{(\ell)} \sim \mathcal{P}^{(\ell)}$$.  $$\ell$$.

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_layer-robustness-results.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_layer-robustness-resnet.png)

1.  $$\theta_0$$;
2.  $$\theta$$;
3.  $$\theta$$  $$m$$.
4.  $$m \odot \theta_0$$. 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-04_risk_curve_loss-mse_sample-4000_epoch-500.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/20200804_layer_equality_256x3.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/20200804_layer_equality_128x4.png)

 $$d$$  $$\mathbf{P}$$  $$\mathbf{P}\theta^{(d)}$$  $$\mathbf{P}$$  $$d$$  $$\sum_{i=1}^d \theta^{(d)}_i \mathbf{P}^\top_{(:,i)}$$,  $$\mathbf{P}$$.

 $$d$$, 

 $$d$$

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/20200804_intrinsic-dimension-net-64-64-and-128.png)