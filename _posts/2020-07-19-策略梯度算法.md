---
layout:		post
title:  	策略梯度算法
subtitle:   
date:       2020-07-19
author:     一轩明月
header-img: img/post-bg-ai.jpg
catalog:    true
tags:
    - reinforcement learning
excerpt:    本文深入介绍策略梯度，为什么有效，和近些年提出的许多新策略梯度算法：普通策略梯度，演员-评论家，非策略演员-评论家，A3C，A2C,DPG,DDPG,D4PG,MADDPG,TRPO,PRO,ACER,ACTKR,SAC,TD3 以及 SVPG
---

> 编译自：Policy Gradient Algorithms， [Lilian Weng](https://lilianweng.github.io/lil-log/)

### 什么是策略梯度

策略梯度是一种求解强化学习问题的方法，如果你未曾涉猎过相关领域内容，请先看一下“[强化学习入门——关键概念](https://libertydream.github.io/2020/07/05/强化学习综述/#关键概念)”一节，了解一下问题定义以及核心概念。

#### 符号说明

下面的符号说明表能降低理解文中方程的难度。

| 符号                    | 含义                                                         |
| ----------------------- | ------------------------------------------------------------ |
| $$s \in \mathcal{S}$$   | 状态                                                         |
| $$a \in \mathcal{A}$$   | 行动                                                         |
| $$r \in \mathcal{R}$$   | 回报                                                         |
| $$S_t, A_t, R_t$$       | 过程中 $$t$$ 时刻的状态，行动和回报。可能也偶尔会用 $$s_t, a_t, r_t$$ |
| $$\gamma$$              | 折扣因子；对未来回报不确定性的惩罚； $$0<\gamma \leqslant 1$$. |
| $$G_t$$                 | 收益；或折扣回报； $$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$ |
| $$P(s', r \vert s, a)$$ | 从当前状态为 $$s$$，行动为 $$a$$，回报为 $$r$$ 的条件下转移到下一状态 $$s'$$ 的概率 |
| $$\pi(a \vert s)$$      | 随机策略（代理人行动策略）； $$\pi_\theta(.)$$ 是参数为 $$\theta$$ 的策略 |
| $$\mu(s)$$              | 既定策略；也可以记为 $$\pi(s)$$，但用个不同的字母能提高辨识度，不需要过多解释就能判断策略是随机的还是确定的。不管是用 $$\pi$$ 还是 $$\mu$$ 都是强化学习算法的学习目标 |
| $$V(s)$$                | 状态价值函数，代表状态 $$s$$ 的预期收益； $$V_w(.)$$ 是参数为 $$w$$ 的状态价值函数 |
| $$V^\pi(s)$$            | 按策略 $$\pi$$ 走时状态 $$s$$ 的价值；$$V^\pi (s) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s]$$ |
| $$Q(s, a)$$             | 行动价值函数，类似 $$V(s)$$，只是它评估的是一对状态和行动 $$(s, a)$$ 的预期收益； $$Q_w(.)$$ 是参数为 $$w$$ 的价值函数 |
| $$Q^\pi(s, a)$$         | 类似于 $$V^\pi(.)$$，策略为 $$\pi$$ 时（状态，行动）对的价值; $$Q^\pi(s, a) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s, A_t = a]$$ |
| $$A(s, a)$$             | 优势函数, $$A(s, a) = Q(s, a) - V(s)$$；可以看作是另一个版本的 Q 值，选定状态价值做基准从而方差更低 |

#### 策略梯度

强化学习的目标是给代理人找到最佳行动策略，从而最大化所得回报。**策略梯度（policy gradient）**法希望直接对策略建模，并直接优化策略，一般是用拿 $$\theta$$ 做参数的一个函数对策略建模，$$\pi_\theta(a \vert s)$$。由此得到回报价值（目标）函数，然后可以用各种算法优化 $$\theta$$ 使收益最大化。

回报函数定义为：
$$
J(\theta) 
= \sum_{s \in \mathcal{S}} d^\pi(s) V^\pi(s) 
= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a)
$$
其中 $$d^\pi(s)$$ 是马尔可夫链 $$\pi_\theta$$（ $$\pi$$ 下的策略性状态分布）的静态分布。简单起见，当其他函数角标里的策略为 $$\pi_\theta$$ 时省去参数 $$\theta$$ ；举例来说，$$d^{\pi}$$ 和 $$Q^\pi$$ 写全了应该是 $$d^{\pi_\theta}$$ 和 $$Q^{\pi_\theta}$$。

试想你沿着状态间的马尔可夫链一直走，最终随着时间推移，你落在某一状态上的概率不再变化——这就是 $$\pi_\theta$$ 的静态概率。$$d^\pi(s) = \lim_{t \to \infty} P(s_t = s \vert s_0, \pi_\theta)$$ 是从 $$s_0$$ 起，按策略 $$\pi_\theta$$ 走 $$t$$ 步后 $$s_t=s$$ 的概率。实际上 PageRank 算法之所以有效的一大原因就是因为马尔可夫链存在静态分布，感兴趣的话可以看看这篇[文章](https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/)。

很自然的会认为基于策略的方法在连续空间内更有用。因为有无尽的行动和状态有待估计价值，也因此基于价值的方法在连续空间内的计算开销高到天上去了。比如在广义策略迭代中，策略改进步骤 $$\arg\max_{a \in \mathcal{A}} Q^\pi(s, a)$$ 需要遍历整个行动空间，经受[维度诅咒](https://en.wikipedia.org/wiki/Curse_of_dimensionality)的折磨。

借助 *梯度上升* 法，可以按梯度 $$\nabla_\theta J(\theta)$$ 所示方向移动 $$\theta$$ 来寻找能带来最大收益的 $$\pi_\theta$$ 所对应的 $$\theta$$。

#### 策略梯度定理

计算梯度 $$\nabla_\theta J(\theta)$$ 会有些棘手，因为既要看行动选择（直接由 $$\pi_\theta$$ 决定），还要看既定行为下状态的静态分布（由 $$\pi_\theta$$ 间接决定）。考虑到一般情况下并不清楚环境状况，很难估计策略更新给状态分布带来的影响。

所幸还有**策略梯度定理**能帮我们。定理给出了一个目标函数导数的优秀变式，不涉及状态分布 $$d^\pi(.)$$ 的导数，极大简化了梯度 $$\nabla_\theta J(\theta)$$ 的计算过程。
$$
\begin{aligned}
\nabla_\theta J(\theta) 
&= \nabla_\theta \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \\
&\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) 
\end{aligned}
$$

#### 策略梯度定理证明

这一节会有些晦涩难懂，我们要看下证明过程（[Sutton 与 Barto, 2017](http://incompleteideas.net/book/bookdraft2017nov5.pdf); 章节13.1），弄清楚为什么策略梯度定理是正确的。

让我们从状态价值函数的导数开始：
$$
\begin{aligned}
& \nabla_\theta V^\pi(s) \\
=& \nabla_\theta \Big(\sum_{a \in \mathcal{A}} \pi_\theta(a \vert s)Q^\pi(s, a) \Big) & \\
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta Q^\pi(s, a)} \Big) & \scriptstyle{\text{；复合函数求导规则}} \\
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta \sum_{s', r} P(s',r \vert s,a)(r + V^\pi(s'))} \Big) & \scriptstyle{Q^\pi \text{ 分成当前回报与未来回报}} \\
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s', r} P(s',r \vert s,a) \nabla_\theta V^\pi(s')} \Big) & \scriptstyle{P(s',r \vert s,a) \text{ 或 } r \text{ 都与 }\theta \text{ 无关}}\\
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s'} P(s' \vert s,a) \nabla_\theta V^\pi(s')} \Big) & \scriptstyle{\text{；因为 }  P(s' \vert s, a) = \sum_r P(s', r \vert s, a)}
\end{aligned}
$$

于是得到

$$
\color{red}{\nabla_\theta V^\pi(s)} \color{black}= \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \sum_{s'} P(s' \vert s,a) \color{red}{\nabla_\theta V^\pi(s')}\color{black} \Big)
$$
方程有良好的递归性（看红色部分），未来状态价值函数 $$V^\pi(s')$$ 可以按相同方式不断展开。

考虑如下探访序列，策略 $$\pi_\theta$$ 下走 $$k$$ 步后从状态 $$s$$ 到状态 $$x$$ 的概率记为  $$\rho^\pi(s \to x, k)$$：
$$
s \xrightarrow[]{a \sim \pi_\theta(.\vert s)} s' \xrightarrow[]{a \sim \pi_\theta(.\vert s')} s'' \xrightarrow[]{a \sim \pi_\theta(.\vert s'')} \dots
$$

- 当 $$k=0$$ 时，$$\rho^\pi(s \to s, k=0) = 1$$.
- 当 $$k=1$$，考察所有可能行动并计算到目标状态的转移概率：$$\rho^\pi(s \to s', k=1) = \sum_a \pi_\theta(a \vert s) P(s' \vert s, a)$$.
-  目标是按策略 $$\pi_\theta$$ 走 $$k+1$$ 步从状态 $$s$$ 到 $$x$$，可以先走 $$k$$ 步从 $$s$$ 到中间点 $$s'$$（任何状态都可以是中间点  $$s' \in \mathcal{S}$$） 然后最后一步抵达最终状态 $$x$$。这样我们可以递归更新访问概率：$$\rho^\pi(s \to x, k+1) = \sum_{s'} \rho^\pi(s \to s', k) \rho^\pi(s' \to x, 1)$$

回到 $$\nabla_\theta V^\pi(s)$$ 的递归展开式，令 $$\phi(s) = \sum_{a \in \mathcal{A}} \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a)$$ 简化一下数学表达。如果无限的将 $$\nabla_\theta V^\pi(.)$$ 拓展下去，容易看出按这个递归过程并记录所有访问概率，可以从起始状态经走几步转移到任意状态，这就得到了 $$\nabla_\theta V^\pi(s)$$! 
$$
\begin{aligned}
& \color{red}{\nabla_\theta V^\pi(s)} \\
=& \phi(s) + \sum_a \pi_\theta(a \vert s) \sum_{s'} P(s' \vert s,a) \color{red}{\nabla_\theta V^\pi(s')} \\
=& \phi(s) + \sum_{s'} \sum_a \pi_\theta(a \vert s) P(s' \vert s,a) \color{red}{\nabla_\theta V^\pi(s')} \\
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \color{red}{\nabla_\theta V^\pi(s')} \\
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \color{red}{[ \phi(s') + \sum_{s''} \rho^\pi(s' \to s'', 1) \nabla_\theta V^\pi(s'')]} \\
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \phi(s') + \sum_{s''} \rho^\pi(s \to s'', 2)\color{red}{\nabla_\theta V^\pi(s'')}\color{black} \scriptstyle{ \text{ ；}s'\text{ 是 }s \to s'' \text{ 的中间点}}\\
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \phi(s') + \sum_{s''} \rho^\pi(s \to s'', 2)\phi(s'') + \sum_{s'''} \rho^\pi(s \to s''', 3)\color{red}{\nabla_\theta V^\pi(s''')} \\
=& \dots \scriptstyle{\text{; 不断将 }\nabla_\theta V^\pi(.)}\text{ 展开} \\
=& \sum_{x\in\mathcal{S}}\sum_{k=0}^\infty \rho^\pi(s \to x, k) \phi(x)
\end{aligned}
$$
有了这个变式就能丢掉 Q 值函数的导数了 $$\nabla_\theta Q^\pi(s, a)$$，将其插入目标函数 $$J(\theta)$$ 得到：
$$
\begin{aligned}
\nabla_\theta J(\theta)
&= \nabla_\theta V^\pi(s_0) & \scriptstyle{\text{；从随机状态 } s_0} \text{ 开始}\\
&= \sum_{s}\color{blue}{\sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)} \phi(s) &\scriptstyle{\text{；令 }\color{blue}{\eta(s) = \sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)}} \\
&= \sum_{s}\eta(s) \phi(s) & \\
&= \Big( {\sum_s \eta(s)} \Big)\sum_{s}\frac{\eta(s)}{\sum_s \eta(s)} \phi(s) & \scriptstyle{\text{；将 } \eta(s), s\in\mathcal{S} \text{ 归一化变成概率分布}}\\
&\propto \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) & \scriptstyle{\sum_s \eta(s)\text{  是个常数}} \\
&= \sum_s d^\pi(s) \sum_a \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) & \scriptstyle{d^\pi(s) = \frac{\eta(s)}{\sum_s \eta(s)}\text{ 为静态分布}}
\end{aligned}
$$
此时，比例常数（ $$\sum_s \eta(s)$$）是状态经历的平均长度；连续情况下其为 1（[Sutton 与 Barto, 2017](http://incompleteideas.net/book/bookdraft2017nov5.pdf); 章节 13.2），梯度可以进一步写成：
$$
\begin{aligned}
\nabla_\theta J(\theta) 
&\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s)  &\\
&= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} &\\
&= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] & \scriptstyle{\text{；因为 } (\ln x)' = 1/x}
\end{aligned}
$$

其中 $$\mathbb{E}_\pi$$ 是指 $$\mathbb{E}_{s \sim d_\pi, a \sim \pi_\theta}$$ ，即状态和行动分布都按策略 $$\pi_\theta$$ 来（策略性）。

策略梯度理论为各种策略梯度算法打下了理论基础。这种普通的策略梯度更新没偏差但方差很大。下面的很多算法都是在寻求降低方差的同时保证偏差不变
$$
\nabla_\theta J(\theta)  = \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)]
$$
这有一个对策略梯度方法一般形式的不错总结，借鉴了 [GAE](https://arxiv.org/pdf/1506.02438.pdf)（总体优势估计，general advantage estimation） 论文（[Schulman 等, 2016](https://arxiv.org/abs/1506.02438)），而且这篇[文章](https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/)充分介绍了 GAE 的几个要素，推荐阅读。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_general_form_policy_gradient.png)

*图 1  策略梯度方法一般形式（图片来源：[Schulman 等, 2016](https://arxiv.org/abs/1506.02438)）*

### 策略梯度算法

近些年提出的策略梯度算法多如牛毛，我自然没办法穷尽介绍，只在此介绍一些我恰巧了解和读过的。

#### REINFORCE

**REINFORCE**（蒙特卡罗策略梯度）采用[蒙特卡罗法](https://libertydream.github.io/2020/07/05/强化学习综述/#蒙特卡罗法)对事件样本估计收益，以此更新策略参数 $$\theta$$。REINFORCE 的有效性来自于样本梯度的期望值等于实际梯度：
$$
\begin{aligned}
\nabla_\theta J(\theta)
&= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] & \\
&= \mathbb{E}_\pi [G_t \nabla_\theta \ln \pi_\theta(A_t \vert S_t)] & \scriptstyle{\text{；因为 } Q^\pi(S_t, A_t) = \mathbb{E}_\pi[G_t \vert S_t, A_t]}
\end{aligned}
$$
所以可以用实际样本轨迹测算 $$G_t$$ 并用其更新策略梯度。该方法依赖于完整的轨迹序列，这也是它是一种蒙特卡罗方法的原因。

整个过程简单易懂：

1.  随机初始化策略参数 $$\theta$$ 
2.  按策略 $$\pi_\theta$$ 得到一条轨迹： $$S_1, A_1, R_2, S_2, A_2, \dots, S_T$$.
3.  对 $$t=1,2,\dots,T$$
   1.  评估收益 $$G_t$$；
   2.  更新策略参数： $$\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla_\theta \ln \pi_\theta(A_t \vert S_t)$$

一个广泛使用的 REINFORCE 变式是从收益 $$G_t$$ 中减去一个基准值，以此*降低梯度估计方差同时保证偏差不变*（如果可能的话我们总想这样做）。举例来讲，一个常用基准是从行动价值中减掉状态价值，如果打算这样做，就在梯度上升更新中用优势  $$A(s, a) = Q(s, a) - V(s)$$ 进行计算。这篇[文章](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/)漂亮地解释了为什么基准会对降低方差有效，还不算一系列策略梯度基础。

#### 演员-评论家

策略梯度的两个主要部分是策略模型和价值函数。除策略外，再学习下价值函数很有意义，毕竟知道了价值函数可以辅助策略更新过程，比如在普通策略梯度中减掉梯度方差，这也恰好就是**演员-评论家（Actor-Critic）**方法的做法。

演员-评论家方法由两个模型组成，它们可以随意共享参数：

-  **评论家**要更新价值函数参数 $$w$$，看算法而定可能是行动价值 $$Q_w(a \vert s)$$ 或是状态价值 $$V_w(s)$$
-  **演员**要给 $$\pi_\theta(a \vert s)$$更新策略参数 $$\theta$$ ，更新方向评论家负责

以一个简单的取行动价值的演员-评论家算法为例，看一下是怎么工作的：

1.  随机初始化 $$s, \theta, w$$；采样得到 $$a \sim \pi_\theta(a \vert s)$$
2.  对 $$t = 1 \dots T$$：
   1.  采样得到回报 $$r_t \sim R(s, a)$$ 和下一状态 $$s' \sim P(s' \vert s, a)$$;
   2.  对下一步行动采样 $$a' \sim \pi_\theta(a' \vert s')$$;
   3.  更新策略参数：$$\theta \leftarrow \theta + \alpha_\theta Q_w(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)$$;
   4. 计算 $$t$$ 时刻的行动价值修正值（TD 误差）：<br/>$$\delta_t = r_t + \gamma Q_w(s', a') - Q_w(s, a)$$ <br/>并用它更新行动价值函数的参数：<br/>$$w \leftarrow w + \alpha_w \delta_t \nabla_w Q_w(s, a)$$
   5.  更新 $$a \leftarrow a'$$ 和 $$s \leftarrow s'$$

两个学习率，$$\alpha_\theta$$ 和 $$\alpha_w$$ ，是预先给策略和价值函数分别定好的更新参数。

#### 非策略策略梯度

REINFORCE 和普通的演员-评论家方法都是策略性的：训练样本是按目标策略收集来的，该策略也是我们试图优化的对象。不过非策略方法有些额外优势：

1.  非策略方法不需要完整轨迹，而且为了更高效地采样可以复用任何过往事件（[“经历重放”](https://libertydream.github.io/2020/07/05/强化学习综述/#-深度-q-网络)）
2. 收集样本的行动策略和目标策略不同，[探索](https://libertydream.github.io/2020/07/05/强化学习综述/#探索利用窘境)效果更好

现在来看一下非策略梯度要怎么算。收集样本的行动策略是已知的（像超参数一样预先定好），记为 $$\beta(a \vert s)$$。该行动策略决定了状态分布，目标函数负责统计分布上的回报：
$$
J(\theta)
= \sum_{s \in \mathcal{S}} d^\beta(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s)
= \mathbb{E}_{s \sim d^\beta} \big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \big]
$$
其中 $$d^\beta(s)$$ 是行动策略 $$\beta$$ 的静态分布；回想一下 $$d^\beta(s) = \lim_{t \to \infty} P(S_t = s \vert S_0, \beta)$$ ；而 $$Q^\pi$$ 是目标策略 $$\pi$$ （不是行动策略）影响下的行动价值函数。

既然训练观测是按 $$a \sim \beta(a \vert s)$$ 采样的，可以将梯度重写为：
$$
\begin{aligned}
\nabla_\theta J(\theta)
&= \nabla_\theta \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s)  \Big] & \\ 
&= \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} \big( Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) + \color{red}{\pi_\theta(a \vert s) \nabla_\theta Q^\pi(s, a)} \big) \Big] & \scriptstyle{\text{； 复合函数求导}}\\
&\stackrel{(i)}{\approx} \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) \Big] & \scriptstyle{\text{；忽略红字部分： } \color{red}{\pi_\theta(a \vert s) \nabla_\theta Q^\pi(s, a)}}. \\
&= \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} \beta(a \vert s) \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} \Big] & \\
&= \mathbb{E}_\beta \Big[\frac{\color{blue}{\pi_\theta(a \vert s)}}{\color{blue}{\beta(a \vert s)}} Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s) \Big] & \scriptstyle{\text{；蓝字部分是重要性权重}}
\end{aligned}
$$
其中 $$\frac{\pi_\theta(a \vert s)}{\beta(a \vert s)}$$ 是[重要度权重](http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/)。因为 $$Q^\pi$$ 是目标策略的一个函数，所以也是个策略参数为 $$\theta$$ 的函数。按复合函数求导规则，我们也该求 $$\nabla_\theta Q^\pi(s, a)$$ 的导数，但现实中要算 $$\nabla_\theta Q^\pi(s, a)$$ 太难了。所幸可以忽略掉 Q 值求个近似梯度，仍能保证策略改善效果，并且最终取得实际局部最小值。证明过程可以看[这里](https://arxiv.org/pdf/1205.4839.pdf)（Degris, White 和 Sutton, 2012）

总结一下，非策略环境下用策略梯度的时候，可以简单地将其调整为加权和，权重就是目标策略和行动策略的比值，$$\frac{\pi_\theta(a \vert s)}{\beta(a \vert s)}$$。

#### A3C

 $$J_v(w) = (G_t - V_w(s))^2$$ 

1.  $$\theta$$ and $$w$$; $$\theta'$$ and $$w'$$.
2.  $$t = 1$$
3.  $$T \leq T_\text{MAX}$$:
   1.  $$\mathrm{d}\theta = 0$$ and $$\mathrm{d}w = 0$$.
   2. : $$\theta' = \theta$$ and $$w' = w$$.
   3. $$t_\text{start}$$ = t  $$s_t$$.
   4. ($$s_t$$ != TERMINAL) and $$t - t_\text{start} \leq t_\text{max}$$:
      1.  $$A_t \sim \pi_{\theta'}(A_t \vert S_t)$$  $$R_t$$  $$s_{t+1}$$.
      2.  $$t = t + 1$$ and $$T = T + 1$$
   5. $$R = \begin{cases} 
      0 & \text{if } s_t \text{ is TERMINAL} \\
      V_{w'}(s_t) & \text{otherwise}
      \end{cases}$$
   6.  $$i = t-1, \dots, t_\text{start}$$:
      1. $$R \leftarrow \gamma R + R_i$$; here R is a MC measure of $$G_i$$.
      2.  $$\theta'$$: $$d\theta \leftarrow d\theta + \nabla_{\theta'} \log \pi_{\theta'}(a_i \vert s_i)(R - V_{w'}(s_i))$$;<br/>$$dw \leftarrow dw + 2 (R - V_{w'}(s_i)) \nabla_{w'} (R - V_{w'}(s_i))$$.
   7.  $$\theta$$ using $$\mathrm{d}\theta$$, and $$w$$ using $$\mathrm{d}w$$.

 $$w$$ or $$\theta$$ 

#### A2C

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_A3C_vs_A2C.png)

#### DPG

 $$\pi(. \vert s)$$  $$\mathcal{A}$$  $$a = \mu(s)$$. 

- $$\rho_0(s)$$:
- $$\rho^\mu(s \to s', k)$$: $$\mu$$.
- $$\rho^\mu(s')$$ $$\rho^\mu(s') = \int_\mathcal{S} \sum_{k=1}^\infty \gamma^{k-1} \rho_0(s) \rho^\mu(s \to s', k) ds$$.


$$
J(\theta) = \int_\mathcal{S} \rho^\mu(s) Q(s, \mu_\theta(s)) ds
$$
$$\theta$$:
$$
\begin{aligned}
\nabla_\theta J(\theta) 
&= \int_\mathcal{S} \rho^\mu(s) \nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)} ds \\
&= \mathbb{E}_{s \sim \rho^\mu} [\nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}]
\end{aligned}
$$
 $$\pi_{\mu_\theta, \sigma}$$ y $$\mu_\theta$$  $$\sigma$$, $$\sigma=0$$. 

 $$a = \mu_\theta(s)$$ 
$$
\begin{aligned}
\delta_t &= R_t + \gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t) & \scriptstyle{\text{; TD error in SARSA}}\\
w_{t+1} &= w_t + \alpha_w \delta_t \nabla_w Q_w(s_t, a_t) & \\
\theta_{t+1} &= \theta_t + \alpha_\theta \color{red}{\nabla_a Q_w(s_t, a_t) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}} & \scriptstyle{\text{; Deterministic policy gradient theorem}}
\end{aligned}
$$


 $$\beta(a \vert s)$$  $$\rho^\beta$$:
$$
\begin{aligned}
J_\beta(\theta) &= \int_\mathcal{S} \rho^\beta Q^\mu(s, \mu_\theta(s)) ds \\
\nabla_\theta J_\beta(\theta) &= \mathbb{E}_{s \sim \rho^\beta} [\nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s)  \rvert_{a=\mu_\theta(s)} ]
\end{aligned}
$$
 $$Q^\mu(s, \mu_\theta(s))$$  $$\sum_a \pi(a \vert s) Q^\pi(s, a)$$ 

#### DDPG

 $$\mu'$$  $$\mathcal{N}$$:
$$
\mu'(s) = \mu_\theta(s) + \mathcal{N}
$$
 $$\tau \ll 1$$: $$\theta' \leftarrow \tau \theta + (1 - \tau) \theta'$$. I

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_DDPG_algo.png)

#### D4PG

 $$Z_w$$  $$Q_w(s, a) = \mathbb{E} Z_w(x, a)$$.   $$L(w) = \mathbb{E}[d(\mathcal{T}_{\mu_\theta}, Z_{w'}(s, a), Z_w(s, a)]$$, where $$\mathcal{T}_{\mu_\theta}$$
$$
\begin{aligned}
\nabla_\theta J(\theta) 
&\approx \mathbb{E}_{\rho^\mu} [\nabla_a Q_w(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}] & \scriptstyle{\text{; gradient update in DPG}} \\
&= \mathbb{E}_{\rho^\mu} [\mathbb{E}[\nabla_a Q_w(s, a)] \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}] & \scriptstyle{\text{; expectation of the Q-value distribution.}}
\end{aligned}
$$
**$$N$$-step returns**: $$N$$-step 
$$
r(s_0, a_0) + \mathbb{E}[\sum_{n=1}^{N-1} r(s_n, a_n) + \gamma^N Q(s_N, \mu_\theta(s_N)) \vert s_0, a_0 ]
$$
 $$K$$ i

 $$R$$   $$p_i$$. $$(Rp_i)^{-1}$$  $$(Rp_i)^{-1}$$.

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_D4PG_algo.png)

#### MADDPG

s $$\mathcal{S}$$.  $$\mathcal{A}_1, \dots, \mathcal{A}_N$$,  $$\mathcal{O}_1, \dots, \mathcal{O}_N$$. $$\mathcal{T}: \mathcal{S} \times \mathcal{A}_1 \times \dots \mathcal{A}_N \mapsto \mathcal{S}$$.  $$\pi_{\theta_i}: \mathcal{O}_i \times \mathcal{A}_i \mapsto [0, 1]$$, : $$\mu_{\theta_i}: \mathcal{O}_i \mapsto \mathcal{A}_i$$.

 $$\vec{o} = {o_1, \dots, o_N}$$, $$\vec{\mu} = {\mu_1, \dots, \mu_N}$$ $$\vec{\theta} = {\theta_1, \dots, \theta_N}$$.

 $$Q^\vec{\mu}_i(\vec{o}, a_1, \dots, a_N)$$  $$a_1 \in \mathcal{A}_1, \dots, a_N \in \mathcal{A}_N$$  $$Q^\vec{\mu}_i$$  $$i=1, \dots, N$$  $$\theta_i$$ 
$$
\nabla_{\theta_i} J(\theta_i) = \mathbb{E}_{\vec{o}, a \sim \mathcal{D}} [\nabla_{a_i} Q^{\vec{\mu}}_i (\vec{o}, a_1, \dots, a_N) \nabla_{\theta_i} \mu_{\theta_i}(o_i) \rvert_{a_i=\mu_{\theta_i}(o_i)} ]
$$
 $$\mathcal{D}$$ s $$(\vec{o}, a_1, \dots, a_N, r_1, \dots, r_N, \vec{o}')$$  $$\vec{o}$$, $$a_1, \dots, a_N$$  $$r_1, \dots, r_N$$,$$\vec{o}'$$.
$$
\begin{aligned}
\mathcal{L}(\theta_i) &= \mathbb{E}_{\vec{o}, a_1, \dots, a_N, r_1, \dots, r_N, \vec{o}'}[ (Q^{\vec{\mu}}_i(\vec{o}, a_1, \dots, a_N) - y)^2 ] & \\
\text{where } y &= r_i + \gamma Q^{\vec{\mu}'}_i (\vec{o}', a'_1, \dots, a'_N) \rvert_{a'_j = \mu'_{\theta_j}} & \scriptstyle{\text{; TD target!}}
\end{aligned}
$$
 $$\vec{\mu}'$$

 $$\vec{\mu}$$

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_MADDPG.png)

#### TRPO

 $$\beta$$  $$\pi$$ 
$$
\begin{aligned}
J(\theta)
&= \sum_{s \in \mathcal{S}} \rho^{\pi_{\theta_\text{old}}} \sum_{a \in \mathcal{A}} \big( \pi_\theta(a \vert s) \hat{A}_{\theta_\text{old}}(s, a) \big) & \\
&= \sum_{s \in \mathcal{S}} \rho^{\pi_{\theta_\text{old}}} \sum_{a \in \mathcal{A}} \big( \beta(a \vert s) \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} \hat{A}_{\theta_\text{old}}(s, a) \big) & \scriptstyle{\text{; Importance sampling}} \\
&= \mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}}, a \sim \beta} \big[ \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} \hat{A}_{\theta_\text{old}}(s, a) \big] &
\end{aligned}
$$
 $$\theta_\text{old}$$  $$\rho^{\pi_{\theta_\text{old}}}$$ $$\beta(a \vert s)$$  $$\hat{A}(.)$$  $$A(.)$$ 

 $$\pi_{\theta_\text{old}}(a \vert s)$$ 
$$
J(\theta) = \mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}} \big[ \frac{\pi_\theta(a \vert s)}{\pi_{\theta_\text{old}}(a \vert s)} \hat{A}_{\theta_\text{old}}(s, a) \big]
$$
 $$J(\theta)$$  δ:
$$
\mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}}} [D_\text{KL}(\pi_{\theta_\text{old}}(.\vert s) \| \pi_\theta(.\vert s)] \leq \delta
$$

#### PPO

$$
r(\theta) = \frac{\pi_\theta(a \vert s)}{\pi_{\theta_\text{old}}(a \vert s)}
$$

$$
J^\text{TRPO} (\theta) = \mathbb{E} [ r(\theta) \hat{A}_{\theta_\text{old}}(s, a) ]
$$

 $$\theta_\text{old}$$ and $$\theta$$, $$J^\text{TRPO} (\theta)$$  $$r(\theta)$$  $$[1-\epsilon, 1+\epsilon]$$,  $$\epsilon$$
$$
J^\text{CLIP} (\theta) = \mathbb{E} [ \min( r(\theta) \hat{A}_{\theta_\text{old}}(s, a), \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_{\theta_\text{old}}(s, a))]
$$
 $$\text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon)$$  $$1+\epsilon$$  $$1-\epsilon$$. 
$$
J^\text{CLIP'} (\theta) = \mathbb{E} [ J^\text{CLIP} (\theta) - \color{red}{c_1 (V_\theta(s) - V_\text{target})^2} + \color{blue}{c_2 H(s, \pi_\theta(.))} ]
$$
 $$c_1$$ and $$c_2$$ 

#### ACER

 $$(\pi, \beta)$$,

1.  $$\delta_t = R_t + \gamma \mathbb{E}_{a \sim \pi} Q(S_{t+1}, a) - Q(S_t, A_t)$$; t$$r_t + \gamma \mathbb{E}_{a \sim \pi} Q(s_{t+1}, a) $$ i $$\mathbb{E}_{a \sim \pi}$$y $$\pi$$. 
2. l: $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \delta_t$$.: $$\Delta Q(S_t, A_t) = \alpha \delta_t$$.

$$
\Delta Q^\text{imp}(S_t, A_t) 
= \gamma^t \prod_{1 \leq \tau \leq t} \frac{\pi(A_\tau \vert S_\tau)}{\beta(A_\tau \vert S_\tau)} \delta_t
$$

 $$\Delta Q$$ 
$$
\Delta Q^\text{ret}(S_t, A_t) 
= \gamma^t \prod_{1 \leq \tau \leq t} \min(c, \frac{\pi(A_\tau \vert S_\tau)}{\beta(A_\tau \vert S_\tau)})  \delta_t
$$
 $$Q^\text{ret}$$ $$(Q^\text{ret}(s, a) - Q(s, a))^2$$.

 $$\hat{g}$$, $$\hat{g}_t^\text{acer}$$
$$
\begin{aligned}
\hat{g}_t^\text{acer}
= & \omega_t \big( Q^\text{ret}(S_t, A_t) - V_{\theta_v}(S_t) \big) \nabla_\theta \ln \pi_\theta(A_t \vert S_t) 
  & \scriptstyle{\text{; Let }\omega_t=\frac{\pi(A_t \vert S_t)}{\beta(A_t \vert S_t)}} \\
= & \color{blue}{\min(c, \omega_t) \big( Q^\text{ret}(S_t, A_t) - V_w(S_t) \big) \nabla_\theta \ln \pi_\theta(A_t \vert S_t)} \\
  & + \color{red}{\mathbb{E}_{a \sim \pi} \big[ \max(0, \frac{\omega_t(a) - c}{\omega_t(a)}) \big( Q_w(S_t, a) - V_w(S_t) \big) \nabla_\theta \ln \pi_\theta(a \vert S_t) \big]}
  & \scriptstyle{\text{; Let }\omega_t (a) =\frac{\pi(a \vert S_t)}{\beta(a \vert S_t)}}
\end{aligned}
$$
 $$Q_w(.)$$ and $$V_w(.)$$  $$V_w(.)$$ 

#### ACTKR



#### SAC

$$
J(\theta) = \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi_\theta}} [r(s_t, a_t) + \alpha \mathcal{H}(\pi_\theta(.\vert s_t))]
$$

 $$\mathcal{H}(.)$$ 

- r $$\theta$$, $$\pi_\theta$$.
-  $$w$$, $$Q_w$$.
-  $$\psi$$, $$V_\psi$$
- r $$V$$  $$Q$$ and $$\pi$$

$$
\begin{aligned}
Q(s_t, a_t) &= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s)} [V(s_{t+1})] & \text{; according to Bellman equation.}\\
\text{where }V(s_t) &= \mathbb{E}_{a_t \sim \pi} [Q(s_t, a_t) - \alpha \log \pi(a_t \vert s_t)] & \text{; soft state value function.}
\end{aligned}
$$

$$
\text{Thus, } Q(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{(s_{t+1}, a_{t+1}) \sim \rho_{\pi}} [Q(s_{t+1}, a_{t+1}) - \alpha \log \pi(a_{t+1} \vert s_{t+1})]
$$

$$\rho_\pi(s)$$ and $$\rho_\pi(s, a)$$ $$\pi(a \vert s)$$; 
$$
\begin{aligned}
J_V(\psi) &= \mathbb{E}_{s_t \sim \mathcal{D}} [\frac{1}{2} \big(V_\psi(s_t) - \mathbb{E}[Q_w(s_t, a_t) - \log \pi_\theta(a_t \vert s_t)] \big)^2] \\
\text{with gradient: }\nabla_\psi J_V(\psi) &= \nabla_\psi V_\psi(s_t)\big( V_\psi(s_t) - Q_w(s_t, a_t) + \log \pi_\theta (a_t \vert s_t) \big)
\end{aligned}
$$
 $$\mathcal{D}$$ 
$$
\begin{aligned}
J_Q(w) &= \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}} [\frac{1}{2}\big( Q_w(s_t, a_t) - (r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_\pi(s)}[V_{\bar{\psi}}(s_{t+1})]) \big)^2] \\
\text{with gradient: } \nabla_w J_Q(w) &= \nabla_w Q_w(s_t, a_t) \big( Q_w(s_t, a_t) - r(s_t, a_t) - \gamma V_{\bar{\psi}}(s_{t+1})\big) 
\end{aligned}
$$
 $$\bar{\psi}$$
$$
\begin{aligned}
\pi_\text{new} 
&= \arg\min_{\pi' \in \Pi} D_\text{KL} \Big( \pi'(.\vert s_t) \| \frac{\exp(Q^{\pi_\text{old}}(s_t, .))}{Z^{\pi_\text{old}}(s_t)} \Big) \\[6pt]
&= \arg\min_{\pi' \in \Pi} D_\text{KL} \big( \pi'(.\vert s_t) \| \exp(Q^{\pi_\text{old}}(s_t, .) - \log Z^{\pi_\text{old}}(s_t)) \big) \\[6pt]
\text{objective for update: } J_\pi(\theta) &= \nabla_\theta D_\text{KL} \big( \pi_\theta(. \vert s_t) \| \exp(Q_w(s_t, .) - \log Z_w(s_t)) \big) \\[6pt]
&= \mathbb{E}_{a_t\sim\pi} \Big[ - \log \big( \frac{\exp(Q_w(s_t, a_t) - \log Z_w(s_t))}{\pi_\theta(a_t \vert s_t)} \big) \Big] \\[6pt]
&= \mathbb{E}_{a_t\sim\pi} [ \log \pi_\theta(a_t \vert s_t) - Q_w(s_t, a_t) + \log Z_w(s_t) ]
\end{aligned}
$$
 $$\Pi$$  $$\Pi$$ $$Z^{\pi_\text{old}}(s_t)$$  $$J_\pi(\theta)$$ $$\Pi$$.

 $$Q^{\pi_\text{new}}(s_t, a_t) \geq Q^{\pi_\text{old}}(s_t, a_t)$$, 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_SAC_algo.png)

#### 温度自调节 SAC

$$
\max_{\pi_0, \dots, \pi_T} \mathbb{E} \Big[ \sum_{t=0}^T r(s_t, a_t)\Big] \text{s.t. } \forall t\text{, } \mathcal{H}(\pi_t) \geq \mathcal{H}_0
$$

 $$\mathcal{H}_0$$ 

 $$\mathbb{E} \Big[ \sum_{t=0}^T r(s_t, a_t)\Big]$$  $$\pi_t$$ , $$\pi_{t-1}$$
$$
\underbrace{\max_{\pi_0} \Big( \mathbb{E}[r(s_0, a_0)]+ \underbrace{\max_{\pi_1} \Big(\mathbb{E}[...] + \underbrace{\max_{\pi_T} \mathbb{E}[r(s_T, a_T)]}_\text{1st maximization} \Big)}_\text{second but last maximization} \Big)}_\text{last maximization}
$$
r $$\gamma=1$$.

 $$T$$:
$$
\text{maximize } \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) ] \text{ s.t. } \mathcal{H}(\pi_T) - \mathcal{H}_0 \geq 0
$$

$$
\begin{aligned}
h(\pi_T) &= \mathcal{H}(\pi_T) - \mathcal{H}_0 = \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [-\log \pi_T(a_T\vert s_T)] - \mathcal{H}_0\\
f(\pi_T) &= \begin{cases}
\mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) ], & \text{if }h(\pi_T) \geq 0 \\
-\infty, & \text{otherwise}
\end{cases}
\end{aligned}
$$


$$
\text{maximize } f(\pi_T) \text{ s.t. } h(\pi_T) \geq 0
$$
$$\alpha_T$$:
$$
L(\pi_T, \alpha_T) = f(\pi_T) + \alpha_T h(\pi_T)
$$
$$L(\pi_T, \alpha_T)$$  $$\alpha_T$$  $$\pi_T$$, 

- , $$h(\pi_T) \geq 0$$,  $$\alpha_T=0$$  $$f(\pi_T)$$. Thus, $$L(\pi_T, 0) = f(\pi_T)$$.
-  $$h(\pi_T) < 0$$,  $$L(\pi_T, \alpha_T) \to -\infty$$   $$\alpha_T \to \infty$$.  $$L(\pi_T, \infty) = -\infty = f(\pi_T)$$.


$$
f(\pi_T) = \min_{\alpha_T \geq 0} L(\pi_T, \alpha_T)
$$

$$
\max_{\pi_T} f(\pi_T) = \min_{\alpha_T \geq 0} \max_{\pi_T} L(\pi_T, \alpha_T)
$$
$$f(\pi_T)$$,  $$\max_{\pi_T} f(\pi_T)$$ $$-\infty$$, 
$$
\begin{aligned}
\max_{\pi_T} \mathbb{E}[ r(s_T, a_T) ]
&= \max_{\pi_T} f(\pi_T) \\
&= \min_{\alpha_T \geq 0}  \max_{\pi_T} L(\pi_T, \alpha_T) \\
&= \min_{\alpha_T \geq 0}  \max_{\pi_T} f(\pi_T) + \alpha_T h(\pi_T) \\ 
&= \min_{\alpha_T \geq 0}  \max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) ] + \alpha_T ( \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [-\log \pi_T(a_T\vert s_T)] - \mathcal{H}_0) \\ 
&= \min_{\alpha_T \geq 0}  \max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T)  - \alpha_T \log \pi_T(a_T\vert s_T)] - \alpha_T \mathcal{H}_0 \\
&= \min_{\alpha_T \geq 0}  \max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T)  + \alpha_T \mathcal{H}(\pi_T) - \alpha_T \mathcal{H}_0 ]
\end{aligned}
$$
 $$\pi_T$$ and $$\alpha_T$$  $$\alpha_T$$, $$\pi_T^{*}$$ $$L(\pi_T^{*}, \alpha_T)$$.  $$\pi_T^{*}$$  $$\alpha_T^{*}$$  $$L(\pi_T^{*}, \alpha_T)$$. 
$$
\begin{aligned}
\pi^{*}_T
&= \arg\max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T)  + \alpha_T \mathcal{H}(\pi_T) - \alpha_T \mathcal{H}_0 ] \\
\color{blue}{\alpha^{*}_T}
&\color{blue}{=} \color{blue}{\arg\min_{\alpha_T \geq 0} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi^{*}}} [\alpha_T \mathcal{H}(\pi^{*}_T) - \alpha_T \mathcal{H}_0 ]}
\end{aligned}
$$

$$
\text{Thus, }\max_{\pi_T} \mathbb{E} [ r(s_T, a_T) ] 
= \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi^{*}}} [ r(s_T, a_T)  + \alpha^{*}_T \mathcal{H}(\pi^{*}_T) - \alpha^{*}_T \mathcal{H}_0 ]
$$


$$
\begin{aligned}
Q_{T-1}(s_{T-1}, a_{T-1}) 
&= r(s_{T-1}, a_{T-1}) + \mathbb{E} [Q(s_T, a_T) - \alpha_T \log \pi(a_T \vert s_T)] \\
&= r(s_{T-1}, a_{T-1}) + \mathbb{E} [r(s_T, a_T)] + \alpha_T \mathcal{H}(\pi_T) \\
Q_{T-1}^{*}(s_{T-1}, a_{T-1}) 
&= r(s_{T-1}, a_{T-1}) + \max_{\pi_T} \mathbb{E} [r(s_T, a_T)] +  \alpha_T \mathcal{H}(\pi^{*}_T) & \text{; plug in the optimal }\pi_T^{*}
\end{aligned}
$$
 $$T-1$$:
$$
\begin{aligned}
&\max_{\pi_{T-1}}\Big(\mathbb{E}[r(s_{T-1}, a_{T-1})] + \max_{\pi_T} \mathbb{E}[r(s_T, a_T] \Big) \\
&= \max_{\pi_{T-1}} \Big( Q^{*}_{T-1}(s_{T-1}, a_{T-1}) - \alpha^{*}_T \mathcal{H}(\pi^{*}_T) \Big) & \text{; should s.t. } \mathcal{H}(\pi_{T-1}) - \mathcal{H}_0 \geq 0 \\
&= \min_{\alpha_{T-1} \geq 0}  \max_{\pi_{T-1}} \Big( Q^{*}_{T-1}(s_{T-1}, a_{T-1}) - \alpha^{*}_T \mathcal{H}(\pi^{*}_T) + \alpha_{T-1} \big( \mathcal{H}(\pi_{T-1}) - \mathcal{H}_0 \big) \Big) & \text{; dual problem w/ Lagrangian.} \\
&= \min_{\alpha_{T-1} \geq 0}  \max_{\pi_{T-1}} \Big( Q^{*}_{T-1}(s_{T-1}, a_{T-1}) + \alpha_{T-1} \mathcal{H}(\pi_{T-1}) - \alpha_{T-1}\mathcal{H}_0 \Big) - \alpha^{*}_T \mathcal{H}(\pi^{*}_T)
\end{aligned}
$$

$$
\begin{aligned}
\pi^{*}_{T-1} &= \arg\max_{\pi_{T-1}} \mathbb{E}_{(s_{T-1}, a_{T-1}) \sim \rho_\pi} [Q^{*}_{T-1}(s_{T-1}, a_{T-1}) + \alpha_{T-1} \mathcal{H}(\pi_{T-1}) - \alpha_{T-1} \mathcal{H}_0 ] \\
\color{green}{\alpha^{*}_{T-1}} &\color{green}{=} \color{green}{\arg\min_{\alpha_{T-1} \geq 0} \mathbb{E}_{(s_{T-1}, a_{T-1}) \sim \rho_{\pi^{*}}} [ \alpha_{T-1} \mathcal{H}(\pi^{*}_{T-1}) - \alpha_{T-1}\mathcal{H}_0 ]}
\end{aligned}
$$

$$\alpha_{T-1}$$  $$\alpha_{T-1}$$ 
$$
J(\alpha) = \mathbb{E}_{a_t \sim \pi_t} [-\alpha \log \pi_t(a_t \mid s_t) - \alpha \mathcal{H}_0]
$$
 $$\alpha$$  $$J(\alpha)$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_SAC2_algo.png)

#### TD3

 $$(\mu_{\theta_1}, \mu_{\theta_2})$$  $$(Q_{w_1}, Q_{w_2})$$,
$$
\begin{aligned}
y_1 &= r + \gamma Q_{w_2}(s', \mu_{\theta_1}(s'))\\
y_2 &= r + \gamma Q_{w_1}(s', \mu_{\theta_2}(s'))
\end{aligned}
$$

$$
\begin{aligned}
y_1 &= r + \gamma \min_{i=1,2}Q_{w_i}(s', \mu_{\theta_1}(s'))\\
y_2 &= r + \gamma \min_{i=1,2} Q_{w_i}(s', \mu_{\theta_2}(s'))
\end{aligned}
$$

$$
\begin{aligned}
y &= r + \gamma Q_w (s', \mu_{\theta}(s') + \epsilon) & \\
\epsilon &\sim \text{clip}(\mathcal{N}(0, \sigma), -c, +c) & \scriptstyle{\text{ ; clipped random noises.}}
\end{aligned}
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_TD3.png)

#### SVPG

 $$\theta$$.

 $$\theta$$. $$\theta \sim q(\theta)$$  $$q(\theta)$$.  $$q$$  $$q_0$$,  $$\theta$$ $$q_0$$ 
$$
\hat{J}(\theta) = \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha D_\text{KL}(q\|q_0)
$$
 $$\mathbb{E}_{\theta \sim q} [R(\theta)]$$  $$\theta \sim q(\theta)$$  $$D_\text{KL}$$ 

 $$q_0$$  $$q_0(\theta)$$ 
$$
\begin{aligned}
\hat{J}(\theta) 
&= \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha D_\text{KL}(q\|q_0) \\
&= \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha \mathbb{E}_{\theta \sim q} [\log q(\theta) - \log q_0(\theta)] \\
&= \mathbb{E}_{\theta \sim q} [J(\theta)] + \alpha H(q(\theta))
\end{aligned}
$$
 $$\hat{J}(\theta) = \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha D_\text{KL}(q\|q_0)$$ w.r.t. $$q$$:
$$
\begin{aligned}
\nabla_q \hat{J}(\theta) 
&= \nabla_q \big( \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha D_\text{KL}(q\|q_0) \big) \\
&= \nabla_q \int_\theta \big( q(\theta) J(\theta) - \alpha q(\theta)\log q(\theta) + \alpha q(\theta) \log q_0(\theta) \big) \\
&= \int_\theta \big( J(\theta) - \alpha \log q(\theta) -\alpha + \alpha \log q_0(\theta) \big) \\
&= 0
\end{aligned}
$$

$$
\log q^{*}(\theta) = \frac{1}{\alpha} J(\theta) + \log q_0(\theta) - 1 \text{ thus } \underbrace{ q^{*}(\theta) }_\textrm{"posterior"} \propto \underbrace{\exp ( J(\theta) / \alpha )}_\textrm{"likelihood"} \underbrace{q_0(\theta)}_\textrm{prior}
$$
 $$\alpha$$  $$\alpha \rightarrow 0$$, $$\theta$$ $$J(\theta)$$. $$\alpha \rightarrow \infty$$, $$\theta$$ 

$$q(\theta)$$, $$\{\theta_i\}_{i=1}^n$$ 
$$
\theta_i \gets \theta_i + \epsilon \phi^{*}(\theta_i) \text{ where } \phi^{*} = \max_{\phi \in \mathcal{H}} \{ - \nabla_\epsilon D_\text{KL} (q'_{[\theta + \epsilon \phi(\theta)]} \| q) \text{ s.t. } \|\phi\|_{\mathcal{H}} \leq 1\}
$$
 $$\epsilon$$  $$\phi^{*}$$  $$\mathcal{H}$$ of $$\theta$$  $$q'(.)$$ f $$\theta + \epsilon \phi(\theta)$$.

| Method                                                       | Update space                                            |
| ------------------------------------------------------------ | ------------------------------------------------------- |
| Plain gradient                                               | $$\Delta \theta$$ on the parameter space                |
| [Natural gradient]({{ site.baseurl }}{% post_url 2019-09-05-evolution-strategies %}#natural-gradients) | $$\Delta \theta$$ on the search distribution space      |
| SVGD                                                         | $$\Delta \theta$$ on the kernel function space (edited) |

f $$\phi^{*}$$ $$k(\vartheta, \theta)$$,
$$
\begin{aligned}
\phi^{*}(\theta_i) 
&= \mathbb{E}_{\vartheta \sim q'} [\nabla_\vartheta \log q(\vartheta) k(\vartheta, \theta_i) + \nabla_\vartheta k(\vartheta, \theta_i)]\\
&= \frac{1}{n} \sum_{j=1}^n [\color{red}{\nabla_{\theta_j} \log q(\theta_j) k(\theta_j, \theta_i)} + \color{green}{\nabla_{\theta_j} k(\theta_j, \theta_i)}] & \scriptstyle{\text{;approximate }q'\text{ with current particle values}}
\end{aligned}
$$

-  $$\theta_i$$  $$q$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_SVPG.png)

#### IMPALA



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_IMPALA.png)

 $$V_\theta$$  $$\theta$$  $$\pi_\phi$$ $$\phi$$. y $$\mu$$. 

 $$t$$, $$(s_t, a_t, s_{t+1}, r_t)$$, $$\theta$$  $$n$$-step
$$
\begin{aligned}
v_t  &= V_\theta(s_t) + \sum_{i=t}^{t+n-1} \gamma^{i-t} \big(\prod_{j=t}^{i-1} c_j\big) \color{red}{\delta_i V} \\
&= V_\theta(s_t) + \sum_{i=t}^{t+n-1} \gamma^{i-t} \big(\prod_{j=t}^{i-1} c_j\big) \color{red}{\rho_i (r_i + \gamma V_\theta(s_{i+1}) - V_\theta(s_i))}
\end{aligned}
$$
t $$\delta_i V$$  $$V$$. $$\rho_i = \min\big(\bar{\rho}, \frac{\pi(a_i \vert s_i)}{\mu(a_i \vert s_i)}\big)$$  $$c_j = \min\big(\bar{c}, \frac{\pi(a_j \vert s_j)}{\mu(a_j \vert s_j)}\big)$$  $$c_t, \dots, c_{i-1}$$  $$\delta_i V$$  $$i$$  $$t$$  $$\rho_i=1$$ and $$c_j=1$$  $$\bar{c} \geq 1$$)

$$\bar{\rho}$$ and $$\bar{c}$$  $$\bar{\rho} \geq \bar{c}$$. $$\bar{\rho}$$  $$\bar{\rho} =\infty$$ $$V^\pi$$;  $$\bar{\rho}$$  $$V^\mu$$; $$\pi$$ and $$\mu$$.
$$
\Delta\theta = (v_t - V_\theta(s_t))\nabla_\theta V_\theta(s_t)
$$
$$\phi$$ 
$$
\begin{aligned}
\Delta \phi 
&= \rho_t \nabla_\phi \log \pi_\phi(a_t \vert s_t) \big(r_t + \gamma v_{t+1} - V_\theta(s_t)\big) + \nabla_\phi H(\pi_\phi)\\
&= \rho_t \nabla_\phi \log \pi_\phi(a_t \vert s_t) \big(r_t + \gamma v_{t+1} - V_\theta(s_t)\big) - \nabla_\phi \sum_a \pi_\phi(a\vert s_t)\log \pi_\phi(a\vert s_t)
\end{aligned}
$$
 $$r_t + \gamma v_{t+1}$$ $$V_\theta(s_t)$$  $$H(\pi_\phi)$$

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_IMPALA-arch.png)

### 快速总结













