---
layout:		post
title:  	策略梯度算法
subtitle:   
date:       2020-07-19
author:     一轩明月
header-img: img/post-bg-ai.jpg
catalog:    true
tags:
    - reinforcement learning
excerpt:    本文深入介绍了什么是策略梯度，它为什么有效，并分析了许多近些年新提出的策略梯度算法：普通策略梯度，演员-评论家，非策略演员-评论家，A3C，A2C,DPG,DDPG,D4PG,MADDPG,TRPO,PRO,ACER,ACTKR,SAC,TD3 以及 SVPG
---

> 编译自：Policy Gradient Algorithms， [Lilian Weng](https://lilianweng.github.io/lil-log/)

### 什么是策略梯度

策略梯度是一种求解强化学习问题的方法，如果你未曾涉猎过有关强化学习的知识，可以先看一下“[强化学习入门——关键概念](https://libertydream.github.io/2020/07/05/强化学习综述/#关键概念)”一节，熟悉一下问题内涵以及相关的核心概念。

#### 符号说明

下面的符号表可以帮你更好的理解文中出现的方程。

| 符号                    | 含义                                                         |
| ----------------------- | ------------------------------------------------------------ |
| $$s \in \mathcal{S}$$   | 状态                                                         |
| $$a \in \mathcal{A}$$   | 行动                                                         |
| $$r \in \mathcal{R}$$   | 回报                                                         |
| $$S_t, A_t, R_t$$       | 学习轨迹上 $$t$$ 时刻的状态，行动和回报。可能也偶尔会用 $$s_t, a_t, r_t$$ |
| $$\gamma$$              | 折扣因子；对未来回报的不确定性的惩罚； $$0<\gamma \leqslant 1$$. |
| $$G_t$$                 | 收益；或打折扣的未来回报； $$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$ |
| $$P(s', r \vert s, a)$$ | 若当前状态为 $$s$$，行动为 $$a$$，回报为 $$r$$，转移到下一状态 $$s'$$ 的概率 |
| $$\pi(a \vert s)$$      | 随机策略（代理人行动策略）； $$\pi_\theta(.)$$ 是参数为 $$\theta$$ 的策略 |
| $$\mu(s)$$              | 既定策略；也可以记为 $$\pi(s)$$，但用个不同的字母能提高辨识度，不需要过多解释就能判断策略是随机的还是确定的。不管是用 $$\pi$$ 还是 $$\mu$$ 都是强化学习算法的学习目标 |
| $$V(s)$$                | 状态价值函数，代表状态 $$s$$ 的预期收益； $$V_w(.)$$ 是参数为 $$w$$ 的状态价值函数 |
| $$V^\pi(s)$$            | 按策略 $$\pi$$ 指示行动时，状态 $$s$$ 的价值；$$V^\pi (s) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s]$$ |
| $$Q(s, a)$$             | 行动价值函数，类似于 $$V(s)$$，只是它评估的是一对状态和行动 $$(s, a)$$ 的预期收益； $$Q_w(.)$$ 是参数为 $$w$$ 的行动价值函数 |
| $$Q^\pi(s, a)$$         | 类似于 $$V^\pi(.)$$，按策略 $$\pi$$ 行动时（状态，行动）对的价值; $$Q^\pi(s, a) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s, A_t = a]$$ |
| $$A(s, a)$$             | 收益函数, $$A(s, a) = Q(s, a) - V(s)$$；可以看作是另一个版本的 Q 值，减去了作为基准的状态价值所以方差更小 |

#### 策略梯度

强化学习的目标是给代理人找到最佳行动策略，进而使回报最大化。**策略梯度（policy gradient）**法的目标是直接对策略建模并进行优化，一般是用一个参数为 $$\theta$$ 的函数 $$\pi_\theta(a \vert s)$$ 对策略建模。该策略决定了回报（目标）函数的价值，可以尝试各种算法优化 $$\theta$$ 以获得最大收益。

回报函数的定义为：


$$
J(\theta) 
= \sum_{s \in \mathcal{S}} d^\pi(s) V^\pi(s) 
= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a)
$$


其中 $$d^\pi(s)$$ 是 $$\pi_\theta$$（ $$\pi$$ 下的策略性状态分布）所对应的马尔可夫链的静态分布。简单起见，当策略 $$\pi_\theta$$ 出现在其他函数的角标里时省去参数 $$\theta$$ ；举例来说，$$d^{\pi}$$ 和 $$Q^\pi$$ 写全了应该是 $$d^{\pi_\theta}$$ 和 $$Q^{\pi_\theta}$$。

试想你在马尔可夫链上的状态间游走，随着时间推移，最终你落在某一状态上的概率不再变化——这就是 $$\pi_\theta$$ 的静态概率。$$d^\pi(s) = \lim_{t \to \infty} P(s_t = s \vert s_0, \pi_\theta)$$ 是从 $$s_0$$ 起，按策略 $$\pi_\theta$$ 走 $$t$$ 步后 $$s_t=s$$ 的概率。实际上 PageRank 算法之所以有效的一大原因就是因为马尔可夫链存在静态分布，感兴趣的话可以看看这篇[文章](https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/)。

自然的，可以看出基于策略的方法在连续空间内更有用。因为有无尽的行动和状态有待估计价值，也因此基于价值的方法在连续空间内的计算开销高到天上去了。比如在[广义策略迭代](https://libertydream.github.io/2020/07/05/强化学习综述/#-策略迭代)中，改善策略时计算 $$\arg\max_{a \in \mathcal{A}} Q^\pi(s, a)$$ 需要遍历整个行动空间，常要经受[维度诅咒](https://en.wikipedia.org/wiki/Curse_of_dimensionality)的折磨。

借助 *梯度上升* 法，可以按梯度 $$\nabla_\theta J(\theta)$$ 所示方向移动 $$\theta$$ 来寻找能带来最大收益的 $$\pi_\theta$$ 所对应的 $$\theta$$。

#### 策略梯度定理

计算梯度 $$\nabla_\theta J(\theta)$$ 会有些棘手，因为既要看行动选择（直接由 $$\pi_\theta$$ 决定），还要看既定行为下状态的静态分布（由 $$\pi_\theta$$ 间接决定）。考虑到一般情况下并不清楚环境状况，很难预料策略更新会给状态分布带来怎样的影响。

所幸**策略梯度定理**能帮到我们。定理给出了一个目标函数导数的优秀变式，不涉及状态分布 $$d^\pi(.)$$ 的导数，极大简化了梯度 $$\nabla_\theta J(\theta)$$ 的计算过程。


$$
\begin{aligned}
\nabla_\theta J(\theta) 
&= \nabla_\theta \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \\
&\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) 
\end{aligned}
$$



#### 策略梯度定理证明

这一节会有些晦涩难懂，我们要看下证明过程（[Sutton 与 Barto, 2017](http://incompleteideas.net/book/bookdraft2017nov5.pdf); 章节13.1），弄清楚为什么策略梯度定理是正确的。

让我们从状态价值函数的导数开始：
$$
\begin{aligned}
& \nabla_\theta V^\pi(s) \\
=& \nabla_\theta \Big(\sum_{a \in \mathcal{A}} \pi_\theta(a \vert s)Q^\pi(s, a) \Big) & \\
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta Q^\pi(s, a)} \Big) & \scriptstyle{\text{；链式求导}} \\
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta \sum_{s', r} P(s',r \vert s,a)(r + V^\pi(s'))} \Big) & \scriptstyle{Q^\pi \text{ 分成当前回报与未来回报}} \\
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s', r} P(s',r \vert s,a) \nabla_\theta V^\pi(s')} \Big) & \scriptstyle{P(s',r \vert s,a) \text{ 或 } r \text{ 都与 }\theta \text{ 无关}}\\
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s'} P(s' \vert s,a) \nabla_\theta V^\pi(s')} \Big) & \scriptstyle{\text{；因为 }  P(s' \vert s, a) = \sum_r P(s', r \vert s, a)}
\end{aligned}
$$

于是得到


$$
\color{red}{\nabla_\theta V^\pi(s)} \color{black}= \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \sum_{s'} P(s' \vert s,a) \color{red}{\nabla_\theta V^\pi(s')} \Big)
$$


方程有良好的递归性（看红色部分），未来状态价值函数 $$V^\pi(s')$$ 可以按相同方式不断展开。

考虑如下游走序列，按策略 $$\pi_\theta$$ 走 $$k$$ 步后从状态 $$s$$ 到状态 $$x$$ 的概率记为  $$\rho^\pi(s \to x, k)$$：


$$
s \xrightarrow[]{a \sim \pi_\theta(.\vert s)} s' \xrightarrow[]{a \sim \pi_\theta(.\vert s')} s'' \xrightarrow[]{a \sim \pi_\theta(.\vert s'')} \dots
$$



- 当 $$k=0$$ 时，$$\rho^\pi(s \to s, k=0) = 1$$.
- 当 $$k=1$$，考察所有可能行动并统计达到目标状态的转移概率：$$\rho^\pi(s \to s', k=1) = \sum_a \pi_\theta(a \vert s) P(s' \vert s, a)$$.
-  我们是想按策略 $$\pi_\theta$$ 走 $$k+1$$ 步从状态 $$s$$ 到 $$x$$，可以先走 $$k$$ 步从 $$s$$ 到中间点 $$s'$$（任何状态都可以是中间点  $$s' \in \mathcal{S}$$） 然后最后再走一步抵达最终状态 $$x$$。这样我们可以递归更新访问概率：$$\rho^\pi(s \to x, k+1) = \sum_{s'} \rho^\pi(s \to s', k) \rho^\pi(s' \to x, 1)$$

回到 $$\nabla_\theta V^\pi(s)$$ 的递归展开式，令 $$\phi(s) = \sum_{a \in \mathcal{A}} \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a)$$ 简化一下数学表达。如果将 $$\nabla_\theta V^\pi(.)$$ 无限地展开，容易看出如果服从该递归过程并统计所有访问概率，就可以从起始状态开始走若干步转移到任意状态，这样就得到了 $$\nabla_\theta V^\pi(s)$$
$$
\begin{aligned}
& \color{red}{\nabla_\theta V^\pi(s)} \\
=& \phi(s) + \sum_a \pi_\theta(a \vert s) \sum_{s'} P(s' \vert s,a) \color{red}{\nabla_\theta V^\pi(s')} \\
=& \phi(s) + \sum_{s'} \sum_a \pi_\theta(a \vert s) P(s' \vert s,a) \color{red}{\nabla_\theta V^\pi(s')} \\
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \color{red}{\nabla_\theta V^\pi(s')} \\
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \color{red}{[ \phi(s') + \sum_{s''} \rho^\pi(s' \to s'', 1) \nabla_\theta V^\pi(s'')]} \\
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \phi(s') + \sum_{s''} \rho^\pi(s \to s'', 2)\color{red}{\nabla_\theta V^\pi(s'')}\color{black} \scriptstyle{ \text{ ；}s'\text{ 是 }s \to s'' \text{ 的中间点}}\\
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \phi(s') + \sum_{s''} \rho^\pi(s \to s'', 2)\phi(s'') + \sum_{s'''} \rho^\pi(s \to s''', 3)\color{red}{\nabla_\theta V^\pi(s''')} \\
=& \dots \scriptstyle{\text{; 不断将 }\nabla_\theta V^\pi(.)}\text{ 展开} \\
=& \sum_{x\in\mathcal{S}}\sum_{k=0}^\infty \rho^\pi(s \to x, k) \phi(x)
\end{aligned}
$$
有了这个变式就能丢掉 Q 值函数的导数 $$\nabla_\theta Q^\pi(s, a)$$ 了，将其插入目标函数 $$J(\theta)$$ 得到：


$$
\begin{aligned}
\nabla_\theta J(\theta)
&= \nabla_\theta V^\pi(s_0) & \scriptstyle{\text{；从随机状态 } s_0} \text{ 开始}\\
&= \sum_{s}\color{blue}{\sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)} \phi(s) &\scriptstyle{\text{；令 }\color{blue}{\eta(s) = \sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)}} \\
&= \sum_{s}\eta(s) \phi(s) & \\
&= \Big( {\sum_s \eta(s)} \Big)\sum_{s}\frac{\eta(s)}{\sum_s \eta(s)} \phi(s) & \scriptstyle{\text{；将 } \eta(s), s\in\mathcal{S} \text{ 归一化变成概率分布}}\\
&\propto \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) & \scriptstyle{\sum_s \eta(s)\text{  是个常数}} \\
&= \sum_s d^\pi(s) \sum_a \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) & \scriptstyle{d^\pi(s) = \frac{\eta(s)}{\sum_s \eta(s)}\text{ 为静态分布}}
\end{aligned}
$$


此时，比例常数（ $$\sum_s \eta(s)$$）是状态链的平均长度；连续情况下其为 1（[Sutton 与 Barto, 2017](http://incompleteideas.net/book/bookdraft2017nov5.pdf); 章节 13.2），而梯度可以进一步写成：


$$
\begin{aligned}
\nabla_\theta J(\theta) 
&\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s)  &\\
&= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} &\\
&= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] & \scriptstyle{\text{；因为 } (\ln x)' = 1/x}
\end{aligned}
$$



其中 $$\mathbb{E}_\pi$$ 是指 $$\mathbb{E}_{s \sim d_\pi, a \sim \pi_\theta}$$ ，即状态和行动分布都由策略 $$\pi_\theta$$ 决定（策略性）。

策略梯度理论为各种策略梯度算法打下了理论基础。这种普通的策略梯度更新不存在偏差但方差很大。下文中的很多算法都是在寻求降低方差的同时保证偏差不变


$$
\nabla_\theta J(\theta)  = \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)]
$$


这有一个对策略梯度方法一般形式的不错总结，借鉴了 [GAE](https://arxiv.org/pdf/1506.02438.pdf)（总体收益估计，general advantage estimation） 论文（[Schulman 等, 2016](https://arxiv.org/abs/1506.02438)），而且这篇[文章](https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/)充分介绍了 GAE 的几个要素，推荐阅读。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_general_form_policy_gradient.png)

*图 1  策略梯度方法的一般形式（图片来源：[Schulman 等, 2016](https://arxiv.org/abs/1506.02438)）*

### 策略梯度算法

近些年提出的策略梯度算法多如牛毛，我自然无法尽数介绍，仅在此讲些我恰巧了解和读过的。

#### REINFORCE

**REINFORCE**（蒙特卡罗策略梯度）靠[蒙特卡罗法](https://libertydream.github.io/2020/07/05/强化学习综述/#蒙特卡罗法)获取预期收益，蒙特卡罗法用事件样本更新策略参数 $$\theta$$。REINFORCE 的有效性来自于样本梯度的期望值等于实际梯度：


$$
\begin{aligned}
\nabla_\theta J(\theta)
&= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] & \\
&= \mathbb{E}_\pi [G_t \nabla_\theta \ln \pi_\theta(A_t \vert S_t)] & \scriptstyle{\text{；因为 } Q^\pi(S_t, A_t) = \mathbb{E}_\pi[G_t \vert S_t, A_t]}
\end{aligned}
$$


所以可以用实际样本轨迹测算 $$G_t$$ 并用其更新策略梯度。该方法依赖于完整的轨迹序列，这也是它是一种蒙特卡罗方法的原因。

整个过程简单易懂：

1.  随机初始化策略参数 $$\theta$$ 
2.  按策略 $$\pi_\theta$$ 得到学习轨迹： $$S_1, A_1, R_2, S_2, A_2, \dots, S_T$$.
3.  对 $$t=1,2,\dots,T$$
   1.  评估收益 $$G_t$$；
   2.  更新策略参数： $$\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla_\theta \ln \pi_\theta(A_t \vert S_t)$$

一个广泛使用的 REINFORCE 变式是从收益 $$G_t$$ 中减去一个基准值，从而 *降低梯度估计方差的同时保证偏差不变*（如果可能的话我们总想这样做）。举例来讲，一个常用基准是从行动价值中减掉状态价值，如果打算这样做，就在梯度上升更新中用收益  $$A(s, a) = Q(s, a) - V(s)$$ 进行计算。这篇[文章](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/)除了介绍策略梯度的基础外，还漂亮地解释了为什么基准会对降低方差有效。

#### 演员-评论家

策略梯度的两个主要部分是策略模型和价值函数。除策略外，再学习下价值函数很有意义，毕竟知道了价值函数可以辅助策略更新过程，比如在普通策略梯度中减掉梯度方差，这也恰好就是**演员-评论家（Actor-Critic）**方法的做法。

演员-评论家方法由两个模型组成，它们可以随意共享参数：

-  **评论家**要更新价值函数参数 $$w$$，视算法而定可能是行动价值 $$Q_w(a \vert s)$$ 或是状态价值 $$V_w(s)$$
-  **演员**要给 $$\pi_\theta(a \vert s)$$更新策略参数 $$\theta$$ ，更新方向评论家负责

以一个简单的基于行动价值的演员-评论家算法为例，看一下是怎么工作的：

1.  随机初始化 $$s, \theta, w$$；采样得到 $$a \sim \pi_\theta(a \vert s)$$
2.  对 $$t = 1 \dots T$$：
   1.  采样得到回报 $$r_t \sim R(s, a)$$ 和下一状态 $$s' \sim P(s' \vert s, a)$$;
   2.  对下一步行动采样 $$a' \sim \pi_\theta(a' \vert s')$$;
   3.  更新策略参数：$$\theta \leftarrow \theta + \alpha_\theta Q_w(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)$$;
   4. 计算 $$t$$ 时刻的行动价值修正值（TD 误差）：<br/>$$\delta_t = r_t + \gamma Q_w(s', a') - Q_w(s, a)$$ <br/>并用它更新行动价值函数的参数：<br/>$$w \leftarrow w + \alpha_w \delta_t \nabla_w Q_w(s, a)$$
   5.  更新 $$a \leftarrow a'$$ 和 $$s \leftarrow s'$$

两个学习率，$$\alpha_\theta$$ 和 $$\alpha_w$$ ，是预先给行动策略和价值函数分别定好的更新参数。

#### 非策略策略梯度

REINFORCE 和普通的演员-评论家方法都是策略性的：通过目标策略收集训练样本，并以目标策略为优化对象。但相较而言，非策略性方法有些额外优势：

1.  非策略方法不需要完整的事件轨迹，而且可以复用任何过往事件（[“经历重放”](https://libertydream.github.io/2020/07/05/强化学习综述/#-深度-q-网络)）提高采样效率
2. 用来采样的行动策略和目标策略不同，[探索](https://libertydream.github.io/2020/07/05/强化学习综述/#探索利用窘境)效果更好

现在来看一下非策略梯度要怎么算。收集样本的行动策略是已知的（像超参数一样预先定好），记为 $$\beta(a \vert s)$$。该行动策略决定了状态分布，目标函数负责统计分布上的回报：


$$
J(\theta)
= \sum_{s \in \mathcal{S}} d^\beta(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s)
= \mathbb{E}_{s \sim d^\beta} \big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \big]
$$


其中 $$d^\beta(s)$$ 是行动策略 $$\beta$$ 的静态分布；回想一下 $$d^\beta(s) = \lim_{t \to \infty} P(S_t = s \vert S_0, \beta)$$ ；而 $$Q^\pi$$ 是目标策略 $$\pi$$ （不是行动策略）对应的行动价值函数。

既然训练观测是按 $$a \sim \beta(a \vert s)$$ 采样的，可以将梯度重写为：


$$
\begin{aligned}
\nabla_\theta J(\theta)
&= \nabla_\theta \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s)  \Big] & \\ 
&= \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} \big( Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) + \color{red}{\pi_\theta(a \vert s) \nabla_\theta Q^\pi(s, a)} \big) \Big] & \scriptstyle{\text{； 复合函数求导}}\\
&\stackrel{(i)}{\approx} \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) \Big] & \scriptstyle{\text{；忽略红字部分： } \color{red}{\pi_\theta(a \vert s) \nabla_\theta Q^\pi(s, a)}}. \\
&= \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} \beta(a \vert s) \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} \Big] & \\
&= \mathbb{E}_\beta \Big[\frac{\color{blue}{\pi_\theta(a \vert s)}}{\color{blue}{\beta(a \vert s)}} Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s) \Big] & \scriptstyle{\text{；蓝字部分是重要性权重}}
\end{aligned}
$$


其中 $$\frac{\pi_\theta(a \vert s)}{\beta(a \vert s)}$$ 代表[重要性](http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/)。因为 $$Q^\pi$$ 是目标策略的一个函数，所以也是个策略参数为 $$\theta$$ 的函数。按复合函数求导规则，我们也该求 $$\nabla_\theta Q^\pi(s, a)$$ 的导数，但现实中要算 $$\nabla_\theta Q^\pi(s, a)$$ 太难了。所幸可以忽略掉 Q 值求个近似梯度，仍能保证策略改善效果，并且最终取得实际局部最小值。证明见[论文](https://arxiv.org/pdf/1205.4839.pdf)（Degris, White 和 Sutton, 2012）

总结一下，非策略环境下用策略梯度的时候，可以简单地将其调整为加权和，权重就是目标策略和行动策略的比值，$$\frac{\pi_\theta(a \vert s)}{\beta(a \vert s)}$$。

#### A3C

[论文](https://arxiv.org/abs/1602.01783)|[代码](https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient/a3c)

**异步收益演员-评论家（Asynchronous Advantage Actor-Critic）**，简写作 A3C，是一种经典的专注于并行训练的策略梯度方法。

A3C 中，在评论家学习价值函数的同时，并行训练多个演员并且不时的通过全局参数进行同步。因此，可以说 A3C 是为并行训练而生的。

以状态价值函数为例。状态价值的损失函数是要最小化均方误差，$$J_v(w) = (G_t - V_w(s))^2$$ ，用梯度下降法寻找最优 $$w$$。同时，状态价值函数也是策略梯度更新的基础。

算法流程大致如下：

1.  有全局参数 $$\theta$$ 和 $$w$$；与之相应的有线程自用参数 $$\theta'$$ 和 $$w'$$
2.  初始化时间 $$t = 1$$
3.  当 $$T \leqslant T_\text{MAX}$$:
   1.  重置梯度：$$\mathrm{d}\theta = 0$$ ，$$\mathrm{d}w = 0$$
   2.  将线程参数与全局参数同步: $$\theta' = \theta$$，$$w' = w$$
   3.  令 $$t_\text{start} = t$$，采样得到起始状态 $$s_t$$
   4.  当 $$s_t \neq \text{TERMINAL}$$ 且 $$t - t_\text{start} \leqslant t_\text{max}$$:
      1.  选择行动 $$A_t \sim \pi_{\theta'}(A_t \vert S_t)$$，收获回报 $$R_t$$，抵达新状态 $$s_{t+1}$$
      2.  更新 $$t = t + 1$$， $$T = T + 1$$
   5.  初始化记录预期收益的变量 $$R = \begin{cases} 
      0 &  s_t \text{ 为 TERMINAL} \\
      V_{w'}(s_t) & \text{其他}
      \end{cases}$$
   6.  对 $$i = t-1, \dots, t_\text{start}$$:
      1.   $$R \leftarrow \gamma R + R_i$$; 这里 $$R$$ 是 $$G_i$$ 的 MC 度量
      2.  结合 $$\theta'$$ 更新梯度:<br/> $$\mathrm{d}\theta \leftarrow \mathrm{d}\theta + \nabla_{\theta'} \log \pi_{\theta'}(a_i \vert s_i)(R - V_{w'}(s_i))$$;<br/>结合 $$w'$$ 更新梯度：<br/>$$\mathrm{d}w \leftarrow \mathrm{d}w + 2 (R - V_{w'}(s_i)) \nabla_{w'} (R - V_{w'}(s_i))$$
   7.  同时用 $$\mathrm{d}\theta$$ 更新 $$\theta$$ , 用 $$\mathrm{d}w$$ 更新 $$w$$

A3C 支持多代理并行训练。累计梯度那步（6.2）可以看作是最小批随机梯度更新的并行版：每个线程独立在各自方向上对 $$w$$ 或 $$\theta$$ 进行一点修正

#### A2C

[论文](https://arxiv.org/abs/1602.01783)|[代码](https://github.com/openai/baselines/blob/master/baselines/a2c/a2c.py)

**A2C** 是同步确定版的 A3C，这也是叫 “A2C” 的原因，第一个 “A” (“asynchronous”) 被删了。A3C 里每个代理人独自和全局参数沟通，所以有时代理线程执行的是不同版本的策略，从而集成更新的并非是最优结果。为解决这种不一致性，更新全局参数前 A2C 中的协调器会等所有演员的工作完成，然后下一轮迭代的时候演员们会从相同策略起步。同步的梯度更新使训练更紧凑，无形中加快了收敛速度。

经[证实](https://blog.openai.com/baselines-acktr-a2c/)，A2C 在取得相同或强于 A3C 性能的基础上，GPU 利用率更高，大 batch 下工作效果更好。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_A3C_vs_A2C.png)

*图 2  A3C 与 A2C 架构对比*

#### DPG

[论文](https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf)|代码

上面介绍的方法里，策略函数 $$\pi(. \vert s)$$ 总表示为当前状态下行动 $$\mathcal{A}$$ 的概率分布，所以都是 *随机* 的。**既定策略梯度 （Deterministic policy gradient，DPG）** 则将策略作为确定的选择结果： $$a = \mu(s)$$。看上去可能有点怪——只有一个行动可选怎么计算行动概率的梯度呢？且容细细道来。

为便于讨论，更新一些符号含义：

- $$\rho_0(s)$$：状态的起始分布
- $$\rho^\mu(s \to s', k)$$：从状态 $$s$$ 开始，按策略 $$\mu$$ 走 $$k$$ 步后到达状态 $$s'$$ 的访问概率密度
- $$\rho^\mu(s')$$：带折扣的状态分布，具体为<br/> $$\rho^\mu(s') = \int_\mathcal{S} \sum_{k=1}^\infty \gamma^{k-1} \rho_0(s) \rho^\mu(s \to s', k) \mathrm{d}s$$

要优化的目标函数为：


$$
J(\theta) = \int_\mathcal{S} \rho^\mu(s) Q(s, \mu_\theta(s)) ds
$$


**既定策略梯度定理**：来算下梯度吧，按照链式法则，先取行动 $$a$$ 的 Q 梯度，然后再取 $$\theta$$ 下既定策略函数 $$\mu$$ 的梯度:


$$
\begin{aligned}
\nabla_\theta J(\theta) 
&= \int_\mathcal{S} \rho^\mu(s) \nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)} ds \\
&= \mathbb{E}_{s \sim \rho^\mu} [\nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}]
\end{aligned}
$$


可以把既定策略看作是随机情况的特例，即概率分布中只有一个非零值，对应某个行动。实际上在 DPG 论文中，作者指出如果随机策略 $$\pi_{\mu_\theta, \sigma}$$ 是经既定策略 $$\mu_\theta$$ 和变分变量 $$\sigma$$ 重新参数化了的，随机策略最终等价于 $$\sigma=0$$ 时的确定型情景。相较于既定策略，我们认为随机策略会需要更多样本，因为它统合了整个状态和行动空间的数据。

既定策略梯度定理可以和普通策略梯度框架结合。以策略性演员-评论家算法为例，算法每次迭代的时候，明确选定两个行动 $$a = \mu_\theta(s)$$ ，而 [SARSA](https://libertydream.github.io/2020/07/05/强化学习综述/#-sarsa策略性-td-控制) 用上面刚算过的新梯度更新策略参数：


$$
\begin{aligned}
\delta_t &= R_t + \gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t) & \scriptstyle{\text{; SARSA 的 TD 误差}}\\
w_{t+1} &= w_t + \alpha_w \delta_t \nabla_w Q_w(s_t, a_t) & \\
\theta_{t+1} &= \theta_t + \alpha_\theta \color{red}{\nabla_a Q_w(s_t, a_t) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}} & \scriptstyle{\text{; 既定策略梯度定理}}
\end{aligned}
$$



但除非环境内有足够的噪声，因为策略的确定性情导致[探索](https://libertydream.github.io/2020/07/05/强化学习综述/#探索利用窘境)会不怎么充分。可以选择往策略里加点噪声（讽刺地是这样策略就不是确定的了）或者通过一个不同的随机行动策略收集样本，进行非策略地学习。

非策略方法中训练轨迹是由随机策略 $$\beta(a \vert s)$$ 生成的，所以状态分布遵从相应的折扣状态密度 $$\rho^\beta$$：


$$
\begin{aligned}
J_\beta(\theta) &= \int_\mathcal{S} \rho^\beta Q^\mu(s, \mu_\theta(s)) ds \\
\nabla_\theta J_\beta(\theta) &= \mathbb{E}_{s \sim \rho^\beta} [\nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s)  \rvert_{a=\mu_\theta(s)} ]
\end{aligned}
$$


因为策略是确定的，我们只需要 $$Q^\mu(s, \mu_\theta(s))$$ 而不是用 $$\sum_a \pi(a \vert s) Q^\pi(s, a)$$ 作为给定状态 $$s$$ 的预期回报。用随机策略的非策略方法，通常会借重要度采样来修正行动和目标策略间的失调，如前文所述。但因为既定策略梯度删掉了对行动的积分，也就不需要重要度采样了。

#### DDPG

[论文](https://arxiv.org/pdf/1509.02971.pdf)|[代码](https://github.com/openai/baselines/tree/master/baselines/ddpg)

**深度既定策略梯度（Deep Deterministic Policy Gradient）**，简写为 **DDPG**，是一个免于建模的非策略性演员-评论家算法，结合了 [DPG](# DPG) 和 [DQN](https://libertydream.github.io/2020/07/05/强化学习综述/#-深度-q-网络)。记得吗，DQN 通过经历重放和冻结目标网络稳定了 Q 函数学习过程。原始 DQN 网络是用在离散空间上，DDPG 借演员-评论家框架将其拓展到连续空间，同时学习一个确定的策略。

为了更好的探索，给策略中加上噪声 $$\mathcal{N}$$ 得到探索策略 $$\mu'$$  :


$$
\mu'(s) = \mu_\theta(s) + \mathcal{N}
$$


此外，DDPG 无论是对演员还是评论家都是采用软更新（“保守策略迭代”）， $$\tau \ll 1$$: $$\theta' \leftarrow \tau \theta + (1 - \tau) \theta'$$。这样目标网络价值被迫只能缓慢变化，这与 DQN 中定期冻结目标网络的做法不同。

论文中有个细节对机器人技术格外有帮助——怎样对低维特征的不同物理单元作归一化。例如，输入机器人的位置和速度让模型学习策略，这些物理统计信息天然不同，甚至同类统计项的值在机器人间也大相径庭。通过对最小批中样本的每个维度进行[批归一化](http://proceedings.mlr.press/v37/ioffe15.pdf)，从而解决问题。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_DDPG_algo.png)

*图 3  DDPG 算法*

#### D4PG

[论文](https://openreview.net/forum?id=SyZipzbCb)|代码

**分布式分发 DDPG（Distributed Distributional DDPG，D4PG）**对 DDPG 进行了一系列改良使其能分布式运行

（1）**分布式评论家**：评论家将 Q 值的期望作为随机变量——一个经 $$w$$ 参数化的分布 $$Z_w$$，所以 $$Q_w(s, a) = \mathbb{E} Z_w(x, a)$$。学习分布参数期间的损失就是最小化两个分布间的距离——分布式 TD 误差：$$L(w) = \mathbb{E}[d(\mathcal{T}_{\mu_\theta}, Z_{w'}(s, a), Z_w(s, a)]$$，其中 $$\mathcal{T}_{\mu_\theta}$$ 是贝尔曼运算符。

既定策略梯度更新形式变成：


$$
\begin{aligned}
\nabla_\theta J(\theta) 
&\approx \mathbb{E}_{\rho^\mu} [\nabla_a Q_w(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}] & \scriptstyle{\text{;  DPG 的梯度更新}} \\
&= \mathbb{E}_{\rho^\mu} [\mathbb{E}[\nabla_a Q_w(s, a)] \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}] & \scriptstyle{\text{; Q 值分布的期望}}
\end{aligned}
$$


（2）**$$N$$ 步收益**：计算 TD 误差的时候，D4PG 会算 $$N$$ 步 TD 目标值而非一步，以此整合未来几步内的收益。所以新 TD 目标为：


$$
r(s_0, a_0) + \mathbb{E}[\sum_{n=1}^{N-1} r(s_n, a_n) + \gamma^N Q(s_N, \mu_\theta(s_N)) \vert s_0, a_0 ]
$$


（3）**多演员分布式并行**：D4PG 用 $$K$$ 个独立演员并行收集经历，并将数据传到相同的回放缓存中

（4）**带优先级的经历重放（PER）**：最后一处修改是以非均匀概率 $$p_i$$ 从大小为 $$R$$  的回放缓存中进行采样。这样样本 $$i$$ 有 $$(Rp_i)^{-1}$$ 的概率被选中，所以重要度权重为 $$(Rp_i)^{-1}$$

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_D4PG_algo.png)

*图 4  D4PG 算法；注意原论文中的变量字母和本文略有不同，我用 $$\mu(.)$$ 表示既定策略而不是 $$\pi(.)$$*

#### MADDPG

[论文](https://arxiv.org/pdf/1706.02275.pdf)|[代码](https://github.com/openai/maddpg)

**多代理 DDPG（Multi-agent DDPG，MADDPG）**拓展了 DDPG 适用范围，多个代理间只用局部信息就能协调配合完成任务。在代理人的视角中环境是不稳定的，因为其他代理人的策略更新的很快，自己也不知道其中细节。MADDPG 是专为解决这类环境变化问题和代理间沟通问题而重新设计出来的一个演员-评论家模型。

正经话讲，上述问题可以看作是多代理版的 MDP，也称为*马尔可夫游戏*。MADDPG 为局部可观测的马尔可夫游戏而生。比方说，有 $$N$$ 个代理和一系列状态 $$\mathcal{S}$$，每个代理有一组可能的行动，$$\mathcal{A}_1, \dots, \mathcal{A}_N$$，以及一组观测 $$\mathcal{O}_1, \dots, \mathcal{O}_N$$。状态转移函数囊括整个状态、行动和观测空间 $$\mathcal{T}: \mathcal{S} \times \mathcal{A}_1 \times \dots \mathcal{A}_N \mapsto \mathcal{S}$$。各个代理的随机策略只包括它自己的状态和行动：$$\pi_{\theta_i}: \mathcal{O}_i \times \mathcal{A}_i \mapsto [0, 1]$$，一个结合自身观测或既定策略的行动概率分布： $$\mu_{\theta_i}: \mathcal{O}_i \mapsto \mathcal{A}_i$$

令 $$\vec{o} = {o_1, \dots, o_N}$$, $$\vec{\mu} = {\mu_1, \dots, \mu_N}$$ 并用 $$\vec{\theta} = {\theta_1, \dots, \theta_N}$$ 对策略进行参数化

MADDPG 中的评论家要为 $$i$$ 号代理学习一个中心化的行动价值函数 $$Q^\vec{\mu}_i(\vec{o}, a_1, \dots, a_N)$$  ，其中 $$a_1 \in \mathcal{A}_1, \dots, a_N \in \mathcal{A}_N$$ 是所有代理的行动。对 $$i=1, \dots, N$$ 每个 $$Q^\vec{\mu}_i$$ 都独自进行学习，因而不同代理的回报结构可能五花八门，竞争环境会出现相互冲突的回报。 与此同时，与各代理一一对应的演员们，也在探索和更新自己的策略参数 $$\theta_i$$。

**演员更新：**


$$
\nabla_{\theta_i} J(\theta_i) = \mathbb{E}_{\vec{o}, a \sim \mathcal{D}} [\nabla_{a_i} Q^{\vec{\mu}}_i (\vec{o}, a_1, \dots, a_N) \nabla_{\theta_i} \mu_{\theta_i}(o_i) \rvert_{a_i=\mu_{\theta_i}(o_i)} ]
$$


其中 $$\mathcal{D}$$ 是用来回放经历的记忆缓存，其中包括多个事件样本 $$(\vec{o}, a_1, \dots, a_N, r_1, \dots, r_N, \vec{o}')$$——给定当前观测 $$\vec{o}$$，代理采取行动 $$a_1, \dots, a_N$$ 并获得回报 $$r_1, \dots, r_N$$，得到新观测 $$\vec{o}'$$

**评论家更新：**


$$
\begin{aligned}
\mathcal{L}(\theta_i) &= \mathbb{E}_{\vec{o}, a_1, \dots, a_N, r_1, \dots, r_N, \vec{o}'}[ (Q^{\vec{\mu}}_i(\vec{o}, a_1, \dots, a_N) - y)^2 ] & \\
\text{其中 } y &= r_i + \gamma Q^{\vec{\mu}'}_i (\vec{o}', a'_1, \dots, a'_N) \rvert_{a'_j = \mu'_{\theta_j}} & \scriptstyle{\text{; TD 目标}}
\end{aligned}
$$


其中 $$\vec{\mu}'$$ 是目标策略，带延迟性软更新参数。

如果评论家更新期间策略 $$\vec{\mu}$$ 未知，可以让各代理学习并推演其他代理策略的近似结果。用近似策略 MADDPG 也仍就可以高效学习，尽管推断出的策略可能不准。

要降低由相互竞争和代理协同引起的高方差，MADDPG 给了个办法——*策略集成*：

1. 给一个代理训练 $$K$$ 个策略
2. 给出现的事件随机选定一个策略
3. 集成 $$K$$ 个策略进行梯度更新

总结一下，MADDPG 在 DDGP 之上加了三个部件使其适应于多代理环境：

- 中心化的评论家+去中心化的演员
- 演员可以估算其他代理的策略进行学习
- 策略集成有助于减少方差

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_MADDPG.png)

*图 5  MADDPG 架构设计*

#### TRPO

[论文](https://arxiv.org/pdf/1502.05477.pdf)|[代码](https://github.com/openai/baselines/tree/master/baselines/trpo_mpi)

为改善训练稳定性，应该避免参数波动使策略变化跨步太大。**置信域策略优化（Trust region policy optimization，TRPO）**践行了这一想法，对每轮策略更新的幅度都加上了[ KL 散度](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#kullbackleibler-and-jensenshannon-divergence)约束。

试想一个用非策略 RL 的场景，收集轨迹的策略 $$\beta$$ 和要优化的策略 $$\pi$$ 不同。非策略模型的目标函数是要计算状态分布和行动带来的总收益，而训练数据分布和真实策略状态分布间的失调由重要度采样估计函数进行弥补。


$$
\begin{aligned}
J(\theta)
&= \sum_{s \in \mathcal{S}} \rho^{\pi_{\theta_\text{old}}} \sum_{a \in \mathcal{A}} \big( \pi_\theta(a \vert s) \hat{A}_{\theta_\text{old}}(s, a) \big) & \\
&= \sum_{s \in \mathcal{S}} \rho^{\pi_{\theta_\text{old}}} \sum_{a \in \mathcal{A}} \big( \beta(a \vert s) \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} \hat{A}_{\theta_\text{old}}(s, a) \big) & \scriptstyle{\text{; 重要度采样}} \\
&= \mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}}, a \sim \beta} \big[ \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} \hat{A}_{\theta_\text{old}}(s, a) \big] &
\end{aligned}
$$


其中 $$\theta_\text{old}$$ 是更新前的策略参数，是已知条件。$$\rho^{\pi_{\theta_\text{old}}}$$ 的定义和[前文](# DPG)一致；$$\beta(a \vert s)$$ 是负责收集轨迹的行动策略。注意我们用的是预估收益 $$\hat{A}(.)$$ 而不是实际收益函数 $$A(.)$$。因为通常无法获知实际回报。

按策略训练时，理论上收集数据的策略应该和要优化的策略一样才对。但当运算器和优化器异步并行运作的时候，行动策略可能会过期。TPRO 考虑到这点微小差异：将行动策略记为 $$\pi_{\theta_\text{old}}(a \vert s)$$ 所以目标函数变成：


$$
J(\theta) = \mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}} \big[ \frac{\pi_\theta(a \vert s)}{\pi_{\theta_\text{old}}(a \vert s)} \hat{A}_{\theta_\text{old}}(s, a) \big]
$$


TPRO 的目标是最大化目标函数 $$J(\theta)$$，但要受*置信域约束* 管制，新旧策略间的 KL 散度值要足够小，小于参数 $$\delta$$：


$$
\mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}}} [D_\text{KL}(\pi_{\theta_\text{old}}(.\vert s) \| \pi_\theta(.\vert s)] \leqslant \delta
$$



这样，在约束条件下新旧策略间的差异不会太大。同时 TRPO 保证策略迭代期间会不断改善效果（很妙，对吧？）感兴趣的话可以看看[论文](https://arxiv.org/pdf/1502.05477.pdf)中的证明

#### PPO

[论文](https://arxiv.org/pdf/1707.06347.pdf)|[代码](https://github.com/openai/baselines/tree/master/baselines/ppo1)

TRPO 有点复杂，但我们还想实现类似的约束效果，**近似策略优化（proximal policy optimization，PPO）**精简了目标，同时保留了相近的性能。

首先，将新旧策略间的概率比值记为：


$$
r(\theta) = \frac{\pi_\theta(a \vert s)}{\pi_{\theta_\text{old}}(a \vert s)}
$$



然后 TPRO（策略性的）的目标函数就变成：


$$
J^\text{TRPO} (\theta) = \mathbb{E} [ r(\theta) \hat{A}_{\theta_\text{old}}(s, a) ]
$$



若 $$\theta_\text{old}$$ 和 $$\theta$$ 之间没有限制，最大化 $$J^\text{TRPO} (\theta)$$ 时会因过大的参数更新幅度和大额策略比例导致状态不稳定。PPO 通过强令 $$r(\theta)$$ 处于间隔在 1 左右的小区间内来施加约束，准确地讲是 $$[1-\epsilon, 1+\epsilon]$$，其中 $$\epsilon$$ 是超参数。


$$
J^\text{CLIP} (\theta) = \mathbb{E} [ \min( r(\theta) \hat{A}_{\theta_\text{old}}(s, a), \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_{\theta_\text{old}}(s, a))]
$$


函数 $$\text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon)$$ 会将比率限制在上到 $$1+\epsilon$$ 下至 $$1-\epsilon$$ 的范围内。PPO 的目标函数会选择原始值和剪切版中较小的一个，这样就不会因追求极致回报而扩大策略更新幅度了。

若策略（演员）和价值（评论家）函数共享参数，在将 PPO 用到网络架构上的时候，除了修剪过的回报，目标函数因价值估计（红色部分）上加了误差项而得到增强，因加了项信息熵（蓝色部分）可以更充分地探索。


$$
J^\text{CLIP'} (\theta) = \mathbb{E} [ J^\text{CLIP} (\theta) - \color{red}{c_1 (V_\theta(s) - V_\text{target})^2} + \color{blue}{c_2 H(s, \pi_\theta(.))} ]
$$


其中 $$c_1$$ 和 $$c_2$$ 是两个常数类的超参数。

经一系列基准任务测试，证实 PPO 虽经大幅简化，效果还是很好。

#### ACER

[论文](https://arxiv.org/pdf/1611.01224.pdf)|[代码](https://github.com/openai/baselines/tree/master/baselines/acer)

**经历重放演员-评论家算法（actor-critic with experience replay）**，简写为 **ACER** ，是一个带经历回放机制的非策略性演员-评论家模型，极大增强了采样效率并减少了数据勾连。A3C 给 ACER 打下了基础，但那是策略性的；ACER 算是非策略版 A3C。使A3C 摆脱策略的主要困难在于怎么控制非策略估计函数的稳定性。ACER 对此进行了三项设计：

- 采用回溯 Q 值估计；
- 用偏差校正截断重要度权重
- 使用高效的 TRPO

**回溯 Q 值估计**

[回溯](http://papers.nips.cc/paper/6538-safe-and-efficient-off-policy-reinforcement-learning.pdf)是种非策略、基于收益的 Q 值估计算法，保证对任何目标、行动策略对 $$(\pi, \beta)$$ 都能收敛，数据效力也同样不俗。

回想一下 TD 学习是怎么预测的：

1.  计算 TD 误差：$$\delta_t = R_t + \gamma \mathbb{E}_{a \sim \pi} Q(S_{t+1}, a) - Q(S_t, A_t)$$；其中 $$r_t + \gamma \mathbb{E}_{a \sim \pi} Q(s_{t+1}, a) $$ 就是 “TD 目标”。用期望 $$\mathbb{E}_{a \sim \pi}$$ 是因为后续几步我们能做出的最好估计就是照当前策略 $$\pi$$ 走
2. 修正误差来更新值：$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \delta_t$$。换句话说，Q 的增量与 TD 误差呈比例关系： $$\Delta Q(S_t, A_t) = \alpha \delta_t$$

如果是非策略性事件轨迹，Q 值更新里需要加上重要度采样


$$
\Delta Q^\text{imp}(S_t, A_t) 
= \gamma^t \prod_{1 \leqslant \tau \leqslant t} \frac{\pi(A_\tau \vert S_\tau)}{\beta(A_\tau \vert S_\tau)} \delta_t
$$



重要度权重的乘积看起来非常吓人，不难想象它带来的方差有多大，甚至可以大到天上去。回溯 Q 值估计方法修改了 $$\Delta Q$$，令权重不能超过常量 $$c$$ 从而简化了重要度权重计算：


$$
\Delta Q^\text{ret}(S_t, A_t) 
= \gamma^t \prod_{1 \leqslant \tau \leqslant t} \min(c, \frac{\pi(A_\tau \vert S_\tau)}{\beta(A_\tau \vert S_\tau)})  \delta_t
$$


ACER 将 $$Q^\text{ret}$$ 作为目标，通过最小化 L2 误差 $$(Q^\text{ret}(s, a) - Q(s, a))^2$$ 训练评论家。

**重要度权重截断**

为减少策略梯度 $$\hat{g}$$ 的方差，ACER 会用常量 $$c$$ 将重要度权重截断，加了个修正项。$$\hat{g}_t^\text{acer}$$ 是 $$t$$ 时刻的 ACER 策略梯度。


$$
\begin{aligned}
\hat{g}_t^\text{acer}
= & \omega_t \big( Q^\text{ret}(S_t, A_t) - V_{\theta_v}(S_t) \big) \nabla_\theta \ln \pi_\theta(A_t \vert S_t) 
  & \scriptstyle{\text{; 令 }\omega_t=\frac{\pi(A_t \vert S_t)}{\beta(A_t \vert S_t)}} \\
= & \color{blue}{\min(c, \omega_t) \big( Q^\text{ret}(S_t, A_t) - V_w(S_t) \big) \nabla_\theta \ln \pi_\theta(A_t \vert S_t)} \\
  & + \color{red}{\mathbb{E}_{a \sim \pi} \big[ \max(0, \frac{\omega_t(a) - c}{\omega_t(a)}) \big( Q_w(S_t, a) - V_w(S_t) \big) \nabla_\theta \ln \pi_\theta(a \vert S_t) \big]}
  & \scriptstyle{\text{; 令 }\omega_t (a) =\frac{\pi(a \vert S_t)}{\beta(a \vert S_t)}}
\end{aligned}
$$


其中 $$Q_w(.)$$ 和 $$V_w(.)$$ 是参数为 $$w$$ 的评论家预测的价值函数。第一项（蓝色）带有裁剪过的重要度权重。除了减掉当基准的状态价值函数 $$V_w(.)$$ ，裁剪同样有利于减少方差。第二项（红色）是为实现无偏估计而做的修正。

**高效 TRPO**

此外，ACER 还借鉴了 TRPO 的思想，只是做了点小改动使计算更高效：相较于计算更新前后两策略间的 KL 散度，ACER 维护了一个过往策略的动态均值，并迫使更新后的策略不会偏离均值太远。

ACER 的论文充满了各种方程，所幸有了关于 TD 学习，Q 学习，重要度采样和 TRPO  的知识铺垫，你会发现论文也没那么难懂了。

#### ACTKR

[论文](https://arxiv.org/pdf/1708.05144.pdf)|[代码](https://github.com/openai/baselines/tree/master/baselines/acktr)

**带克罗内克因子置信域的演员-评论家（actor-critic using Kronecker-factored trust region）算法**，简记为 ACKTR，提出用克罗内克因子近似曲率（Kronecker-factored approximation curvature, [K-FAC](https://arxiv.org/pdf/1503.05671.pdf)）给评论家和演员更新梯度。K-FAC 对*自然梯度* 做了改良，自然梯度和常见的*标准梯度* 十分不同。这篇[文章](http://kvfrans.com/a-intuitive-explanation-of-natural-gradient-descent/)很好的对自然梯度做了直观解释。一句话总结大概是：

> 首先考虑能使新旧网络保持恒定 KL 散度的所有参数组合，该散度值可以看作是步长或学习率。在所有可能组合中选择能最小化损失函数的那个

这里列出 ACTKR 主要是保证文章完整性，但不会在此深究细节，因为涉及到大量有关自然梯度和优化论的理论知识。如果感兴趣的话，读 ACKTR 论文前先看看这些论文/文章：

- Amari. [自然梯度促进高效学习](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf). 1998
- Kakade. [自然策略梯度](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf). 2002
- [自然梯度下降的直观解释](http://kvfrans.com/a-intuitive-explanation-of-natural-gradient-descent/)
- [维基: 克罗内克积](https://en.wikipedia.org/wiki/Kronecker_product)
- Martens & Grosse. [用克罗内克因子近似曲率优化神经网络](http://proceedings.mlr.press/v37/martens15.pdf) 2015

K-FAC 论文里有一高度概括总结：

> 近似分两步。首先将费希尔矩阵的行列分组，每组对应给定层的所有权重，这也使得矩阵区块化。之后将这些块作为更小矩阵间的克罗内克积的近似，这等价于基于网络梯度的统计信息而作的某些近似假设

> 第二步，矩阵进一步近似为可逆的区块化对角或区块化三角矩阵。我们仔细检查了逆协方差、树状图模型和线性回归间的关系，证实该近似可靠。特别要说，该证明并不针对费希尔矩阵本身，我们的实验证明虽然逆费希尔矩阵有这种结构（近似地），费希尔矩阵本身却并没有

#### SAC

[论文](https://arxiv.org/abs/1801.01290)|[代码](https://github.com/haarnoja/sac)

**柔性演员-评论家（Soft Actor-Critic，SAC）**算法为鼓励探索把策略的熵加到了回报里：我们希望能学到一种策略，完成任务的同时尽可能的使行动随机化。这是个遵从最大熵强化学习框架的非策略演员-评论家模型。早先工作是[柔性 Q 学习](https://arxiv.org/abs/1702.08165)。

SAC 的关键部分有三：

- 一个有独立策略和价值函数网络的[演员-评论家](# 演员-评论家)框架
- 一个[非策略](# 非策略策略梯度)公式，能复用早先收集到的数据提高效率
- 熵最大化保证稳定度和探索度

策略训练目标是同时最大化期望收益和熵：


$$
J(\theta) = \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi_\theta}} [r(s_t, a_t) + \alpha \mathcal{H}(\pi_\theta(.\vert s_t))]
$$



其中 $$\mathcal{H}(.)$$ 是熵，$$\alpha$$ 是熵的重要度，也叫*温度* 参数。熵最大化使策略能：（1）更多的进行探索（2）得到接近最优策略的若干模型（即是说，如果有几个选项似乎一样好，策略该给每个分配相等的被选概率）

准确地讲，SAC 要学三个函数：

- 参数为 $$\theta$$ 的策略 $$\pi_\theta$$ 
-  参数为 $$w$$ 的柔性 Q 值函数 $$Q_w$$
-  参数为 $$\psi$$ 的柔性状态价值函数 $$V_\psi$$；理论上知道 $$Q$$ 和 $$\pi$$ 可以推断 $$V$$，但在实践中它有利于稳定训练

柔性 Q 值和柔性状态值的定义如下：


$$
\begin{aligned}
Q(s_t, a_t) &= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s)} [V(s_{t+1})] & \text{; 贝尔曼方程}\\
\text{其中 }V(s_t) &= \mathbb{E}_{a_t \sim \pi} [Q(s_t, a_t) - \alpha \log \pi(a_t \vert s_t)] & \text{; 柔性状态价值函数}
\end{aligned}
$$

$$
\text{所以, } Q(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{(s_{t+1}, a_{t+1}) \sim \rho_{\pi}} [Q(s_{t+1}, a_{t+1}) - \alpha \log \pi(a_{t+1} \vert s_{t+1})]
$$



$$\rho_\pi(s)$$ 和 $$\rho_\pi(s, a)$$ 的定义和 [DPG](# DPG) 一节中的类似。

柔性状态价值函数的训练目标是最小化均方误差：


$$
\begin{aligned}
J_V(\psi) &= \mathbb{E}_{s_t \sim \mathcal{D}} [\frac{1}{2} \big(V_\psi(s_t) - \mathbb{E}[Q_w(s_t, a_t) - \log \pi_\theta(a_t \vert s_t)] \big)^2] \\
\text{梯度: }\nabla_\psi J_V(\psi) &= \nabla_\psi V_\psi(s_t)\big( V_\psi(s_t) - Q_w(s_t, a_t) + \log \pi_\theta (a_t \vert s_t) \big)
\end{aligned}
$$


其中 $$\mathcal{D}$$ 是回放缓存

柔性 Q 函数的训练目标是最小化柔性贝尔曼残差：


$$
\begin{aligned}
J_Q(w) &= \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}} [\frac{1}{2}\big( Q_w(s_t, a_t) - (r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_\pi(s)}[V_{\bar{\psi}}(s_{t+1})]) \big)^2] \\
\text{梯度: } \nabla_w J_Q(w) &= \nabla_w Q_w(s_t, a_t) \big( Q_w(s_t, a_t) - r(s_t, a_t) - \gamma V_{\bar{\psi}}(s_{t+1})\big) 
\end{aligned}
$$


其中 $$\bar{\psi}$$ 是目标价值函数，是指数变化的均值（或者只是“硬性”定期更新），就像 DQN 里为稳定训练而对目标 Q 网络的参数所作的那样。

SAC 更新策略，使 KL 散度最小化：


$$
\begin{aligned}
\pi_\text{new} 
&= \arg\min_{\pi' \in \Pi} D_\text{KL} \Big( \pi'(.\vert s_t) \| \frac{\exp(Q^{\pi_\text{old}}(s_t, .))}{Z^{\pi_\text{old}}(s_t)} \Big) \\[6pt]
&= \arg\min_{\pi' \in \Pi} D_\text{KL} \big( \pi'(.\vert s_t) \| \exp(Q^{\pi_\text{old}}(s_t, .) - \log Z^{\pi_\text{old}}(s_t)) \big) \\[6pt]
\text{更新目标： } J_\pi(\theta) &= \nabla_\theta D_\text{KL} \big( \pi_\theta(. \vert s_t) \| \exp(Q_w(s_t, .) - \log Z_w(s_t)) \big) \\[6pt]
&= \mathbb{E}_{a_t\sim\pi} \Big[ - \log \big( \frac{\exp(Q_w(s_t, a_t) - \log Z_w(s_t))}{\pi_\theta(a_t \vert s_t)} \big) \Big] \\[6pt]
&= \mathbb{E}_{a_t\sim\pi} [ \log \pi_\theta(a_t \vert s_t) - Q_w(s_t, a_t) + \log Z_w(s_t) ]
\end{aligned}
$$


其中 $$\Pi$$ 是一系列可对策略建模的潜在策略，目的在于使其好处理；比如，$$\Pi$$ 可以是高斯混合分布家族，建模成本高但表达能力强而且好处理。$$Z^{\pi_\text{old}}(s_t)$$ 是配分函数，负责对分布归一化，通常比较棘手但对梯度没影响。怎么最小化 $$J_\pi(\theta)$$ 取决于对 $$\Pi$$ 的选择。

该更新过程能保证 $$Q^{\pi_\text{new}}(s_t, a_t) \geqslant Q^{\pi_\text{old}}(s_t, a_t)$$，证明见原[论文](https://arxiv.org/abs/1801.01290)附录 B.2。

当我们定好了目标函数，以及柔性行动-状态值、柔性状态值和策略网络的梯度后，柔性演员-评论家算法就很直观了：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_SAC_algo.png)

*图 6  柔性演员-评论家算法*

#### 自动控温的 SAC

[论文](https://arxiv.org/abs/1812.05905)|[代码](https://github.com/rail-berkeley/softlearning)

就温度参数来讲 SAC 很脆弱，不幸的是调控温度本就很难。因为随着策略效果越来越好，无论是任务期间还是训练过程中，熵会怎么变都是乖谬难测。由此可以对 SAC 进行改良，构建一个有约束的优化问题：最大化期望收益的同时，策略要满足一个最小熵约束：


$$
\max_{\pi_0, \dots, \pi_T} \mathbb{E} \Big[ \sum_{t=0}^T r(s_t, a_t)\Big] \text{s.t. } \forall t\text{, } \mathcal{H}(\pi_t) \geqslant \mathcal{H}_0
$$



其中 $$\mathcal{H}_0$$ 是预先定好的策略熵的最低门槛。

预期收益 $$\mathbb{E} \Big[ \sum_{t=0}^T r(s_t, a_t)\Big]$$ 可以分解成各时刻回报的和。因为 $$t$$ 时刻的策略 $$\pi_t$$ 对之前的策略 $$\pi_{t-1}$$ 没影响，我们可以回退令各步收益最大——这实际上是**动态规划（DP）**。


$$
\underbrace{\max_{\pi_0} \Big( \mathbb{E}[r(s_0, a_0)]+ \underbrace{\max_{\pi_1} \Big(\mathbb{E}[...] + \underbrace{\max_{\pi_T} \mathbb{E}[r(s_T, a_T)]}_\text{初次最大化} \Big)}_\text{倒数第二次最大化} \Big)}_\text{最后一次最大化}
$$


令 $$\gamma=1$$，所以从最后一步 $$T$$ 开始优化：


$$
\max \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) ] \text{ s.t. } \mathcal{H}(\pi_T) - \mathcal{H}_0 \geqslant 0
$$



首先定义下列函数：


$$
\begin{aligned}
h(\pi_T) &= \mathcal{H}(\pi_T) - \mathcal{H}_0 = \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [-\log \pi_T(a_T\vert s_T)] - \mathcal{H}_0\\
f(\pi_T) &= \begin{cases}
\mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) ], & \text{如果 }h(\pi_T) \geqslant 0 \\
-\infty, & \text{其他}
\end{cases}
\end{aligned}
$$



然后最优化问题就变成：


$$
\max f(\pi_T) \text{ s.t. } h(\pi_T) \geqslant 0
$$


要解带不等式约束的最优化问题，可以用拉格朗日乘子（也叫“对偶变量”）$$\alpha_T$$ 构建[拉格朗日表达式](https://cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf)：


$$
L(\pi_T, \alpha_T) = f(\pi_T) + \alpha_T h(\pi_T)
$$


试想给定特定值 $$\pi_T$$ 时*结合 $$\alpha_T$$ 求 $$L(\pi_T, \alpha_T)$$ 最小化* 的场景

- 如果满足约束条件， $$h(\pi_T) \geqslant 0$$，最好情况是设 $$\alpha_T=0$$，因为对 $$f(\pi_T)$$ 没约束。所以 $$L(\pi_T, 0) = f(\pi_T)$$
-  如果不满足约束条件，$$h(\pi_T) < 0$$，可以取 $$\alpha_T \to \infty$$ 使 $$L(\pi_T, \alpha_T) \to -\infty$$ 。所以 $$L(\pi_T, \infty) = -\infty = f(\pi_T)$$

无论哪种情况我们都能得到下列等式：


$$
f(\pi_T) = \min_{\alpha_T \geqslant 0} L(\pi_T, \alpha_T)
$$



同时，还要最大化 $$f(\pi_T)$$


$$
\max_{\pi_T} f(\pi_T) = \min_{\alpha_T \geqslant 0} \max_{\pi_T} L(\pi_T, \alpha_T)
$$


所以为了最大化 $$f(\pi_T)$$，对偶问题列在下面了。注意要想保证 $$\max_{\pi_T} f(\pi_T)$$ 取得最大的同时取值不为 $$-\infty$$，必须满足约束条件。


$$
\begin{aligned}
\max_{\pi_T} \mathbb{E}[ r(s_T, a_T) ]
&= \max_{\pi_T} f(\pi_T) \\
&= \min_{\alpha_T \geqslant 0}  \max_{\pi_T} L(\pi_T, \alpha_T) \\
&= \min_{\alpha_T \geqslant 0}  \max_{\pi_T} f(\pi_T) + \alpha_T h(\pi_T) \\ 
&= \min_{\alpha_T \geqslant 0}  \max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) ] + \alpha_T ( \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [-\log \pi_T(a_T\vert s_T)] - \mathcal{H}_0) \\ 
&= \min_{\alpha_T \geqslant 0}  \max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T)  - \alpha_T \log \pi_T(a_T\vert s_T)] - \alpha_T \mathcal{H}_0 \\
&= \min_{\alpha_T \geqslant 0}  \max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T)  + \alpha_T \mathcal{H}(\pi_T) - \alpha_T \mathcal{H}_0 ]
\end{aligned}
$$


可以交替计算最优的 $$\pi_T$$ 和 $$\alpha_T$$。首先给定当前 $$\alpha_T$$，得到最佳策略 $$\pi_T^{*}$$，使表达式最大 $$L(\pi_T^{*}, \alpha_T)$$。然后带入 $$\pi_T^{*}$$ 并计算 $$\alpha_T^{*}$$ ，使 $$L(\pi_T^{*}, \alpha_T)$$ 最小化。假设我们有一个神经网络学策略，一个网络学温度参数，具体怎么迭代更新要看我们训练的时候怎么更新网络参数。


$$
\begin{aligned}
\pi^{*}_T
&= \arg\max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T)  + \alpha_T \mathcal{H}(\pi_T) - \alpha_T \mathcal{H}_0 ] \\
\color{blue}{\alpha^{*}_T}
&\color{blue}{=} \color{blue}{\arg\min_{\alpha_T \geqslant 0} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi^{*}}} [\alpha_T \mathcal{H}(\pi^{*}_T) - \alpha_T \mathcal{H}_0 ]}
\end{aligned}
$$

$$
\text{所以， }\max_{\pi_T} \mathbb{E} [ r(s_T, a_T) ] 
= \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi^{*}}} [ r(s_T, a_T)  + \alpha^{*}_T \mathcal{H}(\pi^{*}_T) - \alpha^{*}_T \mathcal{H}_0 ]
$$



现在回到柔性 Q 值函数：


$$
\begin{aligned}
Q_{T-1}(s_{T-1}, a_{T-1}) 
&= r(s_{T-1}, a_{T-1}) + \mathbb{E} [Q(s_T, a_T) - \alpha_T \log \pi(a_T \vert s_T)] \\
&= r(s_{T-1}, a_{T-1}) + \mathbb{E} [r(s_T, a_T)] + \alpha_T \mathcal{H}(\pi_T) \\
Q_{T-1}^{*}(s_{T-1}, a_{T-1}) 
&= r(s_{T-1}, a_{T-1}) + \max_{\pi_T} \mathbb{E} [r(s_T, a_T)] +  \alpha_T \mathcal{H}(\pi^{*}_T) & \text{; 带入最优值 }\pi_T^{*}
\end{aligned}
$$


所以倒退一步回到 $$T-1$$ 时，所获期望收益为：


$$
\begin{aligned}
&\max_{\pi_{T-1}}\Big(\mathbb{E}[r(s_{T-1}, a_{T-1})] + \max_{\pi_T} \mathbb{E}[r(s_T, a_T] \Big) \\
&= \max_{\pi_{T-1}} \Big( Q^{*}_{T-1}(s_{T-1}, a_{T-1}) - \alpha^{*}_T \mathcal{H}(\pi^{*}_T) \Big) & \text{; 要满足 } \mathcal{H}(\pi_{T-1}) - \mathcal{H}_0 \geqslant 0 \\
&= \min_{\alpha_{T-1} \geqslant 0}  \max_{\pi_{T-1}} \Big( Q^{*}_{T-1}(s_{T-1}, a_{T-1}) - \alpha^{*}_T \mathcal{H}(\pi^{*}_T) + \alpha_{T-1} \big( \mathcal{H}(\pi_{T-1}) - \mathcal{H}_0 \big) \Big) & \text{; 对偶问题 / 拉格朗日} \\
&= \min_{\alpha_{T-1} \geqslant 0}  \max_{\pi_{T-1}} \Big( Q^{*}_{T-1}(s_{T-1}, a_{T-1}) + \alpha_{T-1} \mathcal{H}(\pi_{T-1}) - \alpha_{T-1}\mathcal{H}_0 \Big) - \alpha^{*}_T \mathcal{H}(\pi^{*}_T)
\end{aligned}
$$



和上一步类似，


$$
\begin{aligned}
\pi^{*}_{T-1} &= \arg\max_{\pi_{T-1}} \mathbb{E}_{(s_{T-1}, a_{T-1}) \sim \rho_\pi} [Q^{*}_{T-1}(s_{T-1}, a_{T-1}) + \alpha_{T-1} \mathcal{H}(\pi_{T-1}) - \alpha_{T-1} \mathcal{H}_0 ] \\
\color{green}{\alpha^{*}_{T-1}} &\color{green}{=} \color{green}{\arg\min_{\alpha_{T-1} \geqslant 0} \mathbb{E}_{(s_{T-1}, a_{T-1}) \sim \rho_{\pi^{*}}} [ \alpha_{T-1} \mathcal{H}(\pi^{*}_{T-1}) - \alpha_{T-1}\mathcal{H}_0 ]}
\end{aligned}
$$



更新<span style="color: #32CD32;">绿色</span> $$\alpha_{T-1}$$ 的方程和更新上面<span style="color: #1E90FF;">蓝色</span> $$\alpha_{T}$$ 的方程形式相同。不断重复这一过程，通过最小化目标函数 $$J(\alpha)$$ 可以学到每一步的最佳温度参数：


$$
J(\alpha) = \mathbb{E}_{a_t \sim \pi_t} [-\alpha \log \pi_t(a_t \mid s_t) - \alpha \mathcal{H}_0]
$$


最终算法和 SAC 一样，只是现在是根据目标 $$J(\alpha)$$ 显式学习 $$\alpha$$ ：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_SAC2_algo.png)

*图 7  自动控温的柔性演员-评论家算法*

#### TD3

[论文](https://arxiv.org/abs/1802.09477)|[代码](https://github.com/sfujim/TD3)

众所周知 Q 学习算法有高估价值函数的“陋习”，过高的估计会在迭代训练过程中传播开来并对策略造成负面影响。这也直接推动了[双 Q 学习](https://papers.nips.cc/paper/3964-double-q-learning)和[双 DQN](https://arxiv.org/abs/1509.06461) 的诞生：用两个价值网络将行动选择和 Q 值更新两个部分解耦。

**孪生延迟深度决策（Twin Delayed Deep Deterministic，TD3）**在 DDPG 之上做了点改动来防止价值函数的高估：

（1）**精简双 Q 学习**：双 Q 学习中，行动选择和 Q 值估计分别由两个网络完成。DDPG 中，给定两个既定演员 $$(\mu_{\theta_1}, \mu_{\theta_2})$$ 和两个相应的评论家 $$(Q_{w_1}, Q_{w_2})$$，双 Q 学习的贝尔曼目标乍看起来是：


$$
\begin{aligned}
y_1 &= r + \gamma Q_{w_2}(s', \mu_{\theta_1}(s'))\\
y_2 &= r + \gamma Q_{w_1}(s', \mu_{\theta_2}(s'))
\end{aligned}
$$



但因为变化不能太快，这两个网络可能太像了以至于不能独立进行决策。相反，*精简双 Q 学习* 使用两估计中的最小值从而降低对偏差的估计，这种低估很难通过训练传播：


$$
\begin{aligned}
y_1 &= r + \gamma \min_{i=1,2}Q_{w_i}(s', \mu_{\theta_1}(s'))\\
y_2 &= r + \gamma \min_{i=1,2} Q_{w_i}(s', \mu_{\theta_2}(s'))
\end{aligned}
$$



（2）**延迟更新目标和策略网络**：演员-评论家模型中，策略和价值更新是深度绑定的：策略很差的时候，价值因为被高估了所以估计值会有差异，同样的，如果价值估计本身不准，策略也会变差。

为减少方差，TD3 更新策略的频率比 Q 函数要稍低些。直到几轮更新后价值误差已足够小，策略网络才会进行更新，否则保持不变。这和 [DQN](https://libertydream.github.io/2020/07/05/强化学习综述/#-深度-q-网络) 里将定期更新的目标网络作为目标的做法类似。

（3）**目标策略平滑化**：考虑到既定策略可能在价值函数的窄峰上会过拟合，TD3 给价值函数加了个平滑的正则化策略：给选定行动少量加点精简过的随机噪声，并在最小批上求平均


$$
\begin{aligned}
y &= r + \gamma Q_w (s', \mu_{\theta}(s') + \epsilon) & \\
\epsilon &\sim \text{clip}(\mathcal{N}(0, \sigma), -c, +c) & \scriptstyle{\text{ ; 精简的随机噪声}}
\end{aligned}
$$


这种方法借鉴了 [SARSA](https://libertydream.github.io/2020/07/05/强化学习综述/#-sarsa策略性-td-控制) 的更新思想，认为相似行动应该有相似价值。最终算法如下：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_TD3.png)

*图 8  TD3 算法*

#### SVPG

[论文](https://arxiv.org/abs/1704.02399)|[代码](https://github.com/dilinwang820/Stein-Variational-Gradient-Descent)

**斯坦因变分策略梯度（Stein Variational Policy Gradient，SVPG）** 使用[斯坦因](https://www.cs.dartmouth.edu/~qliu/stein.html)变分梯度下降（SVGD）算法更新策略参数 $$\theta$$。

最大熵策略优化中，$$\theta$$ 被看作是一个随机变量 $$\theta \sim q(\theta)$$，并且希望模型能学到分布 $$q(\theta)$$。假设我们能事先了解到 $$q$$ 的情况，比方说是 $$q_0$$，那我们希望能通过优化下列目标函数，指导学习过程别让 $$\theta$$ 偏离 $$q_0$$ 太远：


$$
\hat{J}(\theta) = \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha D_\text{KL}(q\|q_0)
$$


其中 $$\mathbb{E}_{\theta \sim q} [R(\theta)]$$ 是期望回报，$$\theta \sim q(\theta)$$ 而 $$D_\text{KL}$$ 是 KL 散度。

如果没有任何先验信息，可以令 $$q_0$$ 为均匀分布，$$q_0(\theta)$$ 为常量。然后上述目标函数就变成 [SAC](# SAC) 了，其中信息熵的那项会鼓励探索：


$$
\begin{aligned}
\hat{J}(\theta) 
&= \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha D_\text{KL}(q\|q_0) \\
&= \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha \mathbb{E}_{\theta \sim q} [\log q(\theta) - \log q_0(\theta)] \\
&= \mathbb{E}_{\theta \sim q} [J(\theta)] + \alpha H(q(\theta))
\end{aligned}
$$


取 $$\hat{J}(\theta) = \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha D_\text{KL}(q\|q_0)$$ 对 $$q$$ 的导数：


$$
\begin{aligned}
\nabla_q \hat{J}(\theta) 
&= \nabla_q \big( \mathbb{E}_{\theta \sim q} [J(\theta)] - \alpha D_\text{KL}(q\|q_0) \big) \\
&= \nabla_q \int_\theta \big( q(\theta) J(\theta) - \alpha q(\theta)\log q(\theta) + \alpha q(\theta) \log q_0(\theta) \big) \\
&= \int_\theta \big( J(\theta) - \alpha \log q(\theta) -\alpha + \alpha \log q_0(\theta) \big) \\
&= 0
\end{aligned}
$$



最优分布为：


$$
\log q^{*}(\theta) = \frac{1}{\alpha} J(\theta) + \log q_0(\theta) - 1 \text{ 所以 } \underbrace{ q^{*}(\theta) }_\textrm{“后验”} \propto \underbrace{\exp ( J(\theta) / \alpha )}_\textrm{“似然”} \underbrace{q_0(\theta)}_\textrm{先验}
$$


温度 $$\alpha$$ 负责平衡探索与利用的关系，当 $$\alpha \rightarrow 0$$， $$\theta$$ 只根据期望收益 $$J(\theta)$$ 更新，当 $$\alpha \rightarrow \infty$$，$$\theta$$ 会一直按先验假设走。

在用 SVGD 法估计目标后验分布 $$q(\theta)$$ 的时候，要依赖于一系列算子 $$\{\theta_i\}_{i=1}^n$$ （独立训练出来的策略代理）同时每个算子按如下方式更新：


$$
\theta_i \gets \theta_i + \epsilon \phi^{*}(\theta_i) \text{ 其中 } \phi^{*} = \max_{\phi \in \mathcal{H}} \{ - \nabla_\epsilon D_\text{KL} (q'_{[\theta + \epsilon \phi(\theta)]} \| q) \text{ s.t. } \|\phi\|_{\mathcal{H}} \leqslant 1\}
$$


其中 $$\epsilon$$ 是学习率，$$\phi^{*}$$ 是 $$\theta$$  形价值向量的一个 [RKHS](http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_1.pdf)（再生希尔伯特空间）$$\mathcal{H}$$ 的单位球面，会最大程度减少算子与目标分布间的 KL 散度值。$$q'(.)$$ 是 $$\theta + \epsilon \phi(\theta)$$ 的分布。

比较下不同的梯度更新方法：

| 方法                                                         | 更新空间                         |
| ------------------------------------------------------------ | -------------------------------- |
| 普通梯度                                                     | $$\Delta \theta$$ 在参数空间     |
| [自然梯度](https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html#natural-gradients) | $$\Delta \theta$$ 在检索分布空间 |
| SVGD                                                         | $$\Delta \theta$$ 在核函数空间   |

其中一种对 $$\phi^{*}$$ 的[估计](https://arxiv.org/abs/1608.04471)方法为借助正定核 $$k(\vartheta, \theta)$$，即高斯[径向基函数](https://en.wikipedia.org/wiki/Radial_basis_function)，测算两算子间的相似度。


$$
\begin{aligned}
\phi^{*}(\theta_i) 
&= \mathbb{E}_{\vartheta \sim q'} [\nabla_\vartheta \log q(\vartheta) k(\vartheta, \theta_i) + \nabla_\vartheta k(\vartheta, \theta_i)]\\
&= \frac{1}{n} \sum_{j=1}^n [\color{red}{\nabla_{\theta_j} \log q(\theta_j) k(\theta_j, \theta_i)} + \color{green}{\nabla_{\theta_j} k(\theta_j, \theta_i)}] & \scriptstyle{\text{；用当前算子值近似 }q'}
\end{aligned}
$$



-  <span style="color:#fc0303;">红色</span>的第一项会鼓励 $$\theta_i$$ 在 $$q$$ 的高概率区间学习，相似算子间共享该区域。=> 尽量与其他算子相似
-   <span style="color:#00c925;">绿色</span>的第二项会将算子彼此推离从而促进策略的多样化。=> 尽量和其他算子保持差异

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_SVPG.png)

一般会对温度参数 $$\alpha$$ 采用退火方法处理，训练初期多探索，后期多利用。

#### IMPALA

[论文](https://arxiv.org/abs/1802.01561)|[代码](https://github.com/deepmind/scalable_agent)

为了增强 RL 获得更强生产力，**IMPALA**（重要度加权演员-学习者框架，“Importance Weighted Actor-Learner Architecture”）在基础的演员-评论家配置之上将行动从学习中解耦，并通过 **V-trace** 非策略性矫正从所有经历轨迹中学习。

几个演员并行生成经历，而学习器会用这些经历优化策略和价值函数的参数。演员会定期从学习器那里拿最新策略参数更新自己。因为行动和学习解耦了，每个时间单元内我们可以加入更多演员生成更多轨迹。也是因为训练策略和行动策略不是完全同步的，所以两者间的那点差异需要非策略性的修正一下。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_IMPALA.png)

设价值函数 $$V_\theta$$ 参数为 $$\theta$$，策略 $$\pi_\phi$$ 参数为 $$\phi$$，回放缓存中的轨迹是由稍旧些的策略 $$\mu$$ 收集得来的。

在训练时刻 $$t$$ ，有 $$(s_t, a_t, s_{t+1}, r_t)$$，价值函数参数 $$\theta$$ 通过当前价值与 V-trace 目标价值间的 L2 损失进行学习。$$n$$ 步 V-trace 目标为：


$$
\begin{aligned}
v_t  &= V_\theta(s_t) + \sum_{i=t}^{t+n-1} \gamma^{i-t} \big(\prod_{j=t}^{i-1} c_j\big) \color{red}{\delta_i V} \\
&= V_\theta(s_t) + \sum_{i=t}^{t+n-1} \gamma^{i-t} \big(\prod_{j=t}^{i-1} c_j\big) \color{red}{\rho_i (r_i + \gamma V_\theta(s_{i+1}) - V_\theta(s_i))}
\end{aligned}
$$


<span style="color:#fc0303;">红色</span>部分 $$\delta_i V$$ 是 $$V$$ 的时差。 $$\rho_i = \min\big(\bar{\rho}, \frac{\pi(a_i \vert s_i)}{\mu(a_i \vert s_i)}\big)$$ 和 $$c_j = \min\big(\bar{c}, \frac{\pi(a_j \vert s_j)}{\mu(a_j \vert s_j)}\big)$$ 是*简化了的[重要度采样（IS）](# 非策略策略梯度)权重*。$$c_t, \dots, c_{i-1}$$ 的积表示 $$i$$ 时刻观察到的时差 $$\delta_i V$$ 会对之前 $$t$$ 时刻价值函数的更新有多大影响。策略性场景下，有 $$\rho_i=1$$ 和 $$c_j=1$$（假设 $$\bar{c} \geqslant 1$$），因此 V-trace 目标变成了策略性 n 步贝尔曼目标。

$$\bar{\rho}$$ 和 $$\bar{c}$$ 是两个修剪常量， $$\bar{\rho} \geqslant \bar{c}$$。$$\bar{\rho}$$ 影响价值函数的收敛定点， $$\bar{c}$$ 影响收敛速度。当 $$\bar{\rho} =\infty$$（不修剪），会收敛于目标策略 $$V^\pi$$ 的价值函数；当 $$\bar{\rho} \rightarrow 0$$，取行动策略 $$V^\mu$$ 的价值函数；介于两者之间时，取 $$\pi$$ 和 $$\mu$$ 之间的策略。

所以，价值函数的参数的更新方向为：


$$
\Delta\theta = (v_t - V_\theta(s_t))\nabla_\theta V_\theta(s_t)
$$


策略参数 $$\phi$$ 通过策略梯度更新：


$$
\begin{aligned}
\Delta \phi 
&= \rho_t \nabla_\phi \log \pi_\phi(a_t \vert s_t) \big(r_t + \gamma v_{t+1} - V_\theta(s_t)\big) + \nabla_\phi H(\pi_\phi)\\
&= \rho_t \nabla_\phi \log \pi_\phi(a_t \vert s_t) \big(r_t + \gamma v_{t+1} - V_\theta(s_t)\big) - \nabla_\phi \sum_a \pi_\phi(a\vert s_t)\log \pi_\phi(a\vert s_t)
\end{aligned}
$$


其中 $$r_t + \gamma v_{t+1}$$ 是 Q 值的估计，要从中减掉基准状态价值 $$V_\theta(s_t)$$。$$H(\pi_\phi)$$ 是为鼓励探索而加上的熵。

实验中，IMPALA 会为多个任务训练一个代理。涉及两个不同的模型框架，一个浅模型（左）和一个深度残差模型（右）。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-09-07_IMPALA-arch.png)

### 简短总结

看完上面的算法，我从中总结出了一些基本要素或原则：

- 努力降低方差并保证偏差不变使训练更稳定
- 非策略性探索效果更好，有利于更充分的利用数据样本
- 经历重放（训练数据从记忆回放缓存中采样）
- 目标网络要么定期冻结，要么比主动学习的策略网络慢点更新
- 批归一化
- 熵正则化奖励
- 评论家和演员共享网络低层参数，两个输出面向策略和价值函数
- 可以用既定策略学习而不是随便的一个
- 对策略更新间的差异加上约束
- 新优化方法（比如 K-FAC）
- 策略的熵最大化利于推动探索
- 别高估价值函数
