---
layout:     post
title:      自然语言处理：从入门到 RNN 与 LSTM
subtitle:   自然语言处理核心概念扫盲
date:       2019-09-07
author:     一轩明月
header-img: img/post-bg-universe.jpg
catalog: 	 true
tags:
    - NLP
excerpt:    本文从自然语言的概念剖析开始，介绍 NLP 领域的基本概念，基本操作并展示深度学习与神经网络是如何促进该领域迅速进化的。
---

> 文章编译自：
>
> https://towardsdatascience.com/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66

机器对人类语言理解能力的提升称得上是机器学习领域最令人振奋的进步之一。这一终极目标在机器学习界称为自然语言处理（Natural Language Processing，NLP）。

本文会介绍 NLP 领域的基本概念并展示深度学习与神经网络是如何促进该领域迅速进化的。在这之前，我们先明晰一些基础概念

## 什么是语言？

自然语言处理中的语言是指由某一类人群共享的词汇及言语规则的集合，用于表述思想和交流沟通，是信息的载体。

对这些词汇和规则的习得是人类成长的一部分，多数情况下每年只会有少量增补与变动。

人类会主动维护词典一类精密复杂的信息源，当某个人遇到生词、陌生概念时可以查阅。当他接触到这些词语后，词语本身会成为他语言库的一部分，能在未来适时调用。

## 计算机理解语言的方式

计算机只能遵循数学规则解决可计算问题。它并不能像人类那样轻易地理解与解释一些事情，但短时间内得到复杂运算结果却是小菜一碟。

> 计算机能够处理某个概念的前提是存在着一种方式能将这一概念用某种数学模型表示出来。

这无疑高度限制了计算机在自然语言处理上施展拳脚的范围与领域。目前计算机已在分类与翻译任务上取得了不俗成绩。

分类是将一段文字归入某个类别，翻译就是用另一种语言表述这段文字

## 什么是自然语言处理

> 自然语言处理，简称 NLP，广义上讲泛指一切用计算机处理、应用自然语言的活动。包括自然语言理解和自然语言生成两大类

自然语言处理已经有超过50年的历史，随着计算机兴起从语言学领域中延伸而来。

### 基本变换

正如前文所提到的，想要让机器理解自然语言就要将文本转换成某种数学结构。下面列举一些广泛使用的方法。

**分词（Tokenizaiton）**：将文本拆解成词。分词可以使用任何字符，但多数情况下是用空格作为分隔符

**词干提取（Stemming）**：简单粗暴地去除词尾以获得基本型（base word）的方法，通常要剔除派生词缀。派生词缀是指当一个词是由另一个词派生而来时所添加的词缀。派生词和词源通常属于不同词性。常用方法是波特算法（Porter's Algorithm）

**词形还原（Lemmatization）**：词义词形解析，通常只是为了去除屈折结尾（inflectional ending）。屈折结尾是改变词义时在尾部添加的一组字母，比如说 -s: bat - bats

> 因为词干构成遵循一系列规则，词干提取后得到的词根可能不是有效的词语，比如 woman 返回 wom。另一方面，词形还原减少了屈折变换后的词，一定程度上保证了词根仍是英文单词

**N-Grams 模型**  

N-grams 方法将邻近的几个词“攒”在一起作为运算的基本单元，N 表示被"攒"在一起的单词数量

举例来说，考虑这样一句话，"Natural Language Processing is essential to Computer Science"

**1-gram** 或者叫 **unigram** 模型对句子分词后得到单个单词的组合，输出 "Natural, Language, Processing, is, essential, to, Computer, Science"

**bigram** 模型则会将句子分成两个一组两个一组的样子，输出 "Natural Language, Language Processing, Processing is, is essential, essential to, to Computer, Computer Science"

类似的，trigram 模型会将句子分成 "Natural Language Processing, Language Processing is, Processing is essential, is essential to, essential to Computer, to Computer Science"，n-grams 模型的分词结果就是 n 个单词的组合序列。

> 将自然语句分解成 n-grams 形式对于维护句子里的单词计数十分重要，是使用传统数理统计方法进行自然语言处理的基石

### 变换方法

**TF-IDF**

TF-IDF 对单词打分以评估其在句子中的影响权重，分数由词频和反文档率构成。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-11_tf_idf.png)

**词频（Term Frequency，TF）**：某个词语在当前文档中出现的次数

**反文档率（Inverse Document Frequency，IDF）**：表征词语提供的信息量，即这个词语在各种文档中是常见还是少见。计算方式是 $$\log \frac{N}{d}$$ ，N 是文档数，d 是包含该词语的文档数

**独热编码**

独热编码（one-hot encodings）是数字化词语的另一方式。词向量长度等于词表大小，每个观测用一个矩阵表示。行数是词表大小，列数等于观测维度数，1 代表词表中的单词在本轮观测中出现了，反之用 0 表示

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-11_one-hot_words.png)

**词体嵌入**

词体嵌入（word embedding）是一系列语言模型与特征学习技术的统称，其将单词或短语映射成实数向量，主要用于神经网络模型

词体嵌入的思想是把一个词从维数等于词表大小的高维空间投射到低维空间，这样投影后两个相似的词间距会更小

便于理解，我们可以将嵌入视为对每一个词向下图一样的特征空间做投影

<img src="https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-11_word_embedding.png" style="zoom:67%;" />

每个词都被分解到性别、皇族、年龄和食物等一系列特征之上，这些特征构成了特征空间，是所有语义的集合。

可实践过程中这些维度并不是这样清晰易理解的，好在维度间关系是由算法计算求解，不会因此遇到什么麻烦。从训练和预测的角度来看，维度具体表示什么含义对于神经网络而言没有意义。

## 表征方法

### 词袋

算法能够从文本数据中抽取出有效关联信息的前提是数据能够清晰、结构化地表示出来。

词袋（Bag of words）方法使用表格来表征数据，其中每一列代表文集总词表中的一个词，每一行代表一次观测，每个单元格（行列交叉处）代表在一次特定观测下，某个单词出现的次数。

借助词袋方法，机器用可解释的矩阵范型理解句子，从而可以对数据使用各式线性代数运算和其他算法构建预测模型

下图是从某媒体期刊中构建的词袋模型

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-12_bag_word.png)

这种表征方式很有效，已经大量应用于常见机器学习任务之中，比如垃圾邮件检测，情感分类等。

但这种表征方法有两个很明显的缺陷：

1. **忽视了文本顺序**，也就失去了词语所处的上下文信息
2. **生成矩阵十分稀疏**，同时会偏袒计数大的单词。试想一下，该算法主要依靠词频工作，但是语言中单词重要性通常和文集中的出现频率呈反比关系，词频越高说明越常见，比如 is，an 这类词语，即使删去也不会给句子含义带来多大影响。所以选择恰当方式衡量单词在句子语义表达上起到多少作用十分重要

### 嵌入矩阵

嵌入矩阵（embedding matrix）是一种呈现词表中每个单词嵌入（embeddings）的方法。行代表单词嵌入空间的维度，列代表词表中的单词。

为了将样本转化为嵌入格式（embedding form），采用的方法是将每个单词的独热编码与嵌入矩阵相乘

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-12_embedding_matrix.png)

记住，这里的独热编码只是一个 n 维向量，n 是词表长度，且只有单词在词表中位置索引处的值为 1。这些独热编码来自词表而非一批观测。

### 循环神经网络

循环神经网络（Recurrent Netural Networks）简称 RNN，是非常重要的神经网络变体，广泛应用于自然语言处理之中。

概念上讲，RNN 区别于标准神经网络的地方在于 RNN 的输入是一个单词而非整个样本。这使得神经网络**能处理不同长度的语句**，其中有一些语句结构复杂标准神经网络无法处理。同时 RNN 可以**共享在不同位置学到的特征**，这一点标准神经网络同样无能为力。

RNN 单独看待句子中的每个单词，将其与 t - 1 时刻的激活值 $$a_{t - 1}$$  一起作为 t 时刻的输入。下图展示了 RNN 的细节

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/20190912114815.png)

上述结构也称为 $$T_{x} = T_{y}$$ 的多对多架构，输入个数和输出个数相等。这种结构在序列模型中很有用。

除此之外还有三种广泛使用的 RNN 架构：

- **多对一 RNN**：顾名思义，这种架构是多个输入得到一个输出。多见于分类任务

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/20190912115520.png)

上图中 H 代表激活函数的输出值

- **一对多 RNN**：这种架构是由一个输入得到一系列输出，可以用于音乐生成，这时输入是第一个音符

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-12_1_m_RNN.png)

- **$$T_{x} \neq T_{y}$$ 的多对多RNN**：输入个数大于输出个数的多对多架构，机器翻译任务常用结构

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-12_m_m_RNN.png)

**Encoder** 是指网络中读取待翻译文本的部分，**Decoder** 是指执行翻译的部分。

**RNN的局限**

1. 上述 RNN 架构顺着语句的一个方向学习依赖关系。基本上自然语言处理中会假设后面出现的词对前面的词的语义没有影响，而结合日常经验可知这显然不对。
2. RNN 在长文本依赖关系捕获上表现不佳，并且有梯度消失（vanishing gradient）的问题

这些缺陷催生了下文的新型 RNN 架构

### 门控循环单元

这种模型对基础循环单元进行改进以增强长文依赖捕获能力，同时改善了梯度消失的问题

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-12_5_grt.png)

门控循环单元（Gated Recurrent Unit，GRU）添加了被称为更新门或重置门的额外存储单元。除了带有 sigmoid 函数的神经单元与 softmax 函数的输出，GRU 还有一个带有 tanh 激活函数的附加单元。tanh 输出值可正可负所以可以用于控制伸缩方向，该单元的输出结合激活后的输入更新记忆单元的值。

正因如此，每次迭代隐层单元和记忆单元的值都会更新。记忆单元的值作为输入计算下一个单元的激活函数值

更详细的解释参见[文档](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)

### 长短文记忆

长短文记忆（Long Short-Term Memory，LSTM） 架构相比 GRU 模型除了更新门（update gate），还有一个遗忘门（forget gate）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-12_lstm_network.png)

这种架构让记忆单元可以选择保留 t-1 时刻的旧值，并将其加入 t 时刻值的计算

关于 LSTM 更多的解释参见[文档](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

### 双向 RNN

上文提到的 RNN 只考虑先前的时间戳，在 NLP 语境下就是只考虑当前单词之前的那些词语的影响。但显然语言结构并非如此，双向 RNN 因此诞生

双向 RNN（bidirectional RNN，BRNN）由前向和后向循环神经网络构成，最终预测值结合两个网络给出

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-12_BRNN.png)