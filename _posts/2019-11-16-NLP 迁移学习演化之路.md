---
layout:     post
title:      图解BERT
subtitle:   NLP 迁移学习的登神长阶
date:       2019-11-16
author:     一轩明月
header-img: img/post-bg-rwd.jpg
catalog: 	 true
tags:
    - NLP
    - Transformer
excerpt:    2018 年是自然语言处理领域发展道路上的转折点，这一切都是因为 BERT，一个从 Transformer 改造来的编码器结构。本文介绍了 BERT 内部架构，能完成的任务，同时介绍了迁移学习演化路径，每次的改进何在，关注点又是什么
---

> 编译自:
>
>   http://jalammar.github.io/illustrated-bert/ 

2018 年是自然语言处理（NLP）领域发展道路上的转折点。人们对如何高效表示词语和语句的理解飞速演化，所构建的表示对于潜在语义和微妙关系的”领悟“渐入佳境。此外，NLP 社区不断推陈出新，免费提供最新组件供人们下载，应用于各自的模型和管道上。（也被称为 NLP 的 ImageNet 时刻，类比于当年 ImageNet 提出后对计算机视觉领域带来的巨大提振效果）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_transformer-ber-ulmfit-elmo.png)

这一过程中， [BERT](https://github.com/google-research/bert)（  **B**idirectional **E**ncoder **R**epresentations from [**T**ransformers](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html), or **BERT** ） 的发布堪称里程碑式的进展，标志着 NLP 新时代的到来。其横扫多项语言处理任务记录达到当下最优水平。模型论文发表不久，其研发团队就开源了这一模型并提供可下载版本，所下载模型已经在大规模语料数据集上预训练好了。这无疑是 NLP 发展史上的高光时刻，从此人们搭建机器学习模型时，如果涉及到语言处理可以直接将该模型拿来用——省时省力，不费脑子不费钱。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-transfer-learning.png)

上图是 BERT 部署两步走。首先下载第一步中的预训练好的模型（基于无标注数据），然后第二步中微调即可使用

BERT 脱胎于一系列新近提出的概念之上——包括但不限于[半监督序列学习](https://arxiv.org/abs/1511.01432)，[ELMo](https://arxiv.org/abs/1802.05365),[ULMFiT](https://arxiv.org/abs/1801.06146),[OpenAI transformer](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) 以及迁移模型 [Transformer](https://arxiv.org/pdf/1706.03762.pdf)。

要理解 BERT 需要很多知识储备。所以了解这些概念前先看看 BERT 都能用来做什么吧。

### 语句分类

最直接的用法是用 BERT 对一条语句进行分类。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_BERT-classification-spam.png)

要得到这么一个模型，主要就是训练一个分类器，训练阶段对 BERT 做点小调整就好。这个训练阶段叫做**微调(Fine-Tuning)**，源于[半监督序列学习](https://arxiv.org/abs/1511.01432)和 ULM-FiT 。

对新人来讲，既然说到了分类器，我们就处于机器学习中的监督学习范畴内了，就是说要训练模型首先得有标记好了的数据集。以这个垃圾分类器为例，标记好的数据集就是邮件消息和对应标签构成的列表。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_spam-labeled-dataset.png)

BERT 还可以用于：

- 情感分析

输入电影/产品评论，输出评论是积极的还是消极的

- 事实检测

输入一个句子，判断是不是陈述句。更有野心的用法是给定一个陈述句，判断说的是真是假

### 模型架构

有了 BERT 用法的直观感受，来看看它是怎么工作的吧。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-base-bert-large.png)

原始论文给出了两种大小版本的 BERT：

- BERT BASE —— 和 Open AI Transformer 同大小，方便性能比对
- BERT LARGE —— 规模极其夸张的模型，取得了当下最有效果

BERT 本身就是一组训练好的转换编码器（Transformer Encoder），如果不清楚什么是迁移模型，请移步[图解 Transformer](https://libertydream.github.io/2019/11/02/图解-Transformer/) 。这是接下来要讨论的 BERT 的基础概念。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-base-bert-large-encoders.png)

两个模型的编码层都很大（论文里叫转换区， Transformer Blocks），基础版 12 层，完全版 24 层。前馈网络规模一样惊人，隐藏单元分别为 768 和 1024 个。相较于 Transformer 原始论文中的配置（6 层编码，512 隐藏单元，8 注意力端口），BERT 注意力端口数也更多，分别有12 个和 16个。

### 模型输入

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-input-output.png)

第一个输入是 [CLS] 标记，指明任务类型为分类（Classification）。和经典 transformer 编码器一样，BERT 接收一个词语序列并自下而上传递处理。每一层都用到了自注意机制，经前馈网络处理后传给下一层的编码器。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-encoders-input.png)

就架构而言，和 Transformer 相似之处就这么多（除去大小，这可以自己设置）。接着得先看看输出的差异了。

### 模型输出

每个位置都会输出一个等同于隐层大小的向量（基础版 BERT 是768）。对于上文语句分类任务，我们只看第一个位置（传递 [CLS] 标记的位置）。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-output-vector.png)

所得到的向量现在可以作为所选分类器的输入了。论文中仅使用单层神经网络的分类器就取得了不俗效果。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-classifier.png)

如果你有更多的标签（比如你所提供的邮件服务可以将邮件标记为”垃圾“，”非垃圾“，”社交“和”推广“），只需要微调网络，添加更多的输出神经元，然后喂给 softmax。

对于有计算机视觉背景的人来说，这种变量传递的场景和网络中 VGGNet 这样的卷积部分与末尾全连接分类部分间发生的情况是一致的。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_vgg-net-classifier.png)

### 嵌入的新时代

新进展带来了新单词编码方式。目前为止，词嵌入一直都是 NLP 任务中的主力，Word2Vec 和 GloVe 这样的方法被广泛使用。不妨先回顾一下这些内容然后看看现在有了哪些变化。

#### 词嵌入回顾

对于要被机器学习模型处理的单词来说，需要合适的方法将它们转换成数字化表示才能进行计算。Word2Vec 指出可以使用一个向量（数值列表）表示单词，同时某种程度上这种方法还能学到语义或者意义联结关系（比如，能指出词语间是相似还是相反，或者”北京“和”中国“与”华盛顿“和”美国“二者描述的含义是否相同），还有能学到语法关系（比如”had“和”has“的关系和”was“与”is“间是一致的）。

人们很快意识到相较于频繁地在一些小数据集上训练嵌入，不如事先在超大规模文本数据集上训练好来的高效。基于此，人们可以直接下载 Word2Vec 或 GloVe 预训练好的单词列表和其嵌入。下面是一个单词 ”sitck“ 的 GloVe 嵌入实例（向量大小为 200）。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_glove-embedding.png)

上面这种太大了且全是数字，我会用下面这种形状指代向量

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_vector-boxes.png)

### ELMo：环境要素

如果采用了 GloVe 的表示，那么单词 ”stick“ 无论在什么环境下都会表示成这个样子。等等，”stick“有很多意思，完全取决于其使用环境。为什么不结合其使用上下文创建嵌入呢？这样既能学到某一场景下的词义，还能学到其他的场景信息。因此，环境感知（contextualized) 词嵌入诞生了。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_elmo-embedding-robin-williams.png)

相较于每个词都是固定的嵌入表达，ELMo 选择在给单词分配嵌入前先看一下整个序列。具体来讲是采用针对特定任务训练好的双向 LSTM 模型构建嵌入。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_elmo-word-embedding.png)

ELMo 无疑在预训练方向上迈出了有益的一步。ELMo LSTM 会在大规模数据集上训练好，之后就可以作为一个组件随需要取用。ELMo 的奥秘在哪呢？

经过不断地”练习“猜测一个单词序列后的内容是什么，ELMo 取得了对语言的理解——这一任务被称为构建语言模型（Language Modeling)。因为我们有海量无标注数据可供模型学习，这还是比较方便的。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_Bert-language-modeling.png)

可以看到每个 ELMo 脑袋后面露出的各层隐态。预训练完毕后嵌入处理时还会派上用场。

ELMo 实际走得更远，它训练的是双向 LSTM——这样所得到的语言模型不仅对下一个词是什么有感觉，对前一个词同样有。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_elmo-forward-backward-language-model-embedding.png)

ELMo 通过某种方式（加权求和）将各隐态（和初始嵌入）组织在一起得到环境感知嵌入。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_elmo-embedding.png)

### ULM-FiT：将迁移学习引入 NLP

ULM-FiT 提出的方法能充分利用模型在预训练时学到的信息——不只是嵌入和情景感知嵌入。其同时给出了一个语言模型和一个过程，可以根据不同任务微调语言模型。

NLP 终于找到了像计算机视觉那样践行迁移学习的方法。

### The Transformer：超越 LSTMs

Transformer 论文及其代码的发布与其在诸如机器翻译任务上的表现，使得一些人开始考虑用 Transformer 替换掉 LSTM 类模型。主要是 Transformer 在长文依赖处理上比后者表现更好。

Transformer 的编码器-解码器结构使其在机器翻译领域傲视群雄。但你该怎么将其用于句子分类呢？你又要怎么将它用于预训练语言模型，可以根据任务不同而动态微调的那种。（业界将那些使用预训练模型或组件的监督学习任务称为下游任务（downstream task）。

### OpenAI Transformer: 一个解码器的事

事实证明我们并不需要整个 Transformer 来进行迁移学习和构建微调语言模型。我们可以只保留 transformer 的解码器。这似乎是一个自然而然的选择，因为其作用就是遮盖未来标识（futer tokens）——逐字翻译时的一个有价值的功能。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_openai-transformer-1.png)

模型构建了 12 层解码器。因为没配置编码器，这些解码器层下也就不会再有编码器-解码器注意力子层了，经典 transformer 是有的。但其依旧保有自注意层（被遮盖了防止看到未来标识，看着答案做题没意义）。

采用这种结构，我们来训练一个模型解决相同的任务：借助大规模无标注数据集预测下一个词。具体就是喂给 7000 本书，这类任务下书比推特或是文章更好，因为即使被很多文本分隔模型依旧可以学到相关关系。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_openai-transformer-language-modeling.png)

### 迁移学习到下游任务

 OpenAI Transformer 预训练好，根据任务调整下解码器，就可以将其用于下游任务了。下来看下语句分类（将邮件信息分为“垃圾”和“非垃圾”）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_openai-transformer-sentence-classification.png)

OpenAI 论文中给出了许多输入变换以应对不同类型任务的输入。下图展示了模型结构和不同任务下的输入变换。是不是很聪明？

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_openai-input transformations.png)

### BERT：从解码器到编码器

OpenAI transformer 给出了基于 Transformer 的可微调预训练模型。但从 LSTMs 到 Transformer 的转变中我们丢掉了些东西不是么。ELMo 模型是双向的，但 OpenAI transformer 只训练了一个前向语言模型（自左向右）。我们能不能构建一个 transformer 模型同时兼顾前向和后向呢？（行话讲叫取决于上下文）

BERT 大叫：“照顾好我七舅姥爷”。

### 遮罩语言模型

BERT：”我们用编码器“

“这太扯了”，Ernie 回道，“谁都知道多层情景下双向参考可能使单词间接看到自己”。

“我们用遮罩”。BERT 自信地说。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_BERT-language-modeling-masked-lm.png)

BERT 会遮住输入中的 15% 的词并让模型预测这些词是什么

找到合适的任务来训练 Transformer 编码器组是一个大难题，BERT 借鉴早先文献中的概念（完形填空）采用“遮盖语言模型”突破了这一障碍。

除了遮盖了 15% 的输入，为了改善后续微调表现，BERT 还加了点私货。有时它会随机将一个词替换成另一个，让模型预测这个位置正确的词是什么。

### 双语句任务

如果你回头看一眼 OpenAI transformer 针对不同任务设计的输入变换，你会注意到有些任务要求模型对两个句子说点聪明话（比如，二者是不是简单的互译？给定一个维基百科实体，再给一个对该实体的问题，能进行解答吗？）

为了提升 BERT 处理多语句关系的效果，预训练过程多了项额外任务：给定的两个句子（A和B），B 是否可能接在 A 的后面

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-next-sentence-prediction.png)

图中标识过分简化了，实际 BERT 使用词片（WordPieces）作为标识而不是词，这样有些词就会被分解成更小的块了。

### 专项任务模型

BERT 论文中给出了若干种将 BERT 用于不同任务的方法。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-tasks.png)

### 用 BERT 进行特征提取

微调并不是 BERT 的唯一打开方式。就像 ELMo 一样，你完全可以使用 BERT 创建环境感知词向量。然后将这些向量喂给你的模型——论文中给出了例证，在类似命名体识别这类的任务上，这么做效果并不会显著低于微调 BERT

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-contexualized-embeddings.png)

同为环境感知，哪个向量更好呢？我认为得视任务而定。论文中检测了 6 个选择（相较于微调模型，得分 96.4）：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-feature-extraction-contextualized-embeddings.png)