---
layout:		post
title:  	生成语言模型
subtitle:   摘要总结，看清趋势
date:       2020-12-21
author:     一轩明月
header-img: img/post-bg-code.jpg
catalog:    true
tags:
    - Transformer
    - attention
    - NLP
excerpt:    本文介绍了几种获取情境化词向量的模型，看一看在各类语言任务上取得 SOTA 效果的大型无监督预训练语言模型透露出什么新趋势，本文既是对之前 BERT，GPT-2等模型的摘要总结，又补充了前驱和后继的部分研究以看清潮流
---

> 编译自：Generalized Language Models， [Lilian Weng](https://lilianweng.github.io/lil-log/)

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_elmo-and-bert.png)

2018 年对 NLP 领域来说是个不折不扣的转折年。像 OpenAI GPT 和 BERT 这样的大规模预训练语言模型，用通用模型架构在各种语言任务上都取得了亮眼表现，这和  ImageNet 分类预训练对视觉任务的影响可以等量齐观$$^{*}$$。前者甚至更胜一筹，NLP 采用的方法简单而强大，预训练时不需要有标签数据，可以随着我们的性子增大训练规模直到极限。

> *注：[He 等人，2018](https://arxiv.org/abs/1811.08883) 发现预训练可能对图像分割任务来说不是那么必要

此前回顾总结的[词嵌入](https://libertydream.github.io/2020/02/13/embedding-%E5%9B%9E%E9%A1%BE/)文章中，各种 embedding 都和上下文没关系，他们都是基于共现关系而非有序语境来进行学习的。所以 “I am eating an apple” 和 “I have an Apple phone”  两句话中的 “apple” ，两个词指代的事物明显不同但还是会有相同的词嵌入向量。

此外，早期解决问题的时候，词嵌入都被用作既有任务模型的附加特征，某种程度上所带来的改进有限。

本文中我们会介绍许多基于情境做 embedding 的方法，以通用格式更简单、经济地将词向量应用到下游任务上。

### CoVe

**语境词向量（Contextual Word Vectors，Cove，[McCann 等人， 2017](https://arxiv.org/abs/1708.00107)）**是一类用编码器做词嵌入的方法，编码器就是基于[注意力](https://libertydream.github.io/2020/04/26/Attention-%E7%BB%BC%E8%BF%B0/)的 seq-to-seq 机器翻译模型里用的那种。不同于传统的词嵌入方法，CoVe 的词表示是关于整个输入序列的函数。

#### NMT 概述

这里的神经机翻译（[NMT](https://github.com/THUNLP-MT/MT-Reading-List)）模型由一个标准的双层双向 LSTM 编码器，和一个额外的基于注意力的双层单向 LSTM 解码器构成。模型预先在英语-德语翻译任务上进行训练。编码器学习并优化英文单词的 embedding 向量，好将其译为德文。直觉上编码器能在将词转为其他语言形式学到高维语义和句法特征，编码器输出的情景化词嵌入可供各种下游语言任务使用。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_nmt-recap.png)

*图 1  CoVe 中的 NMT 基础模型*

-  源语言（英语）的 $$n$$ 字序列：$$x = [x_1, \dots, x_n]$$
-  目标语言（德语）的 $$m$$ 字序列：$$y = [y_1, \dots, y_m]$$
- 源字句的 GloVe 向量：$$\text{GloVe}(x)$$
-  随机初始化目标字句的嵌入向量：$$z = [z_1, \dots, z_m]$$
- biLSTM 编码器输出隐态序列：$$h = [h_1, \dots, h_n] = \text{biLSTM}(\text{GloVe}(x))$$ 和 $$h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]$$。其中前向 LSTM 计算 $$\overrightarrow{h}_t = \text{LSTM}(x_t, \overrightarrow{h}_{t-1})$$，反向计算  $$\overleftarrow{h}_t = \text{LSTM}(x_t, \overleftarrow{h}_{t-1})$$
-  附加的解码器会输出一个字词分布：$$p(y_t \mid H, y_1, \dots, y_{t-1})$$ 其中 $$H$$ 是时间维度上的一组隐态 $$\{h\}$$：


$$
\begin{aligned}
\text{解码器隐态: } s_t &= \text{LSTM}([z_{t-1}; \tilde{h}_{t-1}], s_{t-1}) \\
\text{注意力权重: } \alpha_t &= \text{softmax}(H(W_1 s_t + b_1)) \\
\text{随情境而动的隐态: } \tilde{h}_t &= \tanh(W_2[H^\top\alpha_t;s_t] + b_2) \\
\text{解码器输出: } p(y_t\mid H, y_1, \dots, y_{t-1}) &= \text{softmax}(W_\text{out} \tilde{h}_t + b_\text{out})
\end{aligned}
$$



#### 下游任务使用 CoVe

NMT 编码器的隐态在其他语言任务中称为**环境向量（context vector）**：


$$
\text{CoVe}(x) = \text{biLSTM}(\text{GloVe}(x))
$$



论文将 GloVe 和 CoVe 串联用于问答和分类任务。GloVe 从全局字词共现比率中学习，所以没有句子语境，而 CoVe 是通过处理文本序列得到的，能够捕捉情境信息。



$$
v = [\text{GloVe}(x); \text{CoVe}(x)]
$$


对特定下游任务，我们先生成输入字词的 GloVe+CoVe 的串联向量，然后将其作为附加特征喂给特定任务模型。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_CoVe.png)

*图 2  CoVe embedding 是由编码器生成的，旨在解决机器翻译任务。编码器可以插入任意下游任务模型中（图片来源：[原论文](https://arxiv.org/abs/1708.00107)）*

**总结**：CoVe 的局限性很明显：（1）预训练受限于有监督翻译任务能得到哪些数据集；（2）CoVe 对最终效果的贡献受制于任务模型

下面我们会看到 ELMo 通过无监督预训练克服了问题（1），OpenAI GPT 和 BERT 进一步通过预训练 + 对不同下游任务采用生成模型架构将两个问题都解决了。

### ELMo

**语言模型嵌入（Embeddings from Language Model，ELMo，[Peters 等人 2018](https://arxiv.org/abs/1802.05365)）**通过 *无监督* 的方式预训练一个语言模型来获取情境化字词表示。

#### 双向语言模型

**双向语言模型（bidirectional Language Model，biLM)** 是 ELMo 的基础，当输入为 $$n$$ 个标识组成的序列时，$$(x_1,\dots,x_n)$$，语言模型会学着根据历史预测下个标识的概率。

前向传递期间，历史由目标标识之前的字词构成，


$$
p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_1, \dots, x_{i-1})
$$



反向传递时，历史则由目标标识之后的字词构成



$$
p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_{i+1}, \dots, x_n)
$$


两个方向的预测由多层 LSTM 负责建模，输入标识 $$x_i$$ 在 $$\ell=1,\dots,L$$ 层的隐态为 $$\overrightarrow{\mathbf{h}}_{i,\ell}$$ 和 $$\overleftarrow{\mathbf{h}}_{i,\ell}$$。经 softmax 归一化，用最后一层的隐态 $$\mathbf{h}_{i,L} = [\overrightarrow{\mathbf{h}}_{i,L}; \overleftarrow{\mathbf{h}}_{i,L}]$$ 获取标识概率。嵌入层和 softmax 层共享，参数分别为 $$\Theta_e$$，$$\Theta_s$$。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_ELMo-biLSTM.png)

*图 3  ELMo 的 biLSTM 基础模型*

模型要使两个方向的负对数似然概率最小化（= 正确词的对数似然概率最大化）：

$$
\begin{aligned}
\mathcal{L} = - \sum_{i=1}^n \Big( 
\log p(x_i \mid x_1, \dots, x_{i-1}; \Theta_e, \overrightarrow{\Theta}_\text{LSTM}, \Theta_s) + \\
\log p(x_i \mid x_{i+1}, \dots, x_n; \Theta_e, \overleftarrow{\Theta}_\text{LSTM}, \Theta_s) \Big)
\end{aligned}
$$

#### ELMo 表示

在 $$L$$ 层 biLM 基础上，ELMo 针对任务学习了一种线性组合方式，跨层将所有隐态堆叠起来。标识 $$x_i$$ 的隐态表示有 $$2L+1$$ 个向量：


$$
R_i = \{ \mathbf{h}_{i,\ell} \mid \ell = 0, \dots, L \}
$$


其中 $$\mathbf{h}_{0, \ell}$$ 是嵌入层输出，$$\mathbf{h}_{i, \ell} = [\overrightarrow{\mathbf{h}}_{i,\ell}; \overleftarrow{\mathbf{h}}_{i,\ell}]$$。每个终端任务都要学习一组线性组合权重 $$\mathbf{s}^\text{task}$$，并用 softmax 做归一化。比例因子 $$\gamma^\text{task}$$ 用于纠正 biLM 隐态分布和任务表示分布之间的偏差。


$$
v_i = f(R_i; \Theta^\text{task}) = \gamma^\text{task} \sum_{\ell=0}^L s^\text{task}_i \mathbf{h}_{i,\ell}
$$



为了评估从跨层隐态上得到了哪类信息，分别用不同 biLM 层的表示在语义和语法任务上对 ELMo 进行测试：

- **语义任务**：*词义消除（word sense disambiguation）*任务强调特定语境下的字词含义。biLM 顶层在该任务上比第一层表现更好。
- **语法任务**：*[词性标注](https://en.wikipedia.org/wiki/Part-of-speech_tagging)（part-of-speech tagging）*任务旨在推断某个词在一句话中的语法成分。此时用 biLM 第一层要比用顶层得到的准确率更高。

对比研究表明语法信息较低的层表示更好，而语义信息更高处的层领悟更深。因为不同层携带着不同类型的信息，*将其堆叠起来会有所帮助*。

#### 下游任务使用 ELMo

和 CoVe 助力不同下游任务的方式类似，ELMo 嵌入向量常见于输入或任务模型的底层。此外，对某些[任务](# 常见任务和数据集)（即 SNLI，SQuAD，不包括 SRL）在输出层加上他们同样有所助益。

对那些只有少量有监督数据的任务来说，ELMo 带来的提升是最大的。有了 ELMo，即使标签数据再少我们也能取得类似的效果。

**总结**：语言模型的预训练是无监督式的，而且鉴于无标注文本语料之丰富，理论上预训练规模可以尽可能的大些。但它还是得依赖特定任务模型，所以改善只是渐进式的，给每个任务找个好模型架构仍然很重要。

### 跨视角训练

ELMo 中无监督的预训练和指定任务的学习，是在两个独立训练阶段由两个独立模型完成的。**跨视角训练（Cross-View Training，CVT， [Clark 等人, 2018](https://arxiv.org/abs/1809.08370)）**将二者结合，组成了一个统一的半监督学习过程，辅助任务中有标注数据的监督学习和无标注数据的无监督学习都能改善 biLSTM 编码器的表示。

#### 模型架构

模型由一个双层双向的 LSTM 编码器和一个主预测模块构成。训练时交替将标注数据和无标注数据分批喂给模型。

- 对*有标注的样本*，所有模型参数都通过标准监督学习进行更新。损失是标准交叉熵。
- 对*无标注样本*，主预测模块依旧可以得到一个“柔性”目标，尽管我们并不清楚这有多准。两个辅助任务中，预测器只能看到并处理有限视角下的输入内容，比如只用单方向上的编码器隐态表示。我们希望辅助任务的输出能和初步预测目标匹配，这样就能得知输入全貌了。

这样，编码器被强制将完整的上下文知识提炼为不完整的表示。在这一阶段，biLSTM 编码器处于反向传播状态，但主预测模块是 *固定* 的。辅助预测和主预测间的差距就是我们要最小化的损失。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_CVT.png)

*图 4  半监督语言模型的跨视角训练*

#### 多任务学习

在同步训练多个任务的时候，CVT 给附加的任务加了几个主预测模型，它们共享同样的句子表示编码器。监督训练时，随机选择一个任务后，相应预测器参数和表示编码器得到更新。如果是无标注数据样本，联结所有任务优化编码器，力求每个任务上辅助输出和主预测间的差异最小。

多任务学习有利于增强表示的泛化能力，与此同时还收获一个很好的副产物：从无标注数据得到的全任务标注样本，他们是十分宝贵的数据标签，考虑到跨任务标签有用太稀少。

#### 下游任务使用 CVT

理论上讲主预测模块可以采用任意形式，通用的或者视任务而定的都行。CVT 论文里的例子两种情况都有。

在像 NER 或 POS tagging 这样的序列标注任务（给每个标识分类）中，预测器模块包括两个完整的全连接层，和给输出加上的 softmax 层，以此生成类别标签的概率分布。对每个标识 $$\mathbf{x}_i$$，两层对应的隐态为 $$\mathbf{h}_1^{(i)}$$ 和 $$\mathbf{h}_2^{(i)}$$：


$$
\begin{aligned}
p_\theta(y_i \mid \mathbf{x}_i) 
&= \text{NN}(\mathbf{h}^{(i)}) \\
&= \text{NN}([\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}]) \\
&= \text{softmax} \big( \mathbf{W}\cdot\text{ReLU}(\mathbf{W'}\cdot[\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}]) + \mathbf{b} \big)
\end{aligned}
$$



第一层里只给辅助任务喂前向或后向 LSTM 状态，因为它们只看到了部分上下文，要么左边要么右边，它们必须学着像语言模型那样，预测给定情境中的下一个标识。`fwd` 和 `bwd` 辅助任务只取一个方向。`future` 和 `past` 任务则分别在前向和后向上多走一步。



$$
\begin{aligned}
p_\theta^\text{fwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{fwd}(\overrightarrow{\mathbf{h}}^{(i)}) \\
p_\theta^\text{bwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{bwd}(\overleftarrow{\mathbf{h}}^{(i)}) \\
p_\theta^\text{future}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{future}(\overrightarrow{\mathbf{h}}^{(i-1)}) \\
p_\theta^\text{past}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{past}(\overleftarrow{\mathbf{h}}^{(i+1)})
\end{aligned}
$$


![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_CVT-example.png)

*图 5  序列标注任务依赖于四个辅助预测模型，它们的输入只有单方向的隐态信息：前，后，将来以及过去*

注意，如果主预测模块有 dropout，那处理有标注数据的时候 dropout 层照常工作，但用无标注数据训练，为辅助任务生成“柔性”目标时不参与计算。

机器翻译任务中，主预测模块换成了标准的带 attention 的标准单向 LSTM 解码器。涉及两个辅助任务：（1）dropout，随机使 attention 权重向量的一些值清零；（2）预测目标序列的未来词。用固定的主解码器对输入做[集束搜索](https://en.wikipedia.org/wiki/Beam_search)得到的最优预测目标序列就是辅助任务要努力逼近的主预测项了。

### ULMFiT

受 ImageNet 预训练在计算机视觉任务上攻城略地的启发，生成式预训练 LM + 任务微调的思路最先在 ULMFiT（[Howard 和 Ruder, 2018](https://arxiv.org/abs/1801.06146)）中进行了尝试。基准模型是 [AWD-LSTM](https://arxiv.org/abs/1708.02182)。

ULMFiT 通过下面三步在下游语言分类任务上取得了良好的迁移学习效果：

1）*通用 LM 预训练*：Wikipedia 语料

2）*目标任务 LM 微调*：ULMFiT 提出了两个训练技术来稳定微调过程。

-  **差异化微调（Discriminative fine-tuning）**是基于不同 LM 层学到的信息类型不同这一事实提出的。（上文有[讨论](# ELMo 表示)）。ULMFiT 以不同的学习率调教各层，$$\{\eta^1, \dots, \eta^\ell, \dots, \eta^L\}$$，$$\eta$$ 是第一层的基础学习率，$$\eta^\ell$$ 是第 $$\ell$$ 层，一共 $$L$$ 层。
-  **斜三角学习率（Slanted triangular learning rates，STLR）**是种特殊的学习率，先线性增加而后线性减小。增长期较短，便于模型快速收敛到和任务适配的参数空间，衰减期较长便于取得更好的微调效果。

3）*目标任务分类器微调*：用两个标准前馈层强化预训练 LM，并在末端加上 softmax 归一化来预测目标标签分布。

- **连接池化（Concat pooling）**对过往隐态取 max-pooling 和 mean-pooling，并将其和最终隐态拼在一起
- **逐步解封（Gradual unfreezing）**是为了避免灾难性遗忘，做法是从最后一层开始逐步解开模型各层。首先解开最后一层，微调一轮，然后是倒数第二层，不断重复直到调整完所有层。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_ULMFiT.png)

*图 6  ULMFiT 的三个训练阶段*

### OpenAI GPT

和 ELMo 想法类似，OpenAI 的**生成式预训练 Transformer（Generative Pre-training Transformer，GPT，[Radford 等人, 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)）**通过在巨量文本语料上训练，极大的增加了无监督语言模型的规模。抛开相似之处，GPT 和 ELMo 主要有两点不同。

1. 模型架构不同：ELMo 是将独立训练的自左向右和自右向左的多层 LSTM 进行浅拼接，而 GPT 是个多层 transformer 解码器
2. 情境化嵌入在下游任务中的用法不同：ELMo 是将 embedding 作为额外特征喂给特定任务模型，而 GPT 是将相同的基准模型进行微调来解决各式任务

#### 拿 Transformer 解码器当语言模型

相较于[原始 Transformer](https://arxiv.org/abs/1706.03762) 架构，[Transformer 解码器](https://arxiv.org/abs/1801.10198)模型丢掉了编码器部分，所以只有一个输入序列，而非两个单独的源序列和目标序列。

模型在输入序列的 embedding 上加了多层 transformer 块进行处理。每一块内都有一个遮罩*多头自注意力（multi-headed self-attention）*层和一个*元素级前馈（pointwise feed-forward）*层。经 softmax 归一化后最终可以得到一个目标标识的分布。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_OpenAI-GPT-transformer-decoder.png)

*图 7  OpenAI GPT 的 transformer 解码器模型架构*

损失为负对数似然概率，和 ELMo 一样，但没了反向计算。选定目标词前大小为 $$k$$ 的上下文窗口，损失为：


$$
\mathcal{L}_\text{LM} = -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1})
$$



#### 字节对编码

**字节对编码（Byte Pair Encoding，[BPE](https://arxiv.org/abs/1508.07909)）**用于给输入序列编码。BPE 原是 20 世纪 90 年代提出的一种数据压缩算法，随后被拿去解决机器翻译里的开放词汇表问题，因为在译为某种新语言的时候很容易遇到少见或未知的词。直觉上稀有或未知词经常可以拆成多个子词，BPE 就不断迭代，贪婪地合并常见字符对以寻找最佳分词方式。

#### 有监督微调

OpenAI GPT 所做的最大改进是与任务模型解耦，直接用预训练语言模型！

以分类任务为例。标注数据集中每个输入有 $$n$$ 个标识，$$\mathbf{x} = (x_1, \dots, x_n)$$，和一个标签 $$y$$。GPT 先用预训练过的 transformer 解码器处理输入序列 $$\mathbf{x}$$，最后一个标识 $$x_n$$ 在最后一层的输出为 $$\mathbf{h}_L^{(n)}$$。靠着仅有的训练得到的权重矩阵 $$\mathbf{W}_y$$，模型可以预测类别标签的分布。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_GPT-classification.png)


$$
P(y\mid x_1, \dots, x_n) = \text{softmax}(\mathbf{h}_L^{(n)}\mathbf{W}_y)
$$



损失是求实际标签的负对数似然概率的最小化，此外作者发现加上 LM 损失作为辅助损失会有好处，因为：

1. 训练时利于加速收敛
2. 改善监督模型的泛化效果


$$
\begin{aligned}
\mathcal{L}_\text{cls} &= \sum_{(\mathbf{x}, y) \in \mathcal{D}} \log P(y\mid x_1, \dots, x_n) = \sum_{(\mathbf{x}, y) \in \mathcal{D}} \log \text{softmax}(\mathbf{h}_L^{(n)}(\mathbf{x})\mathbf{W}_y) \\
\mathcal{L}_\text{LM} &= -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1}) \\
\mathcal{L} &= \mathcal{L}_\text{cls} + \lambda \mathcal{L}_\text{LM}
\end{aligned}
$$

有了类似设计后，就没必要给其他终端任务定制模型架构了（见图 8）。如果输入是多个序列，每对序列间会加上一个特殊的定界标识 `$`，定界标识的 embedding 是我们要学习的一个新参数，不过会很简短就是了。

对序列相似度任务来讲，因为排序没必要，先后序都有。多选任务里，情境会和每个候选项进行匹配结对。


![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_openai-input%20transformations.png)

*图 8  微调过的 GPT transformer 模型对各下游任务的训练目标*

**总结**：当时（2018.6）看到这样一个通用框架在大多数语言任务上取得 SOTA，让人感觉很巧妙且倍受鼓舞。一阶段，语言模型的生成式预训练过程可以从尽可能多的免费文本语料中进行学习。二阶段，用一个较小的标注数据集针对特定任务对模型进行微调，并学习一小批新参数。

GPT 的一个局限之处在于单向性——模型只会自左向右的预测上下文。

### BERT

**Transformer 双向编码器表示（Bidirectional Encoder Representations from Transformers，BERT，[Devlin 等人, 2019](https://arxiv.org/abs/1810.04805)）** 算是 GPT 的直系后代：在免费语料上训练一个大规模语言模型，然后针对特定任务微调而不需要单独定制网络架构。

相较于 GPT，BERT 最大的区别和改善之处在于双向训练，模型会学着预测左右两边的上下文，按论文中消融研究结果所述：

> “模型双向性是最重要的新贡献没有之一”

#### 预训练任务

BERT 的模型架构为多层双向 Transformer 编码器。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_transformer-encoder-2.png)

*图 9  Transformer 编码器架构*

为了促进双向预测和句子级理解，相较于在基础语言任务上训练（给定场景下预测下一个标识），BERT 要同时完成两项任务。

**任务 1：遮罩语言模型（Mask language model，MLM）**

> 维基百科：“完形填空（填充缺失测验）是一项练习，测试或评估，由一部分语句构成，其中特定项、字词或符号会被删除（填空文本），与试者的任务是替换缺失语言项……该练习首先由 W.L. Taylor 在 1953 年提出”

毫无疑问，从前后语境中而不单只是从后文中学到的表示能更好的领会个中深意，无论是在语义还是语法上。BERT 通过“*遮罩语言模型* ”任务来实现这一点：

1. 每个句子随机挡住 15% 的内容。因为如果我们只用特殊占位符 `[MASK]` 换掉被遮标识，微调的时候特定标识就再也看不到了。所以 BERT 用了几个启发式技巧：
   - a）80% 的概率用 `[MASK]` 替换选定词
   - b）10% 的概率用随机词替换
   - c）10% 的概率保持不变
2. 模型只预测缺失词，但它不知道哪个词被替换了，或者要预测哪个词。输出大小只有输入的 15%

**任务 2：下一句预测**

许多下游任务涉及到对句子间关系的理解（QA，NLI），BERT 额外加了一个辅助任务，训练一个*二分类器（binary classifier）*判断一句话是不是另一句的下文：

1. 对语句对（A,B）采样：
   - a）50% 的情况 B 是 A 的下文
   - b）50% 的情况不是
2. 模型对两句话进行处理并输出一个二值标签，指明 B 是否就是 A 后的下一句话

上述两个辅助任务的训练数据可以轻易从任意单语语料中获取，所以训练规模不受限制。训练损失是累计平均遮罩 LM 概率，和累计平均下文预测概率。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_language-model-comparison.png)

*图 10  BERT，OpenAI GPT 和 ELMo 模型架构对比*

#### 输入嵌入

输入嵌入是三部分的和：

1. *字段标识嵌入（WordPiece tokenization embeddings）*：字段模型原本是针对日语或德语的分词问题提出的。相较于使用自然分隔的英文单词，它们可以进一步分成更小的子词单元便于处理罕见词或未知词。感兴趣的话可以看看分词优化方式的论文[[1]](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)[[2]](https://arxiv.org/pdf/1609.08144.pdf)。
2. *片段嵌入（segment embedding）*：如果输入有两句话，分别有句子 A 和句子 B 的嵌入向量，并用特殊标识 `[SEP]` 隔开；如果输入只有一句话就只用句子 A 的嵌入向量
3. *位置嵌入（position embeddings）*：位置 embedding 需要学习而非硬编码

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_BERT-input-embedding.png)

*图 11  BERT 输入表示*

注意第一个标识必须是 `[CLS]`——之后下游任务预测中会用到的占位符。

#### 下游任务使用 BERT

BERT 的微调只需要添加很少的参数，就像 OpenAI GPT。

对于分类任务，取首个标识 `[CLS]` 的最终隐态 $$\mathbf{h}^\text{[CLS]}_L$$，将它和一个小权重矩阵相乘， $$\text{softmax}(\mathbf{h}^\text{[CLS]}_L \mathbf{W}_\text{cls})$$

对像 SQuAD 这样的 QA 任务，我们要针对问题预测给定段落的文本跨度。BERT 对每个标识要预测两个概率分布，分别对应描述作为文本范围的开端和末尾的几率。微调时新训练的只有两个小矩阵 $$\mathbf{W}_\text{s}$$ 和 $$\mathbf{W}_\text{e}$$，而 $$\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{s})$$ 和 $$\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{e})$$ 对应两个概率分布。

总体来讲微调下游任务时加上的内容很少——一两个权重矩阵，负责将 Transform 隐态转换成可解释形式。其他情况的实施细节可以看论文了解。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-tasks.png)

*图 12  微调过的 BERT 模型对不同下游任务的训练目标*

可以列张表比较下 OpenAI GPT 和 BERT 间的差别。

|          | **OpenAI GPT**                   | **BERT**                                                    |
| -------- | -------------------------------- | ----------------------------------------------------------- |
| 特殊字符 | 只在微调时引入`[SEP]` 和 `[CLS]` | 预训练阶段就学习 `[SEP]` 、 `[CLS]` 和句子 A/B 的 embedding |
| 训练过程 | 1M 步, batch size 32k 个词       | 1M 步, batch size 128k 个词                                 |
| 微调     | 所有任务 lr = 5e-5               | 看任务定 lr                                                 |

### ALBERT

**精简 BERT（A Lite BERT，ALBERT，[Lan 等人，2019](https://arxiv.org/abs/1909.11942)）**是 BERT 的简化版，相似的配置下参数减少 18 倍，训练速度提升 1.7 倍。ALBERT 所作的改进有三点：排前两位的当然是减少了参数，降低了内存开销从而提升了训练速度，而第三点则是用一个更有挑战性的训练任务替换掉了下一句预测（NSP）任务。

#### 分解式嵌入参数化

BERT 中，字段标识 embedding 的大小 $$E$$ 被设置成与隐态尺寸 $$H$$ 相同，即是说如果想增加模型尺寸（更大的 $$H$$），也就得学习更大的标识 embedding，而这又依赖于词表大小  $$V$$ ，计算成本就有点高了。

理论上，标识嵌入应当学习的是情境独立的表示，而隐态是依赖环境的，所以将隐层大小和词表嵌入的大小分开考虑比较合理。通过分解式嵌入参数化，$$V \times H$$ 的大词表嵌入矩阵就被分成 $$V \times E$$ 和 $$E \times H$$ 的两个小矩阵。一般 $$H \gt E$$ 或 $$H \gg E$$，因子分解可以显著降低参数数量。

#### 跨层参数共享

参数跨层共享有多种方式：（a）只共享前馈部分（b）只共享注意力参数（c）共享所有参数。该方法可以大量削减参数，同时又不会太伤害性能。

#### 句子顺序预测

有趣的是，BERT 的下一句预测（NSP）任务被证明太简单了。ALBERT 换成了句子顺序预测（sentence-order prediction，SOP）的自监督损失，

- 正样本：同文档中连续的两个部分
- 负样本：和上面一样但次序颠倒

对于 NSP 任务，当 A 和 B 的情境不同时，如果模型能检测到所谈话题它就能做出合理预测。相较而言，SOP 更难一些，因为这要求模型充分理解片段间的一致性和顺序关系。

### OpenAI GPT-2

[OpenAI](https://blog.openai.com/better-language-models/) [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 直接继承自 GPT。GPT-2 有 15 亿参数，比原 GPT 大了 10 多倍，在受测的 8 个语言模型数据集上拿了 7 个 SOTA，采用*零尝试迁移配置（zero-shot transfer setting）*不需要任何任务微调。预训练数据集包括 80 亿 Web 页面，页面是从 [Reddit](https://www.reddit.com/) 上爬下来的合格外链。在小数据集和评估*长程依赖（long-term dependency）*的数据集上 GPT-2 进步明显。

#### 零尝试迁移

GPT-2 的预训练就纯是语言建模。所有下游语言任务都被规制成对条件概率的预测，不存在对任务的微调。

- 文本生成就是直接用 LM
- 机器翻译任务，比如英语到汉语，通过 LM 条件化完成。末端加上“英文=中文”和“待翻英文=”两部分
  - 要预测的条件概率可能像这样：`P(?| I like green apples. = 我喜欢绿苹果。 A cat meows at him. = 一只猫对他喵喵叫。 It is raining cats and dogs. =)`
- QA 任务也转成是和翻译类似的形式，给上下文里加上成对的问答
- 摘要任务是在上下文中给文章末尾加上 `TL;DR:`

#### 字节序列 BPE

和原 GPT 一样，GPT-2 也对 UTF-8 字节序列采用了 [BPE](# 字节对编码)。每个字节可以用 8 比特表示 256 种不同的含义，UTF-8 最多可以使用 4 字节来表示一个字符，最高支持 $$2^{31}$$ 种字符。所以用字节序列表示，我们只需要一个大小为 256 的词汇表，而不需要操心预训练、标识化等内容。尽管有这些好处，当前字节级 LM 仍与 SOTA 字词级 LM 间有着不可忽视的性能差距。

BPE 不断贪婪地合并共现字节对，为防止常用词出现多个版本表示（由 `dog` 到`dog.`，`dog!`，`dog?`）GPT-2 不许 BPE 跨类别合并字符（`dog` 不会与 `.`，`!`，`?`这些标点合并）。这一技巧有效增加了最终字节段的质量。

通过字节序列表示，GPT-2 可以对任意 Unicode 字符串给出一个概率，而不需要任何预训练步骤。

#### 模型改进

相较于 GPT，除了更多的 transformer 层和参数，GPT-2 只做了很少的架构调整：

- [层归一化](https://arxiv.org/abs/1607.06450)移到子块输入上，像是“[建块](https://arxiv.org/abs/1603.05027)”型残差单元一样（和原来的“[瓶颈](https://arxiv.org/abs/1512.03385)”类型不同，它是在权重层前进行批归一化）
- 在最后的自注意块之后加了个层归一化
- 改良初始化，使其成为模型深度的一个函数
- 残差层的权重一开始要缩小至 $$\frac {1}{\sqrt{N}}$$，$$N$$ 是残差层数量
- 用更大的词汇表和上下文

### RoBERTa

**稳健优化版 BERT**（ **R**obustly **o**ptimized **BERT** **a**pproach； [Liu 等人，2019](https://arxiv.org/abs/1907.11692)）算是训练 BERT 实现更好效果的新凭据，因为作者发现原 BERT 模型明显训练不足。台面上摆的是：

1. 用更大的 batch size 进行更多步的训练
2. 删掉下一句预测任务
3. 训练数据格式上用更长的序列。论文指出用单独的句子作输入会伤害下游性能，应该连续采样多句构建更长的片段
4. 动态调整遮罩模式。原 BERT 就在预训练时用了一次遮罩，导致训练时都是一个静态罩子。RoBERTa 在 40 轮训练中用了 10 种不同的遮罩方式。

RoBERTa 还加了新数据集 [CommonCrawl News](https://commoncrawl.org/2016/10/news-dataset-available/) 并进一步证明用*更多的数据* 进行预训练有助于改善下游任务性能。训练方式和 GPT-2 一样，都是在字节序列上做 BPE。研究者还发现超参数的选择对模型性能有重大影响。

### T5

**文本到文本迁移 Transformer（Text-to-Text Transfer Transformer，T5，[Colin 等人, 2020](https://arxiv.org/abs/1910.10683)）** 是按[原 Transformer](https://arxiv.org/abs/1706.03762) 架构实现的编码器-解码器语言模型：标识→ embedding →编码器 →解码器 →输出。T5 采用“全能自然语言”框架（[McCann 等人, 2018](https://arxiv.org/abs/1806.08730)），许多常见 NLP 任务被转换成针对上下文的问答形式。相较于显式的 QA 格式，T5 用较短的任务前缀区分任务意图，并分别对每个任务做了模型微调。文本到文本架构用同样的模型解决不同的任务，极大简化了迁移学习的评估过程。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-15_T5.png)

*图 13  T5 任务评估图。文本到文本架构将各任务转换成通用格式：输入文本进目标文本出*

模型在 2019 年 4 月搜集的网络语料上做训练，用上了各种过滤器。模型借助“适配器层”（多加一层做训练）或“逐步解封”（见 [ULMFiT](# ULMFiT)）分别对各下游任务做微调。两个微调方法都是只更新部分参数，同时大部分模型参数不变。T5-11B 在许多 NLP 任务上取得了 SOTA 结果。

正如作者在文中所讲“......我们的目标不是提出新方法，而是提供一个全面的视角看看这个领域发展到了哪一步”，T5 的长论文详细介绍了许多训练设置和评估过程的细节，对有兴趣从头训练 LM 的读者来说是很好的阅读材料。

### 总结

|         |         基础模型          | 预训练 | 下游任务 |       下游模型        |         微调         |
| :-----: | :-----------------------: | :----: | :------: | :-------------------: | :------------------: |
|  CoVe   |     seq2seq NMT 模型      | 有监督 | 基于特征 |       看任务定        |          /           |
|  ELMo   |        双层 biLSTM        | 无监督 | 基于特征 |       看任务定        |          /           |
|   CVT   |        双层 biLSTM        | 半监督 | 基于模型 | 看任务定 / 与任务无关 |          /           |
| ULMFiT  |         AWD-LSTM          | 无监督 | 基于模型 |      与任务无关       | 所有层；各种训练技巧 |
|   GPT   |    Transformer 解码器     | 无监督 | 基于模型 |      与任务无关       | 预训练层+顶部任务层  |
|  BERT   |    Transformer 编码器     | 无监督 | 基于模型 |      与任务无关       | 预训练层+顶部任务层  |
|  GPT-2  |    Transformer 解码器     | 无监督 | 基于模型 |      与任务无关       | 预训练层+顶部任务层  |
| RoBERTa |    Transformer 编码器     | 无监督 | 基于模型 |      与任务无关       | 预训练层+顶部任务层  |
|   T5    | Transformer 编码器-解码器 | 无监督 | 基于模型 |      与任务无关       |   所有层；训练技巧   |

### 指标：复杂度

复杂度通常作为一个固有评估指标，衡量给定情境下语言模型对实际分布的学习效果。

离散概率分布 $$p$$ 的[复杂度](https://en.wikipedia.org/wiki/Perplexity)被定义为熵的乘方：



$$
2^{H(p)} = 2^{-\sum_x p(x) \log_2 p(x)}
$$



给定 $$N$$ 个词构成的序列，$$s = (w_1, \dots, w_N)$$，简单假设每个词的频率相同，都是 $$\frac{1}{N}$$，则熵形式如下：


$$
H(s) = -\sum_{i=1}^N P(w_i) \log_2  p(w_i)  = -\sum_{i=1}^N \frac{1}{N} \log_2  p(w_i)
$$



于是句子复杂度就变为：


$$
\begin{aligned}
2^{H(s)} &= 2^{-\frac{1}{N} \sum_{i=1}^N \log_2  p(w_i)}
= (2^{\sum_{i=1}^N \log_2  p(w_i)})^{-\frac{1}{N}}
= (p(w_1) \dots p(w_N))^{-\frac{1}{N}}
\end{aligned}
$$



语言模型够好，做预测给的概率就高，因此复杂度越低越好。

### 常见任务和数据集

**问答**

- [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) (Stanford Question Answering Dataset)：阅读理解数据集，由一组基于 维基百科文章提出的问题组成，每个问题的答案对应一段文本
- [RACE](http://www.qizhexie.com/data/RACE_leaderboard) (ReAding Comprehension from Examinations)：超大规模阅读理解数据集，超 2.8 万篇文章和近 10 万个问题。数据集源自中国中、高中英语考试测验。

**常识推理**

- [Story Cloze Test](http://cs.rochester.edu/nlp/rocstories/)：一个常识推理框架，考察故事理解和生成能力。要求系统在两个选项中，给由多个句子组成的故事选择正确结局。
- [SWAG](https://rowanzellers.com/swag/) (Situations With Adversarial Generations)：多项选择；包含 11.3 万个结对句子样本，用以评估基本常识推理

**自然语言推理（Natural Language Inference，NLI）**：也叫**文本推演（Text Entailment）**，辨析一个句子可否由另一个句子推理出来

- [RTE](https://aclweb.org/aclwiki/Textual_Entailment_Resource_Pool) (Recognizing Textual Entailment)： 由文本推演挑战创立的数据集
- [SNLI](https://nlp.stanford.edu/projects/snli/) (Stanford Natural Language Inference)：57 万个手写英文句子对，人工打标（`entailment` ，`contradiction` 和 `neutral`）保证分类均衡
- [MNLI](https://www.nyu.edu/projects/bowman/multinli/) (Multi-Genre NLI)：类似 SNLI，文本风格和话题种类更多，收集自转录语音，流行小说和政府报告
- [QNLI](https://gluebenchmark.com/tasks) (Question NLI)：转换自 SQuAD 数据集，变成基于（问题，句子）对的二分类任务
- [SciTail](http://data.allenai.org/scitail/)：从多选型科学测验和网络语句中收集到的推理数据集

**命名实体识别（Named Entity Recognition，NER）**：给文本中字词序列打标，标签是事物名，比如人名，公司名，基因或蛋白质名等等

- [CoNLL 2003 NER task](https://www.clips.uantwerpen.be/conll2003/)：材料来自路透社新闻，包括四类命名实体：人，地点，机构组织和杂项实体名
- [OntoNotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)：该语料涵盖英语，阿拉伯语和汉语，标注四个不同的实体类别（PER，LOC，ORG，MISC）
- [Reuters Corpus](https://trec.nist.gov/data/reuters/reuters.html)：路透社新闻报道集
- Fine-Grained NER (FGN)

**情感分析**

- [SST](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank)
- [IMDb](http://ai.stanford.edu/~amaas/data/sentiment/)：大规模影评数据集，打上了情感二分类标签

**语义成分标注（Semantic Role Labeling，SRL）**：对句子的谓词参数结构建模，通常是回答“谁对谁做了什么”

- [CoNLL-2004 & CoNLL-2005](http://www.lsi.upc.edu/~srlconll/)

**句子相似度**：也叫*复述测验*

- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (MicRosoft Paraphrase Corpus)：都是从网络新闻源中抽取的句子对，没对句子都加了注释以表明是否语义等价
- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs)
- STS Benchmark: Semantic Textual Similarity

**句子接受度**：一项指明句子语法合格率的任务

- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability)：二值单句分类任务

**文本分块**：把文章分成句法特征关系紧密的几部分

- [CoNLL-2000](https://www.clips.uantwerpen.be/conll2000/chunking/)

**词性标注（Part-of-Speech [POS] Tagging）**：给每个标识打上词性成分，比如名词，动词，形容词等。

**机器翻译**：见[斯坦福 NLP](https://nlp.stanford.edu/projects/nmt/) 页面

- WMT 2015 English-Czech data (Large)
- WMT 2014 English-German data (Medium)
- IWSLT 2015 English-Vietnamese data (Small)

**共指消解**：对指代相同潜在实体的部分聚类

- [CoNLL-2012](http://conll.cemantix.org/2012/data.html)

**远程依赖**

- [LAMBADA](http://clic.cimec.unitn.it/lambada/) (LAnguage Modeling Broadened to Account for Discourse Aspects)：从 BookCorpus 抽来的记叙文集，任务是要预测最后一个词。人类要想成功预测需要至少 50 个标识的上下文。
- [Children’s Book Test](https://research.fb.com/downloads/babi/)：语料是来自 [Project Gutenberg](https://www.gutenberg.org/) 的免费图书。任务是要在 10 个候选项中预测缺失词

**多任务基准**

- [GLUE 多任务基准](https://gluebenchmark.com/)
- [decaNLP 基准](https://decanlp.com/)

**无监督预训练数据集**

- [Books corpus](https://googlebooks.byu.edu/)：超 7000 本不同的未出版图书，类型覆盖冒险，幻想，浪漫等
- [1B Word Language Model Benchmark](http://www.statmt.org/lm-benchmark/)
- [英文维基百科](https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia): 大约 25 亿个词

