---
layout:		post
title:  	生成语言模型
subtitle:   深化总结
date:       2020-12-21
author:     一轩明月
header-img: img/post-bg-code.jpg
catalog:    true
tags:
    - Transformer
    - attention
    - NLP
excerpt:    本文介绍了几种学习情景化词向量的模型，算是对之前 BERT，GPT-2等模型的总结，补充了早先和后继研究的部分细节，探讨了这些在各种语言任务上取得令人惊讶的 SOTA 成果的大型无监督预训练模型身上，透露出了什么新趋势
---

> 编译自：Generalized Language Models， [Lilian Weng](https://lilianweng.github.io/lil-log/)

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_elmo-and-bert.png)

2018 年对 NLP 领域来说是个不折不扣的转折年。像 OpenAI GPT 和 BERT 这样的大规模预训练语言模型采用通用模型架构，在各种语言任务上都取得了亮眼表现，和  ImageNet 分类预训练对视觉任务所带来的冲击可以等量齐观$$^{*}$$。前者甚至更胜一筹，NLP 采用的方法简单而强大，预训练时不需要有标签数据，训练规模的增大也不妨碍实验，直至达到我们的极限。

> *注：[He 等人，2018](https://arxiv.org/abs/1811.08883) 发现预训练可能对图像分割任务来说不是那么必要

在此之前的[词嵌入](https://libertydream.github.io/2020/02/13/embedding-%E5%9B%9E%E9%A1%BE/)，各种 embedding 都是针对特定上下文的——基于共现关系而非顺序语境学习。所以对 “I am eating an apple” 和 “I have an Apple phone”  两句话中的 “apple” 来讲，两个词指代的事物明显不同但还是会有相同的词嵌入向量。

尽管如此，早期解决问题的时候，用词嵌入也是将其作为已有的特定任务模型的附加特征，某种程度上所带来的改进有限。

本文中我们会介绍许多种基于环境做 embedding 的方法，好更简单、更容易的将词向量应用到下游任务上，格式通用。

### CoVe

**语境词向量（Contextual Word Vectors，Cove，[McCann 等人， 2017](https://arxiv.org/abs/1708.00107)）**是一类借助编码器做词嵌入的方法，编码器就是基于[注意力](https://libertydream.github.io/2020/04/26/Attention-%E7%BB%BC%E8%BF%B0/)的 seq-to-seq 机器翻译模型里的那种编码器。不同于传统的词嵌入方法，CoVe 的词表示是关于整个输入序列的函数。

#### NMT 概述

这里的神经机翻译（[NMT](https://github.com/THUNLP-MT/MT-Reading-List)）模型由一个标准的双层双向 LSTM 编码器，和一个额外的基于注意力的双层单向 LSTM 解码器构成，预先在英语-德语翻译任务上训练好。编码器学习并优化英文单词的 embedding 向量，好将其译为德文。直觉上编码器能在将词转为其他语言形式前领悟语义和句法内涵，编码器的输出结果被用做各种下游语言任务的情景化词嵌入。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_nmt-recap.png)

图 1  CoVe 中的 NMT 基础模型

-  源语言的 $$n$$ 字序列：$$x = [x_1, \dots, x_n]$$
-  目标语言的 $$m$$ 字序列：$$y = [y_1, \dots, y_m]$$
- 源字句的 GloVe 向量：$$\text{GloVe}(x)$$
-  随机初始化目标字句的嵌入向量：$$z = [z_1, \dots, z_m]$$
- biLSTM 编码器输出隐态序列：$$h = [h_1, \dots, h_n] = \text{biLSTM}(\text{GloVe}(x))$$ 和 $$h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]$$。前向 LSTM 计算 $$\overrightarrow{h}_t = \text{LSTM}(x_t, \overrightarrow{h}_{t-1})$$，反向计算  $$\overleftarrow{h}_t = \text{LSTM}(x_t, \overleftarrow{h}_{t-1})$$
-  附加的解码器输出一个字词分布：$$p(y_t \mid H, y_1, \dots, y_{t-1})$$ 其中 $$H$$ 是时间维度上的一组隐态 $$\{h\}$$：


$$
\begin{aligned}
\text{解码器隐态: } s_t &= \text{LSTM}([z_{t-1}; \tilde{h}_{t-1}], s_{t-1}) \\
\text{注意力权重: } \alpha_t &= \text{softmax}(H(W_1 s_t + b_1)) \\
\text{随情境而动的隐态: } \tilde{h}_t &= \tanh(W_2[H^\top\alpha_t;s_t] + b_2) \\
\text{解码器输出: } p(y_t\mid H, y_1, \dots, y_{t-1}) &= \text{softmax}(W_\text{out} \tilde{h}_t + b_\text{out})
\end{aligned}
$$



#### 下游任务使用 CoVe

NMT 编码器的隐态在其他语言任务中称为**环境向量（context vector）**：


$$
\text{CoVe}(x) = \text{biLSTM}(\text{GloVe}(x))
$$



论文将 GloVe 和 CoVe 串联用于问答和分类任务。GloVe 从全局字词共现率中学习，所以没有句子语境，而 CoVe 是通过处理文本序列得到的，能够捕捉情境信息。



$$
v = [\text{GloVe}(x); \text{CoVe}(x)]
$$


对给定的下游任务，我们先生成输入字词的 GloVe+CoVe 的串联向量，然后将其作为附加特征喂给相关任务模型。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_CoVe.png)

*图 2  CoVe embedding 是由用于机器翻译任务的编码器生成的。编码器可以插入任意下游任务模型中（图片来源：[原论文](https://arxiv.org/abs/1708.00107)）*

**总结**：CoVe 的局限性很明显：（1）预训练限于有监督翻译任务能得到的数据集；（2）CoVe 对最终效果的贡献受制于任务模型架构

下面我们会看到 ELMo 通过无监督预训练克服了问题（1），OpenAI GPT 和 BERT 进一步通过预训练 + 对不同下游任务采用生成模型架构将两个问题都解决了。

### ELMo

**语言模型嵌入（Embeddings from Language Model，ELMo，[Peters 等人 2018](https://arxiv.org/abs/1802.05365)）**通过以 *无监督* 方式预训练一个语言模型来获取情境化字词表示。

#### 双向语言模型

**双向语言模型（bidirectional Language Model，biLM)** 是 ELMo 的基础，当输入为 $$n$$ 个标识组成的序列时，$$(x_1,\dots,x_n)$$，语言模型会学着基于历史预测下一个标识的概率。

前向传递期间，历史由目标标识之前的字词构成，


$$
p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_1, \dots, x_{i-1})
$$



反向传递时，历史则由目标标识之后的字词构成



$$
p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_{i+1}, \dots, x_n)
$$


两个方向的预测由多层 LSTM 负责建模，$$\ell=1,\dots,L$$ 层的输入标识 $$x_i$$ 的隐态为 $$\overrightarrow{\mathbf{h}}_{i,\ell}$$ 和 $$\overleftarrow{\mathbf{h}}_{i,\ell}$$。经 softmax 归一化，用最后一层的隐态 $$\mathbf{h}_{i,L} = [\overrightarrow{\mathbf{h}}_{i,L}; \overleftarrow{\mathbf{h}}_{i,L}]$$ 获取标识概率。他们共享嵌入层和 softmax 层，参数分别为 $$\Theta_e$$，$$\Theta_s$$。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_ELMo-biLSTM.png)

*图 3  ELMo 的 biLSTM 基础模型*

模型要使两个方向的负对数概率最小化（= 最大化正确词的对数概率）：

$$
\begin{aligned}
\mathcal{L} = - \sum_{i=1}^n \Big( 
\log p(x_i \mid x_1, \dots, x_{i-1}; \Theta_e, \overrightarrow{\Theta}_\text{LSTM}, \Theta_s) + \\
\log p(x_i \mid x_{i+1}, \dots, x_n; \Theta_e, \overleftarrow{\Theta}_\text{LSTM}, \Theta_s) \Big)
\end{aligned}
$$

#### ELMo 表示

在 $$L$$ 层 biLM 基础上，ELMo 针对任务学习了一种线性组合方式，跨层将所有隐态堆在了一起。标识 $$x_i$$ 的隐态表示有 $$2L+1$$ 个向量：


$$
R_i = \{ \mathbf{h}_{i,\ell} \mid \ell = 0, \dots, L \}
$$


其中 $$\mathbf{h}_{0, \ell}$$ 是嵌入层输出，$$\mathbf{h}_{i, \ell} = [\overrightarrow{\mathbf{h}}_{i,\ell}; \overleftarrow{\mathbf{h}}_{i,\ell}]$$。每个末端任务都要学习一组线性组合权重 $$\mathbf{s}^\text{task}$$，要用 softmax 做归一化。比例因子 $$\gamma^\text{task}$$ 用于纠正 biLM 隐态分布和任务表示分布之间的偏差。


$$
v_i = f(R_i; \Theta^\text{task}) = \gamma^\text{task} \sum_{\ell=0}^L s^\text{task}_i \mathbf{h}_{i,\ell}
$$



为了评估从跨层隐态上得到了哪类信息，分别在语义和语法任务上使用 ELMo，期间要用不同 biLM 层的表示：

- **语义任务**：*词义消除（word sense disambiguation）*任务是要突出特定语境下词和含义。biLM 顶层比首层在该任务上表现更好。
- **语法任务**：*[词性标注](https://en.wikipedia.org/wiki/Part-of-speech_tagging)（part-of-speech tagging）*任务是要推断一个词在一句话中的语法成分。用 biLM 首层要比用顶层得到的准确率更高。

对比研究表明语法信息较低的层表示更好，而语义信息更高处的层领悟更深。因为不同层适于携带不同类型的信息，*将其堆叠起来会有所帮助*。

#### 下游任务使用 ELMo

和 CoVe 助力不同下游任务的方式类似，ELMo 嵌入向量也是用作输入或任务模型的底层。此外，对某些[任务](# 常见任务和数据集)（即 SNLI，SQuAD，但没 SRL）在输出层加上他们同样有所助益。

对那些只有少量监督数据的任务来说，ELMo 带来的提升是最大的。有了 ELMo，即使标签数据再少我们也能取得类似的效果。

总结：语言模型的预训练是无监督式的，而且鉴于无标注文本语料之丰富，理论上预训练规模可以极尽所能的增大。但它还是得依赖特定任务模型，所以改善只是渐进式的，给每个任务找个好模型架构仍然很重要。

### 跨视角训练

ELMo 中无监督的预训练和指定任务的学习，是在两个独立训练阶段由两个独立模型完成的。**跨视角训练（Cross-View Training，CVT， [Clark 等人, 2018](https://arxiv.org/abs/1809.08370)）**将二者结合，组成了一个统一的半监督学习过程，辅助任务上有标注数据的监督学习和无标注数据的无监督学习都能改善 biLSTM 编码器的表示。

#### 模型架构

模型由一个双层双向的 LSTM 编码器和一个主预测模块构成。训练时交替将标注数据和无标注数据喂给模型。

- 对*有标注的样本*，所有模型参数都通过标准监督学习进行更新。损失是标准交叉熵。
- 对*无标注样本*，主预测模块依旧可以得到一个“柔性”目标，尽管我们无法清楚地知道他们有多准。两个辅助任务中，预测器只能看到并处理有限视角下的输入内容，比如只用单方向上的编码器隐态表示。我们希望辅助任务的输出能和初步预测目标匹配，这样就能看到输入全貌了。

这样，编码器被强制将完整的上下文知识提炼为不完整的表示。在这一阶段，biLSTM 编码器是用反向传播的，但主预测模块是 *固定* 的。辅助预测和主预测间的差距就是我们要最小化的损失。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_CVT.png)

*图 4  半监督语言模型的跨视角训练*

#### 多任务学习

在同步训练多个任务的时候，CVT 给附加的任务加了几个主预测模型，它们共享一个句子表示编码器。监督训练时，随机选择一个任务后，相应预测器参数和表示编码器得到更新。如果是无标注数据样本，联结所有任务优化编码器，力求每个任务上辅助输出和预测间的差距最小化。

多任务学习有利于增强表示的泛化能力，与此同时还收获一个很好的副产物：从无标注数据得到的全任务标注样本，他们是十分宝贵的数据标签，考虑到跨任务标签有用太稀少。

#### 下游任务使用 CVT

理论上讲主预测模块可以采用任意形式，通用的或者视任务而定的都行。CVT 论文里两种情况都有。

在像 NER 或 POS tagging 这样的序列标注任务（给每个标识分类）中，预测器模块包括两个完整的全连接层和给输出加上的 softmax 层，以此生成有关类别标签的概率分布。对每个标识 $$\mathbf{x}_i$$，两层对应的隐态为 $$\mathbf{h}_1^{(i)}$$ 和 $$\mathbf{h}_2^{(i)}$$：


$$
\begin{aligned}
p_\theta(y_i \mid \mathbf{x}_i) 
&= \text{NN}(\mathbf{h}^{(i)}) \\
&= \text{NN}([\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}]) \\
&= \text{softmax} \big( \mathbf{W}\cdot\text{ReLU}(\mathbf{W'}\cdot[\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}]) + \mathbf{b} \big)
\end{aligned}
$$



第一层里只给辅助任务喂前向或后向 LSTM 状态，因为它们只看到了部分上下文，要么左边要么右边，它们必须学着像语言模型那样，在指定情境中预测下一个标识。`fwd` 和 `bwd` 辅助任务只取一个方向。`future` 和 `past` 任务则分别在前向和后向上多走一步。


$$
\begin{aligned}
p_\theta^\text{fwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{fwd}(\overrightarrow{\mathbf{h}}^{(i)}) \\
p_\theta^\text{bwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{bwd}(\overleftarrow{\mathbf{h}}^{(i)}) \\
p_\theta^\text{future}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{future}(\overrightarrow{\mathbf{h}}^{(i-1)}) \\
p_\theta^\text{past}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{past}(\overleftarrow{\mathbf{h}}^{(i+1)})
\end{aligned}
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_CVT-example.png)

*图 5  序列标注任务依赖于四个辅助预测模型，它们的输入只有单方向的隐态信息：前，后，将来以及过去*

注意，如果主预测模块有 dropout，那处理有标注数据的时候 dropout 层照常工作，但用无标注数据训练，为辅助任务生成“柔性”目标时不参与计算。

机器翻译任务中，主预测模块换成了标准的带 attention 的 LSTM 解码器。涉及两个辅助任务：（1）dropout，随机删掉 attention 权重向量的一些值；（2）预测目标序列的未来词。辅助任务相应的主预测是用固定主解码器对输入做[集束搜索](https://en.wikipedia.org/wiki/Beam_search)从而得到最优预测目标序列。

### ULMFiT

受 ImageNet 预训练在计算机视觉任务上攻城略地的启发，生成式预训练 LM + 任务微调的思路最先在 ULMFiT（[Howard 和 Ruder, 2018](https://arxiv.org/abs/1801.06146)）中进行了尝试。基准模型是 [AWD-LSTM](https://arxiv.org/abs/1708.02182)。

ULMFiT 通过三步在下游语言分类任务上取得了良好的迁移学习效果：

1）*通用 LM 预训练*：Wikipedia 语料

2）*目标任务 LM 微调*：ULMFiT 提出了两个训练技术来稳定微调过程。

-  **差异化微调（Discriminative fine-tuning）**是基于不同 LM 层学到的信息类型不同这一事实提出的。（上文有[讨论](# ELMo 表示)）。ULMFiT 以不同的学习率调教各层，$$\{\eta^1, \dots, \eta^\ell, \dots, \eta^L\}$$，$$\eta$$ 是第一层的基础学习率，$$\eta^\ell$$ 是第 $$\ell$$ 层，一共 $$L$$ 层。
-  **斜三角学习率（Slanted triangular learning rates，STLR）**是种特殊的学习率，先增后减。增长阶段较短，便于模型快速收敛到和任务适配的参数空间，衰减器较长好取得更好的微调效果。

3）目标任务分类器微调：加两个标准前馈层强化预训练 LM，末端加上 softmax 归一化来预测目标标签分布。

- **连接池化（Concat pooling）**对过往隐态取 max-pooling 和 mean-pooling，并将其和最终隐态拼在一起
- **逐步解冻（Gradual unfreezing）**是为了避免灾难性遗忘，做法是从最后一层开始逐步解冻模型各层。首先解冻最后一层，微调一轮，然后倒数第二层解冻，不断重复直到调整完所有层。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_ULMFiT.png)

*图 6  ULMFiT 的三个训练阶段*

### OpenAI GPT

依着和 ELMo 类似的想法，OpenAI 的**生成式预训练 Transformer（Generative Pre-training Transformer，GPT，[Radford 等人, 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)）**通过在巨量文本语料上训练，极大的增加了无监督语言模型的规模。抛开相似性，GPT 和 ELMo 主要有两处不同。

1. 模型架构不同：ELMo 是将独立训练的自左向右和自右向左的多层 LSTM 进行浅层拼接，而 GPT 是个多层 transformer 解码器
2. 情境化嵌入在下游任务中的用法不同：ELMo 是将 embedding 作为额外特征喂给特定任务模型，而 GPT 是将相同的基准模型进行微调来解决各式任务

#### 拿 Transformer 解码器当语言模型

相较于[原始 Transformer](https://arxiv.org/abs/1706.03762) 架构，[Transformer 解码器](https://arxiv.org/abs/1801.10198)模型丢掉了编码器部分，所以只有一个输入序列，而非两个独立的源和目标序列。

该模型是在输入序列的 embedding 上加了多层 transformer 块进行处理。每一块内都有一个遮罩*多头自注意力（multi-headed self-attention）*层和一个*元素级前馈（pointwise feed-forward）*层。经 softmax 归一化后最终可以得到一个目标标识的分布。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_OpenAI-GPT-transformer-decoder.png)

图 7  OpenAI GPT 的 transformer 解码器模型架构

损失为负对数似然概率，和 ELMo 一样，但没了反向计算。选定目标词前大小为 $$k$$ 的上下文窗口计算损失：


$$
\mathcal{L}_\text{LM} = -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1})
$$



#### 字节对编码

**字节对编码（Byte Pair Encoding，[BPE](https://arxiv.org/abs/1508.07909)）**用于给输入序列编码。BPE 原是 20 世纪 90 年代提出的一种数据压缩算法，然后被拿去解决机器翻译里的开放词汇表问题，因为在译为某种新语言的时候很容易遇到少见或未知的词。直觉上稀有或未知词经常可以拆成多个子词，BPE 就不断迭代，贪婪地合并常见字符对以寻找最佳字词分割。

#### 有监督微调

OpenAI GPT 所做的最大改进是与任务模型解耦，直接用预训练语言模型！

以分类任务为例。标注数据集中每个输入有 $$n$$ 个标识，$$\mathbf{x} = (x_1, \dots, x_n)$$，和一个标签 $$y$$。GPT 先用预训练 transformer 解码器处理输入序列 $$\mathbf{x}$$，最后一个标识 $$x_n$$ 在最后一层的输出为 $$\mathbf{h}_L^{(n)}$$。靠着仅有的训练得到的权重矩阵 $$\mathbf{W}_y$$，模型可以预测类别标签的分布。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_GPT-classification.png)
$$
P(y\mid x_1, \dots, x_n) = \text{softmax}(\mathbf{h}_L^{(n)}\mathbf{W}_y)
$$


损失则是最小化真实标签的负对数似然概率，此外作者发现加上 LM 损失作为辅助损失会有好处，因为：

1. 训练时能帮着加速收敛
2. 希望改善下监督模型的泛化效果


$$
\begin{aligned}
\mathcal{L}_\text{cls} &= \sum_{(\mathbf{x}, y) \in \mathcal{D}} \log P(y\mid x_1, \dots, x_n) = \sum_{(\mathbf{x}, y) \in \mathcal{D}} \log \text{softmax}(\mathbf{h}_L^{(n)}(\mathbf{x})\mathbf{W}_y) \\
\mathcal{L}_\text{LM} &= -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1}) \\
\mathcal{L} &= \mathcal{L}_\text{cls} + \lambda \mathcal{L}_\text{LM}
\end{aligned}
$$

有了类似设计后，就没必要给其他末端任务定制模型架构了（见图 8）。如果输入里有多个序列，每对序列间会加上一个特殊的定界标识 `$`，定界标识的 embedding 是我们要学习的一个新参数，不过会很简短。

对序列相似度任务来讲，因为排序没必要，先后序都有。多选任务里，情境会和每个候选答案结对。


![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_openai-input%20transformations.png)

*图 8  各下游任务的微调 GPT transformer 模型训练目标*

**总结**：当时（2018.6）看到这样一个通用框架在大多数语言任务上取得 SOTA，让人感觉很巧妙且倍受鼓舞。一阶段，语言模型的生成式预训练过程会吸纳尽可能多的免费文本信息。二阶段，用一个较小的标注数据集针对特定任务对模型进行微调，学习一小批新参数。

GPT 的一个局限之处在于单向性——模型只会自左向右的预测上下文。

### BERT

**Transformer 双向编码器表示（Bidirectional Encoder Representations from Transformers，BERT，[Devlin 等人, 2019](https://arxiv.org/abs/1810.04805)）** 算是 GPT 的直系后代：在免费语料上训练一个大语言模型，然后针对特定任务微调而不需要单独定制网络架构。

相较于 GPT，BERT 最大的区别和改善之处在于双向训练，模型会学者预测左右两边的上下文，论文中消融研究的结果称：

> “模型双向性是最为重要的新贡献”

#### 预训练任务

BERT 的模型架构为多层双向 Transformer 编码器。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_transformer-encoder-2.png)

*图 9  Transformer 编码器模型架构*

为了促进双向预测和句子级理解，相较于在基础语言任务上训练（给定场景下预测下一个标识），BERT 要同时完成两项任务。

**任务 1：遮罩语言模型（Mask language model，MLM）**

> 维基百科：“完形填空（填充缺失测验）是一项练习，测试或评估，由一部分语句构成，其中特定项，字词或符号会被删除（填空文本），参试者的任务是替换缺失语言项……该练习首先由 W.L. Taylor 在 1953 年提出”

毫无疑问，从前后语境中而不只是从后文中学到的表示能更好的领会含义，无论是在语义还是句法上。BERT 通过“*遮罩语言模型* ”任务来实现这一点：

1. 各句子中随机挡住 15% 的内容。因为如果我们纯用特殊占位符 `[MASK]` 换掉被遮标识，微调时候有的标识就再也看不到了。所以 BERT 用了几个启发式技巧：
   - a）80% 的概率用 `[MASK]` 替换选定词
   - b）10% 的概率用随机词替换
   - c）10% 的概率保持不变
2. 模型只预测缺失词，但它不知道哪个词被替换了，或者要预测哪个词。结果的尺寸只有输入的 15%

**任务 2：下一句预测**

许多下游任务涉及到对句子间关系的理解（QA，NLI），BERT 额外加了一个辅助任务，训练一个*二分类器（binary classifier）*判断一句话是不是另一句的下文：

1. 采样语句对（A,B）满足：
   - a）50% 的情况 B 是 A 的下文
   - b）50% 的情况不是
2. 模型对两句话进行处理并输出一个二值标签，指明 B 是否就是 A 后的下一句话

上述两个辅助任务的训练数据可以轻易从任意单语语料中获取，所以训练规模不受限制。训练损失是累计平均遮罩 LM 概率，和累计平均下文预测概率。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_language-model-comparison.png)

*图 10  BERT，OpenAI GPT 和 ELMo 模型架构对比*

#### 输入嵌入

输入嵌入是三部分的和：

1. *字段标识嵌入（WordPiece tokenization embeddings）*：字段模型原本是针对日语或德语的分割问题提出的。相较于使用自然分隔的英文单词，它们可以进一步分成更小的子词单元便于处理罕见词或未知词。感兴趣的话可以看看分词优化方式的论文[[1]](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)[[2]](https://arxiv.org/pdf/1609.08144.pdf)。
2. *分段嵌入（segment embedding）*：如果输入的是两句话，分别对句子 A 和句子 B 做嵌入，并用特殊标识 `[SEP]` 隔开；如果输入只有一句话就只用句子 A 的嵌入
3. *位置嵌入（position embeddings）*：位置信息需要学习而非硬编码

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_BERT-input-embedding.png)

*图 11  BERT 输入表示*

注意第一个标识必须是 `[CLS]`——之后下游任务预测中会用到的占位符。

#### 下游任务使用 BERT

BERT 的微调只需要加很少的参数，就像 OpenAI GPT。

对于分类任务，取第一个标识 `[CLS]` 的最终隐态 $$\mathbf{h}^\text{[CLS]}_L$$，将它和一个小权重矩阵相乘， $$\text{softmax}(\mathbf{h}^\text{[CLS]}_L \mathbf{W}_\text{cls})$$

对像 SQuAD 这样的 QA 任务，我们要针对问题预测给定段落的文本跨度。BERT 对每个标识要预测两个概率分布，分别对应文本开端和末尾。微调时新训练的只有两个小矩阵 $$\mathbf{W}_\text{s}$$ 和 $$\mathbf{W}_\text{e}$$，$$\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{s})$$ 和 $$\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{e})$$ 对应两个概率分布。

总体来讲下游任务微调加上的内容很少——一个或两个权重矩阵，负责将 Transform 隐态转换成可解释形式。其他情况可以看论文了解实施细节。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-tasks.png)

*图 12  微调 BERT 模型对不同下游任务的训练目标*

可以列张表比较下 OpenAI GPT 和 BERT 间的差别。

|          | **OpenAI GPT**                   | **BERT**                                                    |
| -------- | -------------------------------- | ----------------------------------------------------------- |
| 特殊字符 | 只在微调时引入`[SEP]` 和 `[CLS]` | 预训练阶段就学习 `[SEP]` 、 `[CLS]` 和句子 A/B 的 embedding |
| 训练过程 | 1M 步, batch size 32k 个词       | 1M 步, batch size 128k 个词                                 |
| 微调     | 所有任务 lr = 5e-5               | 看任务定 lr                                                 |

### ALBERT

**精简 BERT（A Lite BERT，ALBERT，[Lan 等人，2019](https://arxiv.org/abs/1909.11942)）**是 BERT 的简化版，相似的配置参数减少 18 倍，训练速度提升 1.7 倍。ALBERT 所作的改进有三点：排前两位的当然是减少了参数，降低了内存开销从而提升了训练速度，而第三点则是用一个更有挑战性的训练任务替换掉了下一句预测（NSP）。

#### 分解式嵌入参数化

BERT 中，字段标识 embedding 的大小 $$E$$ 被设置成与隐态尺寸 $$H$$ 相同，即是说如果想增大模型（调高 $$H$$），也就得学习更大的标识 embedding，而这又依赖于词表大小  $$V$$ ，计算成本就有点高了。

理论上，标识嵌入应当学习的是情境独立的表示，而隐态是依赖环境的，所以将隐层大小和词表嵌入的尺寸分开比较合理。通过分解式嵌入参数化，$$V \times H$$ 的大词表嵌入矩阵就被分成 $$V \times E$$ 和 $$E \times H$$ 的两个小矩阵。一般 $$H \gt E$$ 或 $$H \gg E$$，因子分解可以显著降低参数数量。

#### 跨层参数共享

参数跨层共享有多种方式：（a）只共享前馈部分（b）只共享注意力参数（c）共享所有参数。该方法可以大量削减参数，同时又不会太伤害性能。

#### 序列次序预测

有趣的是，BERT 的下一句预测（NSP）任务被证明太简单了。ALBERT 换用序列顺序预测（sentence-order prediction，SOP）的自监督损失，

- 正样本：同文档中连续的两个部分
- 负样本：和上面一样但次序颠倒

对于 NSP 任务，当 A 和 B 的情境不同时，如果模型能检测到所谈话题它就能做出合理预测。相较而言，SOP 更难一些，因为这要求模型充分理解部分间的一致性和顺序。

### OpenAI GPT-2

[OpenAI](https://blog.openai.com/better-language-models/) [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 直接继承自 GPT。GPT-2 有 15 亿参数，比原 GPT 大了 10 多倍，在受测的 8 个语言模型数据集上拿了 7 个 SOTA，采用*零尝试迁移配置（zero-shot transfer setting）*不需要任何任务微调。预训练数据集由 80 亿 Web 页面构成，页面是从 [Reddit](https://www.reddit.com/) 上爬下来的合格外链。在小数据集和测试*长文依赖（long-term dependency）*的数据集上 GPT-2 进步明显。

#### 零尝试迁移

GPT-2 的预训练就纯是语言建模。所有下游语言任务都被当成条件概率预测，也没有任务相关的微调。

#### 字节序列上的 BPE

#### 模型调整

### RoBERTa

### T5

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-15_T5.png)

### 总结

### 指标：复杂度


$$
2^{H(p)} = 2^{-\sum_x p(x) \log_2 p(x)}
$$
 $$N$$  $$s = (w_1, \dots, w_N)$$, , $$\frac{1}{N}$$:
$$
H(s) = -\sum_{i=1}^N P(w_i) \log_2  p(w_i)  = -\sum_{i=1}^N \frac{1}{N} \log_2  p(w_i)
$$

$$
\begin{aligned}
2^{H(s)} &= 2^{-\frac{1}{N} \sum_{i=1}^N \log_2  p(w_i)}
= (2^{\sum_{i=1}^N \log_2  p(w_i)})^{-\frac{1}{N}}
= (p(w_1) \dots p(w_N))^{-\frac{1}{N}}
\end{aligned}
$$

### 常见任务和数据集



