---
layout:		post
title:  	生成语言模型
subtitle:   深化总结
date:       2020-12-21
author:     一轩明月
header-img: img/post-bg-code.jpg
catalog:    true
tags:
    - Transformer
    - attention
    - NLP
excerpt:    本文介绍了几种学习情景化词向量的模型，算是对之前 BERT，GPT-2等模型的总结，补充了早先和后继研究的部分细节，探讨了这些在各种语言任务上取得令人惊讶的 SOTA 成果的大型无监督预训练模型身上，透露出了什么新趋势
---

> 编译自：Generalized Language Models， [Lilian Weng](https://lilianweng.github.io/lil-log/)

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_elmo-and-bert.png)

2018 年对 NLP 领域来说是个不折不扣的转折年。像 OpenAI GPT 和 BERT 这样的大规模预训练语言模型采用通用模型架构，在各种语言任务上都取得了亮眼表现，和  ImageNet 分类预训练对视觉任务所带来的冲击可以等量齐观（×）。前者甚至更胜一筹，NLP 采用的方法简单而强大，预训练时不需要有标签数据，训练规模的增大也不妨碍实验，直至达到我们的极限。

> （×）：[He 等人，2018](https://arxiv.org/abs/1811.08883) 发现预训练可能对图像分割任务来说不是那么必要

在此之前的[词嵌入](https://libertydream.github.io/2020/02/13/embedding-%E5%9B%9E%E9%A1%BE/)，各种 embedding 都是针对特定上下文的——基于共现关系而非顺序语境学习。所以对 “I am eating an apple” 和 “I have an Apple phone”  两句话中的 “apple” 来讲，两个词指代的事物明显不同但还是会有相同的词嵌入向量。

尽管如此，早期解决问题的时候，用词嵌入也是将其作为已有的特定任务模型的附加特征，某种程度上所带来的改进有限。

本文中我们会介绍许多种基于环境做 embedding 的方法，好更简单、更容易的将词向量应用到下游任务上，格式通用。

### CoVe

**语境词向量（Contextual Word Vectors，Cove，[McCann 等人， 2017](https://arxiv.org/abs/1708.00107)）**是一类借助编码器做词嵌入的方法，编码器就是基于[注意力](https://libertydream.github.io/2020/04/26/Attention-%E7%BB%BC%E8%BF%B0/)的 seq-to-seq 机器翻译模型里的那种编码器。不同于传统的词嵌入方法，CoVe 的词表示是关于整个输入序列的函数。

#### NMT 概述

这里的神经机翻译（[NMT](https://github.com/THUNLP-MT/MT-Reading-List)）模型由一个标准的双层双向 LSTM 编码器，和一个额外的基于注意力的双层单向 LSTM 解码器构成，预先在英语-德语翻译任务上训练好。编码器学习并优化英文单词的 embedding 向量，好将其译为德文。直觉上编码器能在将词转为其他语言形式前领悟语义和句法内涵，编码器的输出结果被用做各种下游语言任务的情景化词嵌入。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_nmt-recap.png)

图 1  CoVe 中的 NMT 基础模型

-  源语言的 $$n$$ 字序列：$$x = [x_1, \dots, x_n]$$
-  目标语言的 $$m$$ 字序列：$$y = [y_1, \dots, y_m]$$
- 源字句的 GloVe 向量：$$\text{GloVe}(x)$$
-  随机初始化目标字句的嵌入向量：$$z = [z_1, \dots, z_m]$$
- biLSTM 编码器输出隐态序列：$$h = [h_1, \dots, h_n] = \text{biLSTM}(\text{GloVe}(x))$$ 和 $$h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]$$。前向 LSTM 计算 $$\overrightarrow{h}_t = \text{LSTM}(x_t, \overrightarrow{h}_{t-1})$$，反向计算  $$\overleftarrow{h}_t = \text{LSTM}(x_t, \overleftarrow{h}_{t-1})$$
-  附加的解码器输出一个字词分布：$$p(y_t \mid H, y_1, \dots, y_{t-1})$$ 其中 $$H$$ 是时间维度上的一组隐态 $$\{h\}$$：


$$
\begin{aligned}
\text{解码器隐态: } s_t &= \text{LSTM}([z_{t-1}; \tilde{h}_{t-1}], s_{t-1}) \\
\text{注意力权重: } \alpha_t &= \text{softmax}(H(W_1 s_t + b_1)) \\
\text{随情境而动的隐态: } \tilde{h}_t &= \tanh(W_2[H^\top\alpha_t;s_t] + b_2) \\
\text{解码器输出: } p(y_t\mid H, y_1, \dots, y_{t-1}) &= \text{softmax}(W_\text{out} \tilde{h}_t + b_\text{out})
\end{aligned}
$$



#### 下游任务使用 CoVe

NMT 编码器的隐态在其他语言任务中称为**环境向量（context vector）**：


$$
\text{CoVe}(x) = \text{biLSTM}(\text{GloVe}(x))
$$



论文将 GloVe 和 CoVe 串联用于问答和分类任务。GloVe 从全局字词共现率中学习，所以没有句子语境，而 CoVe 是通过处理文本序列得到的，能够捕捉情境信息。



$$
v = [\text{GloVe}(x); \text{CoVe}(x)]
$$


对给定的下游任务，我们先生成输入字词的 GloVe+CoVe 的串联向量，然后将其作为附加特征喂给相关任务模型。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_CoVe.png)

*图 2  CoVe embedding 是由用于机器翻译任务的编码器生成的。编码器可以插入任意下游任务模型中（图片来源：[原论文](https://arxiv.org/abs/1708.00107)）*

**总结**：CoVe 的局限性很明显：（1）预训练限于有监督翻译任务能得到的数据集；（2）CoVe 对最终效果的贡献受制于任务模型架构

下面我们会看到 ELMo 通过无监督预训练克服了问题（1），OpenAI GPT 和 BERT 进一步通过预训练 + 对不同下游任务采用生成模型架构将两个问题都解决了。

### ELMo

**语言模型嵌入（Embeddings from Language Model，ELMo，[Peters 等人 2018](https://arxiv.org/abs/1802.05365)）**通过以 *无监督* 方式预训练一个语言模型来获取情境化字词表示。

#### 双向语言模型

**双向语言模型（bidirectional Language Model，biLM)** 是 ELMo 的基础，当输入为 $$n$$ 个标识组成的序列时，$$(x_1,\dots,x_n)$$，语言模型会学着基于历史预测下一个标识的概率。

前向传递期间，历史由目标标识之前的字词构成，


$$
p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_1, \dots, x_{i-1})
$$



反向传递时，历史则由目标标识之后的字词构成



$$
p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_{i+1}, \dots, x_n)
$$


两个方向的预测由多层 LSTM 负责建模，$$\ell=1,\dots,L$$ 层的输入标识 $$x_i$$ 的隐态为 $$\overrightarrow{\mathbf{h}}_{i,\ell}$$ 和 $$\overleftarrow{\mathbf{h}}_{i,\ell}$$。经 softmax 归一化，用最后一层的隐态 $$\mathbf{h}_{i,L} = [\overrightarrow{\mathbf{h}}_{i,L}; \overleftarrow{\mathbf{h}}_{i,L}]$$ 获取标识概率。他们共享嵌入层和 softmax 层，参数分别为 $$\Theta_e$$，$$\Theta_s$$。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_ELMo-biLSTM.png)

*图 3  ELMo 的 biLSTM 基础模型*

模型要使两个方向的负对数概率最小化（= 最大化正确词的对数概率）：

$$
\begin{aligned}
\mathcal{L} = - \sum_{i=1}^n \Big( 
\log p(x_i \mid x_1, \dots, x_{i-1}; \Theta_e, \overrightarrow{\Theta}_\text{LSTM}, \Theta_s) + \\
\log p(x_i \mid x_{i+1}, \dots, x_n; \Theta_e, \overleftarrow{\Theta}_\text{LSTM}, \Theta_s) \Big)
\end{aligned}
$$

#### ELMo 表示

在 $$L$$ 层 biLM 基础上，ELMo 针对任务学习了一种线性组合方式，跨层将所有隐态堆在了一起。标识 $$x_i$$ 的隐态表示有 $$2L+1$$ 个向量：


$$
R_i = \{ \mathbf{h}_{i,\ell} \mid \ell = 0, \dots, L \}
$$


其中 $$\mathbf{h}_{0, \ell}$$ 是嵌入层输出，$$\mathbf{h}_{i, \ell} = [\overrightarrow{\mathbf{h}}_{i,\ell}; \overleftarrow{\mathbf{h}}_{i,\ell}]$$。每个末端任务都要学习一组线性组合权重 $$\mathbf{s}^\text{task}$$，要用 softmax 做归一化。比例因子 $$\gamma^\text{task}$$ 用于纠正 biLM 隐态分布和任务表示分布之间的偏差。


$$
v_i = f(R_i; \Theta^\text{task}) = \gamma^\text{task} \sum_{\ell=0}^L s^\text{task}_i \mathbf{h}_{i,\ell}
$$



为了评估从跨层隐态上得到了哪类信息，分别在语义和语法任务上使用 ELMo，期间要用不同 biLM 层的表示：

- **语义任务**：*词义消除（word sense disambiguation）*任务是要突出特定语境下词和含义。biLM 顶层比首层在该任务上表现更好。
- **语法任务**：*[词性标注](https://en.wikipedia.org/wiki/Part-of-speech_tagging)（part-of-speech tagging）*任务是要推断一个词在一句话中的语法成分。用 biLM 首层要比用顶层得到的准确率更高。

对比研究表明语法信息较低的层表示更好，而语义信息更高处的层领悟更深。因为不同层适于携带不同类型的信息，*将其堆叠起来会有所帮助*。

#### 下游任务使用 ELMo

和 CoVe 助力不同下游任务的方式类似，ELMo 嵌入向量也是用作输入或任务模型的底层。此外，对某些[任务](# 常见任务和数据集)（即 SNLI，SQuAD，但没 SRL）在输出层加上他们同样有所助益。

对那些只有少量监督数据的任务来说，ELMo 带来的提升是最大的。有了 ELMo，即使标签数据再少我们也能取得类似的效果。

总结：语言模型的预训练是无监督式的，而且鉴于无标注文本语料之丰富，理论上预训练规模可以极尽所能的增大。但它还是得依赖特定任务模型，所以改善只是渐进式的，给每个任务找个好模型架构仍然很重要。

### 跨视角训练

ELMo 中无监督的预训练和指定任务的学习，是在两个独立训练阶段由两个独立模型完成的。**跨视角训练（Cross-View Training，CVT， [Clark 等人, 2018](https://arxiv.org/abs/1809.08370)）**将二者结合，组成了一个统一的半监督学习过程，辅助任务上有标注数据的监督学习和无标注数据的无监督学习都能改善 biLSTM 编码器的表示。

#### 模型架构

模型由一个双层双向的 LSTM 编码器和一个主预测模块构成。训练时交替将标注数据和无标注数据喂给模型。

- 对*有标注的样本*，所有模型参数都通过标准监督学习进行更新。损失是标准交叉熵。
- 对*无标注样本*，主预测模块依旧可以得到一个“柔性”目标，尽管我们无法清楚地知道他们有多准。两个辅助任务中，预测器只能看到并处理有限视角下的输入内容，比如只用单方向上的编码器隐态表示。我们希望辅助任务的输出能和初步预测目标匹配，这样就能看到输入全貌了。

这样，编码器被强制将完整的上下文知识提炼为不完整的表示。在这一阶段，biLSTM 编码器是用反向传播的，但主预测模块是 *固定* 的。辅助预测和主预测间的差距就是我们要最小化的损失。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_CVT.png)

*图 4  半监督语言模型的跨视角训练*

#### 多任务学习

在同步训练多个任务的时候，CVT 给附加的任务加了几个主预测模型，它们共享一个句子表示编码器。监督训练时，随机选择一个任务后，相应预测器参数和表示编码器得到更新。如果是无标注数据样本，联结所有任务优化编码器，力求每个任务上辅助输出和预测间的差距最小化。

多任务学习有利于增强表示的泛化能力，与此同时还收获一个很好的副产物：从无标注数据得到的全任务标注样本，他们是十分宝贵的数据标签，考虑到跨任务标签有用太稀少。

#### 下游任务中的 CVT

 $$\mathbf{h}_1^{(i)}$$   $$\mathbf{h}_2^{(i)}$$:
$$
\begin{aligned}
p_\theta(y_i \mid \mathbf{x}_i) 
&= \text{NN}(\mathbf{h}^{(i)}) \\
&= \text{NN}([\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}]) \\
&= \text{softmax} \big( \mathbf{W}\cdot\text{ReLU}(\mathbf{W'}\cdot[\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}]) + \mathbf{b} \big)
\end{aligned}
$$

$$
\begin{aligned}
p_\theta^\text{fwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{fwd}(\overrightarrow{\mathbf{h}}^{(i)}) \\
p_\theta^\text{bwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{bwd}(\overleftarrow{\mathbf{h}}^{(i)}) \\
p_\theta^\text{future}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{future}(\overrightarrow{\mathbf{h}}^{(i-1)}) \\
p_\theta^\text{past}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{past}(\overleftarrow{\mathbf{h}}^{(i+1)})
\end{aligned}
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_CVT-example.png)

### ULMFiT



-  $$\{\eta^1, \dots, \eta^\ell, \dots, \eta^L\}$$,   $$\eta$$   $$\eta^\ell$$   $$\ell$$-   $$L$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_ULMFiT.png)

### BERT

#### 预训练任务

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_transformer-encoder-2.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_language-model-comparison.png)

#### 输入嵌入

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_BERT-input-embedding.png)

#### 下游任务中的 BERT

 $$\mathbf{h}^\text{[CLS]}_L$$, , $$\text{softmax}(\mathbf{h}^\text{[CLS]}_L \mathbf{W}_\text{cls})$$. 

, $$\mathbf{W}_\text{s}$$   $$\mathbf{W}_\text{e}$$,  $$\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{s})$$   $$\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{e})$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_bert-tasks.png)

### ALBERT

#### 参数化要素嵌入

 $$E$$   $$H$$.   $$H$$)  ($$V$$).

 $$V \times H$$   $$V \times E$$ and $$E \times H$$.   $$H \gt E$$   $$H \gg E$$, 

#### 跨层参数共享

#### 序列次序预测（SOP）

### RoBERTa

### OpenAI GPT

#### 将 Transformer 解码器作语言模型

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_OpenAI-GPT-transformer-decoder.png)
$$
\mathcal{L}_\text{LM} = -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1})
$$

#### 字节对编码

#### 有监督微调

 $$n$$  , $$\mathbf{x} = (x_1, \dots, x_n)$$,  $$y$$.  $$\mathbf{x}$$  $$x_n$$ is $$\mathbf{h}_L^{(n)}$$.  $$\mathbf{W}_y$$,

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-12-09_GPT-classification.png)
$$
\begin{aligned}
\mathcal{L}_\text{cls} &= \sum_{(\mathbf{x}, y) \in \mathcal{D}} \log P(y\mid x_1, \dots, x_n) = \sum_{(\mathbf{x}, y) \in \mathcal{D}} \log \text{softmax}(\mathbf{h}_L^{(n)}(\mathbf{x})\mathbf{W}_y) \\
\mathcal{L}_\text{LM} &= -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1}) \\
\mathcal{L} &= \mathcal{L}_\text{cls} + \lambda \mathcal{L}_\text{LM}
\end{aligned}
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-11-16_openai-input%20transformations.png)





### OpenAI GPT-2

#### 零尝试 Transfer

#### 字节序列上的 BPE

#### 模型调整

### 总结

### 指标：复杂度


$$
2^{H(p)} = 2^{-\sum_x p(x) \log_2 p(x)}
$$
 $$N$$  $$s = (w_1, \dots, w_N)$$, , $$\frac{1}{N}$$:
$$
H(s) = -\sum_{i=1}^N P(w_i) \log_2  p(w_i)  = -\sum_{i=1}^N \frac{1}{N} \log_2  p(w_i)
$$

$$
\begin{aligned}
2^{H(s)} &= 2^{-\frac{1}{N} \sum_{i=1}^N \log_2  p(w_i)}
= (2^{\sum_{i=1}^N \log_2  p(w_i)})^{-\frac{1}{N}}
= (p(w_1) \dots p(w_N))^{-\frac{1}{N}}
\end{aligned}
$$

### 常见任务和数据集



