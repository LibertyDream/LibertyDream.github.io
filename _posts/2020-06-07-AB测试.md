---
layout:		post
title:  	机器学习模型的 A/B 测试
subtitle:   模型部署系列之八
date:       2020-06-07
author:     一轩明月
header-img: img/post-bg-code.jpg
catalog:    true
tags:
    - production
excerpt:    为了商业目标，在用户行为和模型间建立因果关系，除开离线测试之外还需要对模型进行在线测试。其中一种实验手段是 A/B 测试，通过分组实验，对照观察来把握因果关系。除实验介绍外，本文还给出了一个 A/B 测试样板
---

> 编译自：A/B Testing Machine Learning Models，[Luigi](https://mlinproduction.com/author/luigi/)

本系列的上一篇文章介绍了怎样[通过历史数据测试 ML 模型](https://libertydream.github.io/2020/05/31/测试驱动/)，在将模型部署上线 _之前_ 可以用这些 _离线_ 测试验证数据、基础设施以及模型。可尽管离线测试已经充分展示了模型在历史数据上的性能，但这些测试依旧不能在模型和用户行为结果间构建因果关系。当机器学习用于驱动特定用户行为时，像增加点击率或参与度，就需要**在线验证**或者说**实验**了。

本文会超脱机器学习语境介绍一下实验，然后说明为何对机器学习模型进行在线验证必不可少。本文还会介绍一门在线验证技术—— A/B 测试，及一个能对机器学习模型进行 A/B 测试的框架。

# 在线验证的必要性

想弄清为何对机器学习模型来说线上测试至关重要，首先要明白在线实验的必要性，这可归结为两点。

首先，企业都有自身的目标。这听起来像是废话，但这点确实要时刻放在心上。目标包括但不限于增加年度经常性收入，提高用户参与度，降低波动性等等。目标的实现进度依赖于对特定指标的跟踪，也就是**关键绩效指标（key performance indicators, KPIs）**。为达成目标，公司会采取各式各样的行动，包括但不限于营销活动，发布新功能，迭代既有功能等等。

这就带来一个重要问题，**公司怎么知道这些行动能促进目标实现呢**？这就引向我们要弄清的第二点：怎样确定 _因果关系_。假定某家公司通过 UI 改版来驱动特定用户行为，比如将网站按钮颜色从红色改为绿色，以此提高按钮点击率。或者某家电商选择简化结算流程来使丢弃清空购物车的用户最少。这些公司怎样才有十足把握知晓他们的行动能 _导致_ 积极结果的出现呢？

这就是实验，又称在线测试的用武之地了。线上测试用来在指定标准下衡量行动效果并与基线进行比对。基线也称为 _对照组_，发生变动的也叫 _实验组_。在 UI 例子中，对照组就是原来的按钮颜色（红），新颜色（绿）就是实验组。衡量标准是按钮点击率。

![2020-07-27_ui-ab-test](C:\Users\mfx66\Pictures\2020-07-27_ui-ab-test.png)

为了确认因果关系，要进行随机对照实验。其中一种实验方法称为 A/B 测试，测试中用户被明确地分为不重叠的两组，一组看的是实验组（绿按钮），而另一组见到的是对照组（红按钮）。经过一段时间，时长是实验前定好的，检测一下两组的点击率。如果实验组的指标更高，那就能确定新颜色带来了更高的点击率，可以向所有用户推广新的绿色按钮。假如对照组点击率更高，说明绿色按钮并未提高点击率，应该继续保持原来的红色按钮。

# 为什么对机器学习模型进行 A/B 测试

让我们继续用上面的框架来探讨为什么对机器学习模型进行在线实验验证至关重要。

组织为提升商业效果而投入时间和金钱搭建机器学习模型，商业目标实现进度依靠对 KPI 的考核来衡量。但是当数据科学家和机器学习工程师在机器上搭好了模型，却没有根据这些 KPI 测算进度，而把模型在历史数据上的表现当结果。显然模型离线测试表现好并不意味着它能驱动商业上真正关心的 KPI，因为无法通过从中建立因果关系。

举个例子，视频分享平台通过广告赚取收入，具体来讲是靠观众点击广告带来商业收益。产品团队知道那些在 app 上花费时间较多的用户更有可能点击更多的广告，所以他们选择优化[系列观看时间](https://developers.google.com/machine-learning/problem-framing/framing#bad-objectives)作为[代理指标](https://medium.com/@gibsonbiddle/4-proxy-metrics-a82dd30ca810)。新产品经理这么想并假定更好的推荐会提高参与度并最大化系列观看时间。这样，球传到了数据科学团队脚下。

很快数据科学团队建好了新推荐模型并准备部署上线，模型在历史数据上得到了验证并通过了离线测试，但我们并不知道这个模型是不是会增加视频观看时长。想验证原生猜测，觉得推荐效果好了参与度也就上去了，需要进行随机对照实验，也就是 A/B 测试。

# ML 模型 A/B 测试工业实例

在题为[《构建真实深度学习系统中得到的教训》](https://www.slideshare.net/xamat/lessons-learned-from-building-practical-deep-learning-systems)的演讲中，Xavier Amatriain 介绍了 12 条生产中构建和部署深度学习系统的经验。其中第九条强调需要通过在线实验对机器学习模型进行验证。按 Amatriain 所说，良好的离线性能是开始借助 A/B 测试进行在线验证的标志，要评估是否该将新模型上线，数据科学家们应该

> 在统计意义上相同的人群间使用不同的算法，衡量指标差异

一旦在在线测试中发现了显著改进，就可以将新模型向用户群推广。这意味着在向全体用户推广前需要对模型额外进行一步验证。至于离线性能和 A/B 测试结果间的相关性怎样，Amatriain 表示并不是很容易理解。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-14_offline_online_test_process.png)

2019 年的论文[《150 个成功机器学习模型：Booking.com 中得到的 6 点教训》](https://www.kdd.org/kdd2019/accepted-papers/view/150-successful-machine-learning-models-6-lessons-learned-at-booking.com)阐述了相似观点。在第三部分，题为**建模：模型离线性能只是体检**，作者讲到：

> 对于 Booking.com 来说，我们非常重视模型为我们的客户和生意带来的价值。借助随机对照试验（Randomized Controlled Trials，RCTs）和特定商业指标，如转化率、用户开票率和撤销率，可以估算价值大小。非常有意思的一个发现是，模型性能增强并不意味着价值增长。

> 必须强调，这种相关性的缺失并不是在离线性能和在线性能之间，而是存于离线性能收益和商业价值增长当中。同时我们并不想夸大这一结果的一般性，其外部性很容易受到质疑，比如这些模型适用于特定情景，服务于特定系统，以特定方式构建，而且都向相同商业指标看齐，尽管之前的模型已经提高了指标，它们还是进一步尝试改进。尽管如此，我们仍旧认为相关性的缺失是重大发现，仅当离线指标近乎于商业指标时，才能观测到其中的相关性。

按作者的观点，这一现象是由四个因素造成的：

- **价值绩效饱和**。不可能一直靠改进模型来持续获取商业价值

- **分组饱和**。实验组规模随时间推移不断缩小，统计上价值的显著增长变得越来越难检测到。
- **神秘谷效应**。模型性能随时间不断增强，某些用户会对模型精准预测他们的行为感到焦虑不安。这种消极感受影响到了用户体验。

- **过度优化代理指标**。模型可能会过度优化那些代表了特定商业目的的观测变量

# 怎样对 ML 模型进行 A/B 测试

那该怎样组织 A/B 测试，以判断新模型是否优于当下模型呢？

A/B 测试第一步是要确定我们希望达成的商业结果，并选好指标方便量化核对进度。实验中该指标有时也叫做**总体评价标准（Overall Evaluation Criterion，OEC）**。通常 OEC 是期望商业目标的代理指标而不是直接度量商业效果。偏爱代理指标的一个原因是速度，一个能按小时或日进行测算的 OEC 有助于更快的获得实验反馈。

下一步就要决定实验自身的参数了。要决定的两个参数是样本规模（控制组和对照组分别要多少用户）以及实验时长。用户怎样分组决定了哪些用户会遇到新机器学习模型，哪些继续使用当前部署模型。

实验要做多久取决于功效分析（power analysis），即你给出功效（可能发现假阴性的概率，_避免错放坏人的概率，也是发现事实的概率_）和显著水平（假阳性概率，_零假设“显然是错的”的概率_）。怎样决定这些值很多人都写过了（[例一](http://www.win-vector.com/blog/2014/05/a-clear-picture-of-power-and-significance-in-ab-tests/)，[例二](https://signalvnoise.com/posts/3004-ab-testing-tech-note-determining-sample-size)），也有[计算器](https://abtestguide.com/abtestsize/)能帮你进行计算。

# ML 模型 A/B 测试框架

终于来到了有挑战的部分，怎么实际进行测试呢？做实验意味着同时运行两个模型并保证实验组和对照组所见到的模型正确无误。想正确做到这一点依赖于公司的数据基础设施和数据模型。不妨先做一版原生实现然后进行改进，这里的实现采用的是类似于 Flask 的伪代码。

假定 `model_A` 是当下部署模型，向 `/predict` 终端发起 HTTP 请求可以询问模型。

```python
@app.route("/predict")
def predict():
    features = request.get_json['features']
    return model_A.predict(features)
```

试想将用户按照用户 ID 分成对照组和实验组，实验组面对的是新模型 `model_B`。原生 A/B 测试就是要实现路由逻辑，将用户和服务程序中的模型进行匹配。

```python
TREATMENT_IDS = {} # 实验组用户 ID

@app.route("/predict")
def predict():
    features = request.get_json['features']
    if request.get_json['user_id'] in TREATMENT_IDS:
        return model_B.predict(features)
    else:
        return model_A.predict(features)
```

该框架可以用下图表示

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-07-28_bad-ab-test-arch.png)

这并不是一个好样板，三个原因。

首先是责任委托。在实验之前，API 基准代码只负责处理对成品模型的请求，这可以通过几行代码封装进一个轻量应用当中，而且代码只需要相对来说很少的测试（假设 API 是使用良好测试过的框架写的）。将实验逻辑加到 API 里放大了应用责任，增加了基准代码量，要写的测试用例也不可避免的变多。

其次应该借助消除潜在错误源减缓风险。增加代码总会放大应用出错概率，代码越多测试越多。部署本身就增加了向基准代码中增添 bug 的概率，开始实验的时候要部署一次，实验结束的时候又要部署一次。我们最好借助[精心设计的接口](https://libertydream.github.io/2020/03/01/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%8E%A5%E5%8F%A3/)写成通用 API 逻辑，之后很少会对其进行修改。

然而最重要的原因，至少在操作层面，在于该样板是依靠开发者进行修改。产品实验应该交给产品经理进行管理（或者提供产品视角的某人），所以我们更希望 PM 能在任何必要时刻切换实验开关，而**无需**更新基准代码。如果实验是通过存于数据库中的配置信息进行管理的，这点可以轻易实现。配置项保有用户到实验组的映射，以及一些额外的元数据，像实验何时开启，计划运行时长，功效分析参数等等。

一个更好的做法是在请求服务的用户和模型之间加上额外的抽象层，这一层会按照实验设置分发请求。这样每个训练好的模型都托管于独立环境中，而路由程序担当了用户与模型间的中间人。路由程序接收请求，按实验配置决定要查询哪个模型，然后将请求发给模型支持程序。选定的模型将预测结果反馈给路由程序，路由程序再向用户反馈。这种样板可用下图表示。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-07-28_good-ab-test-architecture.png)

这种方法消除了原生方案的几个问题：

- **责任委托**。我们的应用包含两个服务，模型服务负责进行预测推理，路由服务则负责按照实验配置，分发请求到合适的模型服务器上
- **消除基准代码中的错误源**。实验过程中，在提供服务的同时，并不需要更新模型基准代码，只需要对每个想测试的新模型启动一个新模型服务器即可。这也不需要变动路由服务的代码，特别是当其为了读取实验配置而存在时（这应该是一项需求）

- **触发式实验开关**。产品经理能按实验配置开启或关闭实验，无需开发人员干预

谷歌云和 AWS 都有机制支持在部署机器学习模型的时候进行 A/B 测试。

# ML 模型与多臂老虎机

A/B 测试只是一类在线实验。多臂老虎机（Multi-Armed Bandits，MAB）实验会从测试数据中进行学习，动态增加游客分布比例，支持的花样更多。MAB 依靠“动态流量分配”持续鉴别哪个版本比其他选项更好，其程度如何。一旦鉴别出来 bandit 算法就会动态将流量主体近乎实时地分配给获胜方（收益最大的臂）。这意味着效果不好的臂随时间推移分得的流量会越来越少。MAB 保证测试期间转换总数会最大化，A/B 测试就做不到这点。另一方面，这也意味着你不可能靠统计置信息了解所有臂的效果。

更多关于多臂老虎机和该方法与 A/B 测试的差异，可以看这两篇文章

- [Multi-Armed Bandit(MAB) – A/B Testing Sans Regret](https://vwo.com/blog/multi-armed-bandit-algorithm/)
- [When to Run Bandit Tests Instead of A/B/n Tests](https://cxl.com/blog/bandit-tests/)

# 总结

组织通过 A/B 测试这样的在线实验了解他们的行动是否会带来期望结果。行动可以是营销活动，新产品功能或是既有产品的变动，比如副本销售微调或 UI 的轻微改动。为影响用户行为引入机器学习模型是另一个需要在线实验验证的“撬棍”。

部署模型的时候，要进行 A/B 测试需要额外的代码和相应基础设施，负责配置实验环境并将请求分配给合适模型。本文介绍了一种可用于对机器学习模型进行 A/B 测试的样板。