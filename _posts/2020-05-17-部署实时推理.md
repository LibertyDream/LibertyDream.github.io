---
layout:		post
title:  	上线实时推理
subtitle:   模型部署系列之五
date:       2020-05-17
author:     一轩明月
header-img: img/post-bg-code.jpg
catalog:    true
tags:
    - production
excerpt:    前文探讨了ML模型部署在线模式所要面临的问题，准实时性，A/B 测试，多版本上线，模型监控等。本文从在线模式的适用场合谈起，展示如何借助服务逻辑，机器学习代码和部署配置三项内容实现在线推断。以及如何部署到谷歌云上以自动适应网络流量波动

---

> 编译自：Online Inference for ML Deployment，[Luigi](https://mlinproduction.com/author/luigi/)

# 实时推理场景辨析

一般来讲，只要预测要求同步完成就要用到实时推理。本系列第一篇[文章](https://libertydream.github.io/2020/02/23/模型部署到底是在说什么/)中举了若干例子，讲解终端用户或系统可能通过怎样的方式与机器学习模型生成的预测结果进行互动。不妨探讨一下要实时生成结果的两个例子。

其中一个是说某家电商公司希望系统能在用户于移动或网页端登录时向其推荐产品，因为用户可能在一天中的任意时刻登录，请求一出现推荐内容就要做出回应。这项需求本身并非一定要在线推理，如果能提前成批算出并将预测内容缓存好，运行时取用缓存内容即可。

但假定我们希望将用户最新行为活动纳入推荐计算当中。比如，某用户与推荐产品进行了交互，我们希望根据交互发生的上下文（例如添加到购物车，删除产品等等）更新推荐项。这依赖于近乎实时的输入数据处理，这也强迫我们将模型部署为实时推断模式。预测内容要后台在线计算而非在间隔期预先计算，保证用户新近活动都被计算在内。

此外，模型在移动和 web 端要解耦，这样数据科学家在更新模型，版本回滚，实践各种发行策略时更加容易简便。

另一个例子是[估算 UberEats 送达时间](https://www.infoq.com/articles/uber-eats-time-predictions/)，模型要预估饥肠辘辘的顾客多久才能收到食物。预先批量预测在此例中显然不适用，因为预测要有提前量 Uber 就要预知到一些事：哪位顾客会点餐，他们要从哪个餐厅点餐，会点什么，哪位骑手能接单，交通状况，餐厅信息等等。这些实时数据约束迫使 Uber 的预测模型进行在线实时计算。当然，出于降低模型更新、在线验证和预测监控难度考量，移动和 web 端的模型还是解耦的好。

在不需要即时获取机器学习计算结果的时候就不需要用到实时推理。任何时候如果延迟准许异步获取预测内容，离线批处理都是更好的选择。这并非是说异步结果不能用于在线推理，而是批处理更容易实现同时能可观地降低维护成本。

实时推断需要机器一直处于运行态以响应需求。所以即使在没有任何请求的时候你依然要为这些实例付费。在有人提出”免服务器（serverless）“推断（比如 [AWS Lambda](https://aws.amazon.com/lambda/)）之前，这会导致在计算架构中多了一层复杂性（要记得表象背后，”serverless“ 需要永远在线的机器实现）。个人建议，**如果能以定期批量数据处理的方式完成机器学习任务，别犹豫！**

顺带值得一提流式机器学习的情况。目前为止，当我说实时推理，几乎都是指准实时在线推断。web API 准实时地返回模型结果，相较异步批量推断延时会少很多，但输入数据到模型还是会迟滞一段时间。正如[在线推断挑战](https://libertydream.github.io/2020/03/15/在线推断挑战/)一文所述，从所存储的历史数据中抽取特征较为复杂而且根据数据存储方式的不同可能会涉及一系列组件。

相比于等待数据收集并存好再用，[流式机器学习](https://databricks.com/session/streaming-machine-learning-in-spark)（流数据集上的机器学习）会在数据源源不断地抵达过程中进行模式识别，其能够适应随时变化的数据分布，通常是对应不同时区，并且当存储原始数据不可取时十分有用。这种情况与准实时在线推理存在着显著差异，需要专门处理数据流的一套专业工具套件。

# 在线推断地实现

知道了何时使用实时推理之后，来看一下怎样将机器学习模型部署成该模式。首先，我们要借助著名的 Flask 框架完成我们应用所需的服务逻辑。接着是考虑怎样通过模型接口获取预测内容，最后，则要通过 Google App Engine（GAE）对我们的 API 进行配置和部署。GAE 能轻松实现应用扩容，由此我们可以为海量客户提供机器学习服务。

简言之，在线推理的简易实现包含三个部分：

1. **服务逻辑**：负责接收请求以及进行答复。使用流行的 Flask 库来定义 API
2. **“ML 代码”**：负责预测结果的生成，这里会用到在之前在[模型部署中的接口设计](https://libertydream.github.io/2020/03/01/模型部署的软件接口/)一文中所设计的接口
3. **部署配置**：负责部署 API。这里会用到 Google App Engine

本实例中，假定所有原始数据都已包含在送来的请求当中，通常这与实际情况不符（见[在线推断挑战](https://libertydream.github.io/2020/03/15/在线推断挑战/)）

- 服务逻辑

顾名思义，服务逻辑就要承担接收请求，传达回复的职责。这里用单端点`/predict`定义 API，其会解析请求数据包，生成预测并返回。同时也会定义一个错误处理器，该 API 写在文件 `app.py` 中

```python
import logging

from flask import Flask, request

app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    '''返回模型预测结果'''
    data = request.get_json()
    loginfo('Incoming data:{}'.format(data))
    return {'prediction':None}

@app.errorhandler(500)
def server_error(e):
    logging.exception('An error occured during a request')
    return """
    发生致命错误：<pre>{}</pre>
    详细堆栈路径参见日志
    """.format(e), 500

if __name__ == '__main__':
    app.run(host='127.0.0.1', port=8080, debug=True)
```

`predict`方法接收 JSON 格式的 POST 请求，其中附带原始输入数据。下面看一下机器学习代码

- 机器学习代码

API 应该能从调教好的模型处获取预测结果。一种办法是将模型封存入库并迁移到 Google App Engine 上。但这和我们想创建一个自动化流程的目标相悖，理想状态下我们期望对于每个部署于项目之上的新模型都能保有相同的 API 接口逻辑。

[模型部署中的接口设计](https://libertydream.github.io/2020/03/01/模型部署的软件接口/)一文的主要教训在于提早设计好优良接口能为之后节省时间。由此，可以使用文中定义接口来降低我们的部署工作量。其中，方法 `load_model` 能从远程文件存储系统取回序列化模型并将其加载入内存。通过 `@app.before_first_request` 方法的装饰，可以确保方法会在首次请求前被调用，这样模型就被“嵌入”到服务当中：

```python
model = None

@app.before_first_request
def load_model():
    global model
    model_path = os.environ['REMOTE_MODEL_PATH']
    loginfo('加载模型：{}'.format(model_path))
    model = Model.from_remote(model_path)
```

通过对环境变量 `REMOTE_MODEL_PATH` 的设置，可以借助远程文件路径指定哪个模型要被加载到内存中。而借助环境变量使方法参数化可以防止将具体模型的任意信息应编码到 API 中，这样整个项目就可以用相同的脚本部署在线模型了，这一通行方案促进了自动化、可复用的机器学习部署实现。

载入内存后所要做的就是用模型进行预测了，调整 `predict` 方法加以实现：

```python
@app.route('/predict', methods=['POST'])
def predict():
    """返回模型预测结果"""
    global model
    data = request.get_json()
    loginfo('传入数据:{}'format(data))
    prediction = model.predict(data)
    inp_out = {'input':data, 'prediction':prediction}
    loginfo(inp_out)
    return inp_out
```

很简单不是吗，`global` 关键字告知解释器我们希望使用全局空间下的模型变量，从请求中获取输入数据后调用 `Model` 对象的 `predict` 方法，再将输入数据和相应预测计入日志并返回。

随时记录机器学习部署环境信息十分重要。日志可算是遥测的一种，对输入-输出对的记录以及对记录的收集整理有利于**模型监控**。比如对输入数据分布的监控可以确认是否有概念飘移发生导致预测质量下降。目标结果和训练集分布的比对同样是有效手段，借助**自我修复程序**可以更进一步：如果检测到概念飘移，可以自动[重新训练模型](https://mlinproduction.com/model-retraining/)，高效终止机器学习循环。

完整 `api.py` 内容如下：

```python
import logging
from flask import Flask, request

app = Flask(__name__)

model = None

@app.before_first_request
def load_model():
    global model
    model_path = os.environ['REMOTE_MODEL_PATH']
    loginfo('加载模型：{}'.format(model_path))
    model = Model.from_remote(model_path)

@app.route('/predict', methods=['POST'])
def predict():
    """返回模型预测结果"""
    global model
    data = request.get_json()
    loginfo('传入数据:{}'format(data))
    prediction = model.predict(data)
    inp_out = {'input':data, 'prediction':prediction}
    loginfo(inp_out)
    return inp_out

@app.errorhandler(500)
def server_error(e):
    logging.exception('An error occured during a request')
    return """
    发生致命错误：<pre>{}</pre>
    详细堆栈路径参见日志
    """.format(e), 500

if __name__ == '__main__':
    app.run(host='127.0.0.1', port=8080, debug=True)
```

可以看到我们的脚本当中机器学习代码所占比例极其的小。`api.py` 一共 35 行，只有两行是做机器学习的（13，21行）。正如著名论文[机器学习系统的隐性债务](https://libertydream.github.io/2020/05/10/ML%E9%9A%90%E6%80%A7%E5%80%BA%E5%8A%A1/)所述，实际情况下的 ML 系统通常都是如此，因为所需基础设施繁多且复杂，现实中 ML 代码仅仅只是 ML 系统的一小部分。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-10_real_world_ML_system.png)

# 部署配置

该部署机器学习 API 了，能帮助部署 API 的服务有很多，包括 AWS，Azure，Heroku 等等，本文将会使用 [Google App Engine](https://cloud.google.com/appengine/)（GAE）。GAE 能让开发人员在全托管平台上构建部署网络应用而无需管理服务器。它支持许多像 Python 这样的流行语言，能根据应用流量自动扩（缩）容，同时带有监控、日志、安全以及开箱即用的诊断工具。在数据科学家们部署实时推断机器学习 API 接口的时候，上述特性加持的 App Engine 无疑是一套完美解决方案。

App Engine 提供了[两类编程环境](https://cloud.google.com/appengine/docs/flexible/python/flexible-for-standard-users)。**标准**环境能快速扩容，开销很低，但在某些方面受到限制，比如会强制代码在轻量沙河当中运行，禁止应用的磁盘写操作，限制应用 CPU 和 RAM 的可用量。反之，**弹性**环境下你的 app 会运行在 Google Compute Engine 虚拟机上的 Docker 容器内，限制更少，在便捷性和灵活性间取得完美平衡。

App Engine 的 app 都是层次化组织而成：

- **服务**

应用的逻辑组件可以安全地和其他组件共享 App Engine 特性并进行交流。App Engine 中的各项服务由你的源代码以及相应 App Engine 配置文件构成。部署到服务的一组文件代表该服务的一个版本，每次部署服务都是在相同服务下添加新版本。

- **版本**

app 的各项服务都有多个版本，可以快速在应用的不同版本间进行切换来回滚、测试或是为了其他的临时事务

- **实例**

负责运行服务各版本的底层计算资源。应用会扩充运行态实例数量来提供持久化服务，或是减少数量来最少化闲置实例并降低开销

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-06-27_gae_architecture.png)

# 配置

这里会将 API 配置成单服务应用，负责处理用户请求，用训练过的模型生成预测以及将推荐内容送回客户端。该架构的代码库可以归纳为单目录形式：

```
├── app.yaml # App Engine 配置文件
├── api.py  # 逻辑
└── requirements.txt # 请求
```

用 `app.yaml` 文件在 App Engine 中配置 Python 应用，其中包括 CPU，内存，网络，磁盘资源，大小和环境变量在内的其他通用设置。本文实例的配置如下：

```
runtime: python
env: flex
entrypoint: gunicorn -b :$PORT main:app
runtime_config:
    python_version:3
resources:
    cpu: 2
    memory_gb: 8
    disk_size_gb: 16
```

开头的几项是通用设置。`runtime` 指定应用所需的 App Engine 语言运行环境，这里选的是 `python` 环境，版本设置是在 `runtime_config` 处。`env` 设定应用环境模式，我们选的 `flex` 表示弹性环境。`entrypoint` 是应用启动命令，上面的入口会针对 **PORT** 环境变量设定端口上的 HTTP 请求开启一个进程。

`resources` 部分控制着实例的计算资源。 App Engine 按照 CPU 和指定内存量分配[机器类型](https://cloud.google.com/compute/docs/machine-types)，机器保证最少拥有指定资源但是可能会更多。`cpu` 指定核数，至少为 1 或是 2-96 间的偶数。`memory_gb` 指定要多少 GB 的内存，每个 CPU 内核需要总计 0.9 到 6.5 GB 的内存。`disk_size_gb` 指定有多少 GB 的硬盘，取值范围为 10 GB 到 10240 GB。

要想查看所有的配置文件列表项，参见`app.yaml` [参考文档](https://cloud.google.com/appengine/docs/flexible/python/reference/app-yaml)

# 部署应用

要把应用部署到 App Engine 最简单的方法是在命令行输入 `gcloud app deploy` 命令，该命令会借助 [Cloud Build](https://cloud.google.com/cloud-build/docs/) 服务自动生成容器快照并将其部署到 App Engine 弹性环境下。容器内包含你对运行快照所做的任何局部修改。

部署应用前必须确保：

1. GCP 项目[所有者](https://cloud.google.com/appengine/docs/flexible/python/access-control#owner)有启用的 [App Engine](https://cloud.google.com/appengine/docs/flexible/python/managing-projects-apps-billing#create)
2. 用户帐号拥有[所需权益](https://cloud.google.com/appengine/docs/flexible/python/access-control#primitive_roles)

要验证必须用你的帐号资质通过 `gcloud init` 命令授权 `gcloud` 对谷歌云平台的访问，这会触发包括通过 web 浏览器访问某个 URL 以获取密钥在内的多个步骤。一旦完成即可部署应用。

要部署应用服务的某个版本，在服务 `app.yaml` 文档所在目录下运行 `gcloud app deploy` 命令，默认情况下 `deploy` 命令会生成该版本的唯一 ID，将该版本配置到 GCP 项目上去，也就是指定 `gcloud` 工具用的那个，以及将所有流量导向新版本。

