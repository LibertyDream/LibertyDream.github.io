---
layout:		post
title:  	上线实时推理
subtitle:   模型部署系列之五
date:       2020-05-17
author:     一轩明月
header-img: img/post-bg-code.jpg
catalog:    true
tags:
    - production
excerpt:    前文探讨了ML模型部署在线模式所要面临的问题，准实时性，A/B 测试，多版本上线，模型监控等。本文从在线模式的适用场合谈起，展示如何借助服务逻辑，机器学习代码和部署配置三项内容实现在线推断。以及如何部署到谷歌云上以自动适应网络流量波动
---

> 编译自：Online Inference for ML Deployment，[Luigi](https://mlinproduction.com/author/luigi/)

# 实时推理场景辨析

一般来讲，只要预测要求同步完成就要用到实时推理。本系列第一篇[文章](https://libertydream.github.io/2020/02/23/模型部署到底是在说什么/)中举了若干例子，讲解终端用户或系统可能通过怎样的方式与机器学习模型生成的预测结果进行互动。不妨探讨一下要实时生成结果的两个例子。

其中一个是说某家电商公司希望系统能在用户于移动或网页端登录时向其推荐产品，因为用户可能在一天中的任意时刻登录，请求一出现推荐内容就要做出回应。这项需求本身并非一定要在线推理，如果能提前成批算出并将预测内容缓存好，运行时取用缓存内容即可。

但假定我们希望将用户最新行为活动纳入推荐计算当中。比如，某用户与推荐产品进行了交互，我们希望根据交互发生的上下文（例如添加到购物车，删除产品等等）更新推荐项。这依赖于近乎实时的输入数据处理，这也强迫我们将模型部署为实时推断模式。预测内容要后台在线计算而非在间隔期预先计算，保证用户新近活动都被计算在内。

此外，模型在移动和 web 端要解耦，这样数据科学家在更新模型，版本回滚，实践各种发行策略时更加容易简便。

另一个例子是[估算 UberEats 送达时间](https://www.infoq.com/articles/uber-eats-time-predictions/)，模型要预估饥肠辘辘的顾客多久才能收到食物。预先批量预测在此例中显然不适用，因为预测要有提前量 Uber 就要预知到一些事：哪位顾客会点餐，他们要从哪个餐厅点餐，会点什么，哪位骑手能接单，交通状况，餐厅信息等等。这些实时数据约束迫使 Uber 的预测模型进行在线实时计算。当然，出于降低模型更新、在线验证和预测监控难度考量，移动和 web 端的模型还是解耦的好。

在不需要即时获取机器学习计算结果的时候就不需要用到实时推理。任何时候如果延迟准许异步获取预测内容，离线批处理都是更好的选择。这并非是说异步结果不能用于在线推理，而是批处理更容易实现同时能可观地降低维护成本。

实时推断需要机器一直处于运行态以响应需求。所以即使在没有任何请求的时候你依然要为这些实例付费。在有人提出”免服务器（serverless）“推断（比如 [AWS Lambda](https://aws.amazon.com/lambda/)）之前，这会导致在计算架构中多了一层复杂性（要记得表象背后，”serverless“ 需要永远在线的机器实现）。个人建议，**如果能以定期批量数据处理的方式完成机器学习任务，别犹豫！**

