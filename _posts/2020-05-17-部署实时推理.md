---
layout:		post
title:  	上线实时推理
subtitle:   模型部署系列之五
date:       2020-05-17
author:     一轩明月
header-img: img/post-bg-code.jpg
catalog:    true
tags:
    - production
excerpt:    前文探讨了ML模型部署在线模式所要面临的问题，准实时性，A/B 测试，多版本上线，模型监控等。本文从在线模式的适用场合谈起，展示如何借助服务逻辑，机器学习代码和部署配置三项内容实现在线推断。以及如何部署到谷歌云上以自动适应网络流量波动

---

> 编译自：Online Inference for ML Deployment，[Luigi](https://mlinproduction.com/author/luigi/)

# 实时推理场景辨析

一般来讲，只要预测要求同步完成就要用到实时推理。本系列第一篇[文章](https://libertydream.github.io/2020/02/23/模型部署到底是在说什么/)中举了若干例子，讲解终端用户或系统可能通过怎样的方式与机器学习模型生成的预测结果进行互动。不妨探讨一下要实时生成结果的两个例子。

其中一个是说某家电商公司希望系统能在用户于移动或网页端登录时向其推荐产品，因为用户可能在一天中的任意时刻登录，请求一出现推荐内容就要做出回应。这项需求本身并非一定要在线推理，如果能提前成批算出并将预测内容缓存好，运行时取用缓存内容即可。

但假定我们希望将用户最新行为活动纳入推荐计算当中。比如，某用户与推荐产品进行了交互，我们希望根据交互发生的上下文（例如添加到购物车，删除产品等等）更新推荐项。这依赖于近乎实时的输入数据处理，这也强迫我们将模型部署为实时推断模式。预测内容要后台在线计算而非在间隔期预先计算，保证用户新近活动都被计算在内。

此外，模型在移动和 web 端要解耦，这样数据科学家在更新模型，版本回滚，实践各种发行策略时更加容易简便。

另一个例子是[估算 UberEats 送达时间](https://www.infoq.com/articles/uber-eats-time-predictions/)，模型要预估饥肠辘辘的顾客多久才能收到食物。预先批量预测在此例中显然不适用，因为预测要有提前量 Uber 就要预知到一些事：哪位顾客会点餐，他们要从哪个餐厅点餐，会点什么，哪位骑手能接单，交通状况，餐厅信息等等。这些实时数据约束迫使 Uber 的预测模型进行在线实时计算。当然，出于降低模型更新、在线验证和预测监控难度考量，移动和 web 端的模型还是解耦的好。

在不需要即时获取机器学习计算结果的时候就不需要用到实时推理。任何时候如果延迟准许异步获取预测内容，离线批处理都是更好的选择。这并非是说异步结果不能用于在线推理，而是批处理更容易实现同时能可观地降低维护成本。

实时推断需要机器一直处于运行态以响应需求。所以即使在没有任何请求的时候你依然要为这些实例付费。在有人提出”免服务器（serverless）“推断（比如 [AWS Lambda](https://aws.amazon.com/lambda/)）之前，这会导致在计算架构中多了一层复杂性（要记得表象背后，”serverless“ 需要永远在线的机器实现）。个人建议，**如果能以定期批量数据处理的方式完成机器学习任务，别犹豫！**

顺带值得一提流式机器学习的情况。目前为止，当我说实时推理，几乎都是指准实时在线推断。web API 准实时地返回模型结果，相较异步批量推断延时会少很多，但输入数据到模型还是会迟滞一段时间。正如[在线推断挑战](https://libertydream.github.io/2020/03/15/在线推断挑战/)一文所述，从所存储的历史数据中抽取特征较为复杂而且根据数据存储方式的不同可能会涉及一系列组件。

相比于等待数据收集并存好再用，[流式机器学习](https://databricks.com/session/streaming-machine-learning-in-spark)（流数据集上的机器学习）会在数据源源不断地抵达过程中进行模式识别，其能够适应随时变化的数据分布，通常是对应不同时区，并且当存储原始数据不可取时十分有用。这种情况与准实时在线推理存在着显著差异，需要专门处理数据流的一套专业工具套件。

# 在线推断地实现

知道了何时使用实时推理之后，来看一下怎样将机器学习模型部署成该模式。首先，我们要借助著名的 Flask 框架完成我们应用所需的服务逻辑。接着是考虑怎样通过模型接口获取预测内容，最后，则要通过 Google App Engine（GAE）对我们的 API 进行配置和部署。GAE 能轻松实现应用扩容，由此我们可以为海量客户提供机器学习服务。

简言之，在线推理的简易实现包含三个部分：

1. **服务逻辑**：负责接收请求以及进行答复。使用流行的 Flask 库来定义 API
2. **“ML 代码”**：负责预测结果的生成，这里会用到在之前在[模型部署中的接口设计](https://libertydream.github.io/2020/03/01/模型部署的软件接口/)一文中所设计的接口
3. **部署配置**：负责部署 API。这里会用到 Google App Engine

本实例中，假定所有原始数据都已包含在送来的请求当中，通常这与实际情况不符（见[在线推断挑战](https://libertydream.github.io/2020/03/15/在线推断挑战/)）

- 服务逻辑

顾名思义，服务逻辑就要承担接收请求，传达回复的职责。这里用单端点`/predict`定义 API，其会解析请求数据包，生成预测并返回。同时也会定义一个错误处理器，该 API 写在文件 `app.py` 中

```python
import logging

from flask import Flask, request

app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    '''返回模型预测结果'''
    data = request.get_json()
    loginfo('Incoming data:{}'.format(data))
    return {'prediction':None}

@app.errorhandler(500)
def server_error(e):
    logging.exception('An error occured during a request')
    return """
    发生致命错误：<pre>{}</pre>
    详细堆栈路径参见日志
    """.format(e), 500

if __name__ == '__main__':
    app.run(host='127.0.0.1', port=8080, debug=True)
```

`predict`方法接收 JSON 格式的 POST 请求，其中附带原始输入数据。下面看一下机器学习代码

- 机器学习代码

API 应该能从调教好的模型处获取预测结果。一种办法是将模型封存入库并迁移到 Google App Engine 上。但这和我们想创建一个自动化流程的目标相悖，理想状态下我们期望对于每个部署于项目之上的新模型都能保有相同的 API 接口逻辑。

[模型部署中的接口设计](https://libertydream.github.io/2020/03/01/模型部署的软件接口/)一文的主要教训在于提早设计好优良接口能为之后节省时间。由此，可以使用文中定义接口来降低我们的部署工作量。其中，方法 `load_model` 能从远程文件存储系统取回序列化模型并将其加载入内存。通过 `@app.before_first_request` 方法的装饰，可以确保方法会在首次请求前被调用，这样模型就被“嵌入”到服务当中：

```python
model = None

@app.before_first_request
def load_model():
    global model
    model_path = os.environ['REMOTE_MODEL_PATH']
    loginfo('加载模型：{}'.format(model_path))
    model = Model.from_remote(model_path)
```

通过对环境变量 `REMOTE_MODEL_PATH` 的设置，可以借助远程文件路径指定哪个模型要被加载到内存中。而借助环境变量使方法参数化可以防止将具体模型的任意信息应编码到 API 中，这样整个项目就可以用相同的脚本部署在线模型了，这一通行方案促进了自动化、可复用的机器学习部署实现。