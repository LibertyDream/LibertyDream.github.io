---
layout:		post
title:  	部署批量推断模式
subtitle:   模型部署系列之三
date:       2020-03-08
author:     一轩明月
header-img: img/post-bg-os-metro.jpg
catalog: 	 true
tags:
    - opinions
---

[上一篇](https://libertydream.github.io/2020/03/01/模型部署的软件接口/)部署文章中我们设计了软件接口来降低模型上线难度。本文会探讨如何用这些接口和工作调度机制部署模型，实现批量推断。批量推断多用于周期性执行计划方案的场景中，每次对一批样本进行估计。

我们还会讲到什么时候合适，什么时候又不适合将模型设置成批量推断形态，如何使用 Python 和 cron 实现批量推断以及怎么用产品级工作流管理工具完成同样的任务。

## 什么时候用批量推断

本系列[第一篇文章](https://libertydream.github.io/2020/02/23/模型部署到底是在说什么/)中提到了若干终端用户与模型预测结果交互的场景。其中有一例到要搭建一个潜在顾客评分模型，一些懂技术的分析师们会使用模型结果来发掘用户。他们会使用 SQL 在关系数据库中查询数据，希望每天早上都能看到根据前日得分计算出的新潜在客户分数。

这就是一个典型的批量推断场景。其一，每次计算结果都是针对一批样本，也就是前一天生成的客户评分，其二，每天都要重新计算一次评分结果。所以此时部署模型意味着每天都要执行某个固定的批处理任务。大体流程是检索新潜在客户得分，反序列化训练好的模型，生成新得分，再将预测结果存储到分析师们使用的数据库内。

文章中的另一个场景是电商公司给他的客户们发电子邮件推荐产品。这些营销邮件在当地时间每周一下午和每周五早上发送到用户端，每封邮件有五个推荐项。

这同样是适用批量推断的场景。因为推荐结果是针对所有消费者进行计算的，所有现存用户都是样本集的一员。可以一周执行两次计算任务，一次是在周一下午发送邮件之前，另一次则是周五清晨邮件送出之前。鉴于邮件都是根据当地时间来判断发送时机的，只要确保运算任务在编辑邮件前能够完成即可。

诚然，构建成批样本的方式多种多样。上面的例子我们是通过时区和用户群来分隔一批批的样本的。如果你所在团队的任务是在基于用户群构建多个协同过滤模型这会更有价值。

什么时候执行批量推断要视情况而定。除了一周两次生成预测结果，一般我们会一周跑一次批量推断模型，并将 10 个推荐结果分到两封电子邮件当中。这里做出的牺牲是对于那些十分活跃的购物者来讲邮件所推荐的内容可能会显得有些过时。

从上面的例子可以看出，如果是基于给定样本异步获取计算结果，特别是计算任务彼此间隔一小时以上时，批量推断模式无疑是模型部署的最佳选择

## 不适合批量推断的场景

还是以前面推荐模型的场景为例，想象一个不适于批量推断的场景。比如说电商公司希望无论是网页端还是移动端的用户都能看到产品推荐信息。同时，产品团队想在双端不同位置加上推荐内容，推送什么商品取决于用户近期行为，如浏览的页面、搜索的关键词等。这些需求无疑限制了我们结果计算的具体实现方式，进而影响到我们的部署过程。

首先，推荐内容要能被不同的客户端获取。这意味着我们的部署过程不能和网页端或移动端中的任意一个进行绑定。和其中一方捆绑在一起意味着其他客户端的用户就看不到推荐信息了。[重新训练](https://mlinproduction.com/model-retraining/)好模型以后，怎么更新模型也会是个问题。

其次，推荐内容要用到用户最新的行为信息。这杜绝了我们采用周期性事务，将预测结果缓存备用的推荐方式。对此，一个方案是提高预训练频率，比方说每小时而非每天进行一次训练，但这还是会漏掉最近一小时内活跃用户的活动信息。而且这是在假定每次计算都会成功的前提下，可在实际中绝无可能。执行过批处理任务的人都知道任务失败才是常态。

最后，该推荐场景下对预测延迟的要求也很高。无论是移动端应用还是网页界面都要保证用户能看到推荐的产品，所以预测结果必须在亚秒级时间范围内计算出来，不然页面加载速度过慢会损害用户体验。对延迟的苛刻限制通常是模型不能采取批量推断方案的首要原因。

由此可见，（近乎）实时推荐的场景下不适于批量推断。

## 批量推断的实现

最简单的批量推断实现包含两个部分。一个是应用程序，另一个是编排计划和调用模型的组件。这里我们会用伪代码实现一版简单的批量推断，用 Python 脚本实现应用逻辑，用 [cron](https://en.wikipedia.org/wiki/Cron) 作工作调度器。新建 run_batch_inference.py 实现推断逻辑

```python
import argparse
import logging

logging.basicConfig(level=logging.INFO)

def run_batch_inference(remote_model_path):
    '''
    计算并存储预测结果 \
    
    参数:\
    --------
    remote_model_path:序列化模型在远端的访问路径
    '''
    logging.info('Running batch inference')
    raw_data = get_raw_inference_data()
    
    logging.info('Retrieve serialized model from {}.'.format(remote_model_path))
    model = Model.from_remote(remote_model_path)
    X = model.preprocess(raw_data)
    predictions = model.predict_batch(X)
    
    logging.info('Writing predictions to database')
    write_to_db(raw_data, predictions)
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description = 'Retrieve arguments for batch inference')
    parser.add_argument('--remote_model_path', type=str, required=True, help='Remote path to serialized model.')
    args = parser.parse_args()
    run_batch_inference(remote_model_path=args.remote_model_path)
```

这段伪代码基本包含了实现批量推断所需的全部内容。

批量推断的具体实现被封装进 `run_batch_inference()` 方法。首先是要获取预测用的原始数据，通常是从数据库里拉取并按需参数化。比如，在潜在客户评分模型中该方法可能会接收一个日期，返回当日所有的预测评分。而在推荐系统的例子中，`get_raw_inference_data`可能是要传入时区，按时区获取用户信息。

接着要把已经上线的模型加载入内存。这要怎么做呢？一个选择是写一个程序负责将调教好的模型持久化到分布式文件系统上，比如 S3。而在[上一篇](https://libertydream.github.io/2020/03/01/模型部署的软件接口/)部署系列文章中，有讲到要怎么用 `Model` 类定义接口来支撑 ML 工作流程。其中一个方法 `to_remote()`，会将训练好的模型序列化并上传至 S3 或者谷歌云这样的远程文件系统中去，然后返回序列化模型的访问路径。将该路径传入 run_batch_inference.py 模块就能加载模型到内存了。

接下来的两行处理原始数据和生成预测结果，一样的，这里还是依靠既有接口方法来完成这两项任务。最后 `write_to_db()` 方法负责将结果写入数据库。这里`raw_data` 和 `predictions` 都被传入函数是考虑到原始数据里包含着 ID 字段这样的必要元信息。这里的 ID 既可以是我们潜在客户样本的标号，也可能是待推荐商品面向的若干用户 ID。

## 批量推断计划的定制与执行

有了执行批量推断的 Python 代码，接着就要考虑代码周期性执行的问题。对此有许多方案可选，比如使用工作调度器，Kubernetes CronJobs 等等。这里用 [cron](https://en.wikipedia.org/wiki/Cron) 可能是安排推断计划最简单的方式。Cron 是按时间规划任务的调度器，可以按固定时间间隔周期性的执行任务。

不过虽然用 cron 编排计划很方便，但其本身并不支持失败任务自动重启，发送通知等功能。稍后我们会介绍一些提供这些功能的 cron 替代品。

使用 cron 首先要确定批量推断的频率与时机。然后将计划方案写成 [cron 表达式](https://crontab.guru/)，再把表达式和运行指令添加到  [crontab](https://mlinproduction.com/batch-inference-for-machine-learning-deployment-deployment-series-03/Overview) 文件里。比方说我们打算每天早上 3 点执行一次潜在客户推断任务，对应 cron 表达式为 `0 3 * * *`。所以追加到 crontab 文件内的命令行会是这样：

```
0 3 * * * python3 run_batch_inference.py
```

而要每周一、周五早上 5 点获取推荐产品，需要向 crontab 追加下面这样一条指令

```
 0 5 * * 1,5 python3 run_batch_inference.py
```

这样就完成了批量推断模型的部署。 run_batch_inferency.py 脚本会准时运行并输出预测结果，以此向组织贡献商业价值。

## 批量推断任务调度工具

通过 Python 脚本和 cron 我们简单实现了批量推断与部署过程，事务会定期触发并完成批量推断，但其中还缺少像状态流程监控、自动重试和失败通知等一系列功能。对新手和小白来讲这些功能是十分必要的，如果你曾有过执行批处理任务的经历，也会理解这些额外功能的价值所在。这里介绍几个能高效、容错地部署批量推断模型的工具。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-06_prefect_logo.png)

[Prefect](https://docs.prefect.io/core/) 是一个工作流程管理系统，能托管代码并将其转换为分布式的鲁棒管道。其 UI 界面很全面，比如任务界面、远程执行集群界面和自动规划界面等。Prefect 里添加[通知和警告](https://docs.prefect.io/core/tutorials/slack-notifications.html#installation-instructions)异常简单，官方文档内还有丰富的[案例样本](https://docs.prefect.io/core/examples/)。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-06_Airflow_logo.png)

[Apache Airflow](https://airflow.apache.org/docs/stable/) 一个编程式编写、规划和监控工作流程的平台。Airflow 的特色在于提供了丰富的用户接口，方便用户可视化生产环境中的管道运行情况与监控进程，并在必要时处理故障。Airflow 于 2014 年 10 月开始为 Airbnb 使用，是一个有着庞大用户基础的成熟工具。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-06_kubernetes_logo.png)

Kubernetes Cronjobs 允许用户在 Kubernetes 集群上周期性执行批处理任务。必须要说的是 Cronjobs 并不像其他工作流管理工具一样提供了 UI 界面，但可以通过附件加上这一功能。如果你所在的组织已经在用 Kubernetes 集群并有专门的工程师团队进行维护，Cronjobs 无疑是数据科学家们的首选。

这里有一篇文章介绍了如何[使用 Cronjobs 部署机器学习模型](http://mlinproduction.com/k8s-cronjobs/)。注意默认情况下 Kubernetes 不支持多步骤工作流，但如果用的是 [Argo Workflows](https://argoproj.github.io/docs/argo/examples/README.html) 就不存在这个问题。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-06_jenkins_logo.png)

传统的自动化工具，像 [Jenkins](https://jenkins.io/)，也能完成制定批量训练、批量推断计划的任务。这些工具通常都有错误处理，通知和自动重试等功能，但这些并不是专为机器学习配置的。但如果公司里已经在使用这样的工具了，这些工具同样是不错的选择。

最后值得一提的是，现如今有大量机器学习平台级解决方案可供选择，ML 平台会提供工作流程管理并对其他机器学习核心功能提供支持。其中，开源工具可以选择 [Kubeflow](https://www.kubeflow.org/) 和 [mlflow](https://mlflow.org/)。商业方案包括 [Amazon Sagemaker](https://aws.amazon.com/sagemaker/)，[cnvrg](https://cnvrg.io/) 以及众多[其他选项](https://github.com/alirezadir/Production-Level-Deep-Learning#46-all-in-one-solutions)。