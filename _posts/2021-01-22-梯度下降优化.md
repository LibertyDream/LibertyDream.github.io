---
layout:		post
title:  	梯度下降优化算法概述
subtitle:   
date:       2021-01-22
author:     一轩明月
header-img: img/post-bg-computer-vision.jpg
catalog:    true
tags:
    - 
excerpt:    梯度下降一般是对神经网络和其他机器学习算法进行优化时的首选方式，但通常都被当作一个黑箱。本文对众多流行梯度优化算法的实际工作模式进行了研究，比如 Momentum，Adagrad，Adam 等，同时跟进了近期优化算法进展。
---

> 编译自：An overview of grandien optimization algorithms， [Sebastian Ruder](https://ruder.io/optimizing-gradient-descent/)

本文对许多时下最流行的梯度优化算法的实际工作模式进行了研究。

梯度下降是最流行的求解最优化问题的算法之一，也是目前为止优化神经网络时最常见的方式。同时，所有尖端深度学习库都有各种优化梯度下降的算法实现（比如 [caffe](http://caffe.berkeleyvision.org/tutorial/solver.html)，[keras](https://keras.io/api/optimizers/) 和 [pytorch](https://pytorch.org/docs/stable/optim.html)）。然而这些算法通常都是当作黑箱式优化器来用的，很难对它们的长处与不足给出些实用解释。

本文旨在给你一些直观感受，了解下不同梯度下降优化算法的行为作风，便于选择取用。首先我们会谈到不同的梯度下降变体，然后简单总结一下训练时要面对的挑战。接着就会介绍各种最常见的优化算法，它们对这些问题的解决思路，以及由此带来了怎样的更新规则的推导过程。我们还会简单看下在并行与分布式环境下，优化梯度下降的算法和架构。最后，文章介绍了些有助于优化梯度下降的额外策略。

梯度下降是一种最小化目标函数 $$J(\theta)$$ 的方法，目标函数对模型参数 $$\theta \in \mathbb{R}^{d}$$ 的梯度为 $$\nabla_{\theta} J(\theta)$$ ，梯度下降就是沿着与其反的方向更新参数，学习率 $$\eta$$ 定义了我们达到（局部）最小值期间的步长。换句话说，我们在目标函数创造的坡面上，顺着斜坡向下走直到达到谷底。如果你对梯度下降还不甚了解，你可以先看看这篇神经网络优化的[入门教程](https://cs231n.github.io/optimization-1/)。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-01-06_loss_function_image_tumblr.png)

### 梯度下降分类

梯度下降可分三种，区别在于计算目标函数梯度的时候要用多少数据。通过对数据量的调控，我们在参数更新准确度和更新耗时之间取得平衡。

#### 批梯度下降

普通梯度下降（vanilla gradient descent），也叫批梯度下降（batch gradient descent），在整个训练集上计算损失函数对参数 $$\theta$$ 的梯度：


$$
\theta=\theta-\eta \cdot \nabla_{\theta} J(\theta)
$$



因为仅 *一次* 更新我们就要全量计算一遍梯度，批梯度下降可能会非常慢而且也应付不了那些无法完全载入内存的数据集。批梯度下降也不支持模型 *在线* 更新，也就是说无法快速处理新样本。

写成代码的话，批梯度下降大概形式如下：

```python
for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad
```

预先定好训练几轮，先对参数向量 `params` 在整个数据集计算上损失函数梯度向量 `params_grad`。尖端深度学习库提供了自动化的微分方法，可以高效计算某些参数的梯度。如果你是自己动手求梯度，最好做下梯度检查（可参考梯度检查[小贴士](https://cs231n.github.io/neural-networks-3/)）。

然后我们就沿着梯度的反方向更新参数，更新幅度由学习率控制。对凸误差坡面，批梯度下降保证可以收敛到全局最优，如果是非凸坡面就只收敛到局部最优。

#### 随机梯度下降

相反，随机梯度下降（stochastic gradient descent，SGD）每次更新参数只在 *单个* 训练样本$$x^{(i)}$$ 和标签 $$y^{(i)}$$ 上进行：


$$
\theta=\theta-\eta \cdot \nabla_{\theta} J\left(\theta ; x^{(i)} ; y^{(i)}\right)
$$



批梯度下降在大数据集上会做很多冗余计算，每次更新参数的时候会重复计算相似样本的梯度。SGD 通过每次只做一次更新的方式消除了冗余，因此通常速度会快很多而且可以用在线上。SGD 更新频繁且方差大，导致目标函数会剧烈波动。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-01-06_sgd_fluctuation.png)

*图 1  SGD 波动情况*

当批梯度下降收敛到盆地最小值，SGD 的波动一方面有机会跳到新的、可能更好的局部最优。另一方面，这也导致收敛到精确最小值的过程复杂化，因为 SGD 总是调控过头。但已经证实，如果我们缓慢降低学习率，SGD 的收敛表现和批梯度下降一致，对凸优化和非凸优化几乎可以准确收敛到全局最小和局部最小值。

代码部分就是简单在训练样本上加了层循环，并对每个样本计算梯度。注意这里每轮训练都对数据做了混淆，原因见[下文](#混淆与课程学习)。

```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad
```

#### 最小批梯度下降

最小批梯度下降汲取了两种方法的优点，每次在 $$n$$ 个训练样本构成的小批数据上更新：


$$
\theta=\theta-\eta \cdot \nabla_{\theta} J\left(\theta ; x^{(i: i+n)} ; y^{(i: i+n)}\right)
$$



通过这种方式，a）降低了参数更新的方差，使收敛更加稳定；b）可以利用尖端深度学习库中常见的、高度优化过的矩阵最优化技术，高效计算小批数据的梯度。一般最小批的大小范围在 50 到 256 之间，但还是要看具体的应用情况来定。最小批梯度下降是训练神经网络时候的惯常选择，而用最小批的时候一般也会用术语 SGD 进行指代。

注意，为简单起见本文剩余部分讲到 SGD 变动的时候，会略去参数 $$x^{(i: i+n)}$$，$$y^{(i: i+n)}$$。

代码部分，相较于在样本集上迭代，现在是在大小为 50 的批数据上训练：

```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad
```

### 问题与挑战

普通最小批梯度下降并不保证会很好的收敛，还多了些有待解决的问题：

- 选择合适的学习率比较难。太小收敛会慢的让人发疯，太大会妨碍收敛，使损失函数在最小值附近波动甚至导致发散
- [学习率进程表](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586)尝试通过退火方式在训练时调整学习率，即按照预先定好的方案降低学习率，或是在每轮训练间的目标变动幅度低于某个阈值时调整学习率。但这些方案和阈值都得提前定好，自然也就无法适应数据集特征[$$^{[1]}$$](https://doi.org/10.1109/NNSP.1992.253713)。
- 此外，所有参数更新用的都是一样的学习率。如果数据稀疏或特征间频数差异很大，我们可能并不想对他们进行同样程度的更新，而是对很少出现的特征采用更大的学习率。
- 最小化神经网络的高度非凸误差函数的时候，常见的另一个重大挑战是避免陷入很多并非最优的局部最小值中。[Dauphin 等，2014](https://arxiv.org/abs/1406.2572)指出带来难度的实际不是局部最小值，而是鞍点，也就是那些一面上坡一面却是下坡的点。这些鞍点一般周围都是误差相同的高原，使得 SGD 想要逃离异常困难，因为各个维度上的梯度都近乎为 0。

### 梯度下降优化算法

下面我们就提纲挈领的看下深度学习社区里为应对上述挑战而广泛采用的一些算法。这里我们不会讲实践上对高维数据集计算不可行的算法，比如像[牛顿法](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization)这类二阶方法。

#### Momentum

[Sutton，R.S. 1986](http://incompleteideas.net/papers/sutton-86.pdf) 指出 SGD 很难穿越峡谷，也就是那些表面在一个维度上的弯曲陡峭程度远超另一维的区域，这些区域经常出现在局部最优解附近。这时 SGD 会在峡谷斜坡间震荡，同时犹犹豫豫的朝着底部的局部最优前进，如图 2 所示。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-01-06_sgd_without_momentum.gif)

*图 2  不带动量的 SGD*

[Qian, N. 1999](https://doi.org/10.1016/S0893-6080(98)00116-6) 提出的动量法（Momentum）可以帮助 SGD 加速在相关方向上前进，同时抑制震荡，如图 3 所示。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-01-06_sgd_with_momentum.gif)

*图 3  带动量的 SGD*

具体做法是给当前更新向量加上一个比例因子为 $$\gamma$$ 的过往更新向量


$$
\begin{aligned}
v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\
\theta &=\theta-v_{t}
\end{aligned}
$$


注意：有些实现会交换方程中的符号。动量项 $$\gamma$$ 通常会设为 0.9 或一个类似值。

实际上，使用动量的时候我们是推一个球下坡，向下滚的时候小球会积聚动量，滚的越来越快（直到达到终极速度，如果有空气阻力的话，即 $$\gamma<1$$）。参数更新是一回事：梯度指向相同的维度上动量增长，梯度变向的维度上动量减小。结果就是收敛更快，震荡更小。

#### Nesterov 梯度加速

但对一个往坡下滚的小球来说，盲目地选择斜坡肯定是很难令人满意。我们希望球能聪明点，对它在往哪走有个概念，这样在斜坡坡度升高前就能提前减速。

[Nesterov, Y. 1983](http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf) 提出的 Nesterov 梯度加速（NAG）就是这样一种赋予动量项先见之明的方法。已知我们会用动量 $$\gamma v_{t-1}$$ 来挪动参数 $$\theta$$，所以算下 $$\theta-\gamma v_{t-1}$$ 就能大概知道参数的下一个位置（更新全程不见梯度），粗略了解参数会变成怎样。不用当前参数 $$\theta$$ 计算梯度而是用参数未来的大概位置来计算，我们就有了很强的预见能力：


$$
\begin{aligned}
v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right) \\
\theta &=\theta-v_{t}
\end{aligned}
$$


还是一样，将 $$\gamma$$ 设为 0.9 左右的值。Momentum 方法会先计算当前梯度（图 4 中的小蓝色向量）然后跨一大步迈向累积更新梯度的方向（大蓝色向量），而 NAG 会先往之前累积梯度的方向迈一大步（棕色向量），算下梯度然后做些修正（红色向量），二者结合实现完整的 NAG 更新（绿色向量）。这种有预见性的更新可以防止我们走得太快，加快了响应速度，[Bengio 等人，2012](https://arxiv.org/abs/1212.0901) 证实该方法可以显著增强 RNN 在许多任务上的性能。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-01-06_nesterov_update_vector.png)

*图 4  NAG 与 Momentum 更新*

这里另有一篇解释 NAG 背后思想的[文章](https://cs231n.github.io/neural-networks-3/)，而[Sutskever，2013](https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)中更是对此进行了详细论述。

既然我们可以调整我们的更新方式使其更贴合误差斜坡并逐步加速 SGD，也就想着能不能按各参数的重要度调整更新方式，分情况调大或调小更新幅度。

#### Adagrad

[Duchi 等，2011](http://jmlr.org/papers/v12/duchi11a.html) 提出的 Adagrad 就是一种这么做的基于梯度的优化算法：让学习率适应参数，频繁出现的特征的参数的更新幅度小些（即学习率更小），不那么频繁的特征更新幅度大些（即学习率更大）。正因如此，该方法非常适合处理稀疏数据。[Dean 等，2012](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf) 发现 Adagrad 巨幅改善了 SGD 鲁棒性，并将其用于训练 Google 的大规模神经网络——结合其他措施学习[识别 Youtube 视频中的猫](https://www.wired.com/2012/06/google-x-neural-network/)。此外，[Pennington 等，2014](https://doi.org/10.3115/v1/D14-1162)   有用 Adagrad 训练 GloVe 词 embedding，因为非常用词比常用词需要更大的更新幅度。

之前，我们会一次对全部参数 $$\theta$$ 进行更新，每个参数 $$\theta_{i}$$ 用的都是一样的学习率 $$\eta$$。鉴于 Adagrad 在每个时步 $$t$$ 会对每个参数 $$\theta_{i}$$ 采用不同的学习率，我们先看下每个参数的更新情况，然后看向量整体。简单起见，我们用 $$g_{t}$$ 表示时步 $$t$$ 时的梯度，$$g_{t,i}$$ 是目标函数在 $$t$$ 时刻对参数 $$\theta_{i}$$ 的偏导数：



$$
g_{t, i}=\nabla_{\theta} J\left(\theta_{t, i}\right)
$$



然后在每时步 $$t$$ 各参数 $$\theta_{i}$$ 的 SGD 更新就变成：



$$
\theta_{t+1, i}=\theta_{t, i}-\eta \cdot g_{t, i}
$$



在各自的更新中，Adagrad 会结合过去 $$\theta_{i}$$ 的梯度调整 $$\theta_{i}$$ 在时步 $$t$$ 的通用学习率 $$\eta$$：


$$
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i}
$$



这里 $$G_{t} \in \mathbb{R}^{d \times d}$$ 是一个对角矩阵，每个对角元素 $$i,i$$ 都是 $$\theta_{i}$$ 截至时步 $$t$$ 时梯度的平方和，而 $$\epsilon$$ 是防止除零的平滑项（一般量级在 $$1e-8$$）。有意思的地方在于，不取平方根算法性能会糟糕许多。

因为 $$G_t$$ 包括了所有参数 $$\theta$$ 在各自方向上过往梯度的平方和，现在就可以对 $$G_t$$ 和 $$g_t$$ 求矩阵向量积 $$\odot$$ 实现运算向量化：


$$
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}} \odot g_{t}
$$



Adagrad 一大优点在于省去了手工调整学习率的麻烦，绝大多数实现都是选了默认值 0.01 后就扔那了。

Adagrad 的主要缺陷出在分母上的梯度平方和：因为加入的每一项都是正的，累加和在训练时一直在增长。这会导致学习率萎缩并最终变得微乎其微，算法再也无法获取额外的知识。下面介绍的算法就要试图解决这一问题。

#### Adadelta

[Zeiler，2012](https://arxiv.org/abs/1212.5701) 是对 Adagrad 的拓展，希望简化一下它那迅速单边减小的学习率。相较于计算所有过往梯度的平方，Adadelta 将过往梯度的累加窗口限制为固定大小 $$w$$ 。

相比低效的存储 $$w$$ 个之前的梯度平方项，梯度和被递归地定义为所有过往梯度平方项的一个逐渐衰减的均值。在用均值 $$E\left[g^{2}\right]_{t}$$ 在时步 $$t$$ 的值只依赖于之前的均值和当前的梯度（取一定的比例 $$\gamma$$ ，类似于动量项）：


$$
E\left[g^{2}\right]_{t}=\gamma E\left[g^{2}\right]_{t-1}+(1-\gamma) g_{t}^{2}
$$



我们将 $$\gamma$$ 设为类似动量项一个值，大约 0.9 左右。为简单清楚些，重写一下普通 SGD 对参数更新向量 $$\Delta \theta_{t}$$ 的更新式：


$$
\begin{array}{l}
\Delta \theta_{t}=-\eta \cdot g_{t, i} \\
\theta_{t+1}=\theta_{t}+\Delta \theta_{t}
\end{array}
$$



前面求得 Adagrad 更新向量就变成：


$$
\Delta \theta_{t}=-\frac{\eta}{\sqrt{G_{t}+\epsilon}} \odot g_{t}
$$



现在我们把对角矩阵 $$G_t$$ 换成过往梯度平方 $$E\left[g^{2}\right]_{t}$$ 的衰减平均值：


$$
\Delta \theta_{t}=-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}
$$



因为分母就是梯度的方均根（RMS）形式，可以将其换成简写：


$$
\Delta \theta_{t}=-\frac{\eta}{R M S[g]_{t}} g_{t}
$$



作者强调该更新式中的项并不一致（SGD，Momentum 或者 Adagrad 也一样），就是说更新式应该有一样的假定项做参数。为实现这点，他们先定义了另一个指数型衰退的均值，这回不是梯度平方而是参数平方做更新：



$$
E\left[\Delta \theta^{2}\right]_{t}=\gamma E\left[\Delta \theta^{2}\right]_{t-1}+(1-\gamma) \Delta \theta_{t}^{2}
$$



参数更新的方均根误差于是就变成：



$$
R M S[\Delta \theta]_{t}=\sqrt{E\left[\Delta \theta^{2}\right]_{t}+\epsilon}
$$



因为并不清楚 $$R M S[\Delta \theta]_{t}$$ 是多少，所以我们用直到上一时步的所有参数更新值的 RMS  来当其近似值。将先前更新公式里的学习率 $$\eta$$ 换为 $$R M S[\Delta \theta]_{t-1}$$，最终得到了 Adadelta 的更新公式：


$$
\begin{aligned}
\Delta \theta_{t} &=-\frac{R M S[\Delta \theta]_{t-1}}{R M S[g]_{t}} g_{t} \\
\theta_{t+1} &=\theta_{t}+\Delta \theta_{t}
\end{aligned}
$$



有了 Adadelta 我们甚至不需要设置默认学习率了，因为它已经从更新式中消失了。

#### RMSprop

RMSprop 是一个未发表的自适应学习算法，是 Geoff Hinton 在其 [Coursera 课程](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)上提出来的。

RMSprop 和 Adadelta 是同一时间分头独立开发的两个技术，都是为了解决 Adagrad 的学习率快速衰减的问题。RMSprop 实际上和我们上面提到的 Adadelta 第一个更新向量一样：



$$
\begin{aligned}
E\left[g^{2}\right]_{t} &=0.9 E\left[g^{2}\right]_{t-1}+0.1 g_{t}^{2} \\
\theta_{t+1} &=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}
\end{aligned}
$$


RMSprop 也是把学习率除了个指数级衰减的梯度平方均值。Hinton 建议 $$\gamma$$ 取 0.9，学习率 $$\eta$$ 的默认值 0.001 比较好。

#### Adam

[Kingma 等，2015](https://arxiv.org/pdf/1412.6980) 提出的自适应矩估计（Adaptive Moment Estimation，Adam)是另一个给每个参数计算自适应学习率的方法。除了像 Adadelta 和 RMSprop 一样存了个指数衰减的过往梯度平方的均值 $$v_t$$，Adam 还维护了一个指数衰减的过往梯度的均值 $$m_t$$，类似于 Momentum。如果将动量法看作是滚球下坡，那 Adam 就是一个有摩擦的重球，也就更偏爱误差表面上平坦的最小值，这点可以参考 [Heusel 等，2017](https://arxiv.org/pdf/1706.08500) 所做的工作。我们按下列式子计算过往梯度和梯度平方的衰减均值：



$$
\begin{aligned}
m_{t} &=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\
v_{t} &=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}
\end{aligned}
$$



$$m_t$$ 和 $$v_t$$ 分别是梯度的第一矩（均值）和第二矩（无中心方差）的估计值，这也是方法名的由来。因为 $$m_t$$ 和 $$v_t$$ 是被初始化为 0 向量，Adam 作者发现他们会偏向 0，特别是开始时的几步，当学习率衰减的比较小（即 $$\beta_{1}$$ 和 $$\beta_{2}$$ 接近于 1）时也尤其明显。


$$
\begin{aligned}
\hat{m}_{t} &=\frac{m_{t}}{1-\beta_{1}^{t}} \\
\hat{v}_{t} &=\frac{v_{t}}{1-\beta_{2}^{t}}
\end{aligned}
$$



之后就和 Adadelta 与 RMSprop 类似了，把这些用在参数更新上，得到 Adam 更新式：


$$
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} \hat{m}_{t}
$$


作者给 $$\beta_1$$ 的默认值为 0.9，$$\beta_2$$ 是 0.999，$$\epsilon$$ 则是 $$10^{-8}$$。作者靠实验证明 Adam 实际表现不错，优于其他自适应学习算法。

#### AdaMax

Adam 更新式里的 $$v_{t}$$ 与过往梯度的 $$\ell_{2}$$ 范数（借 $$v_{t-1}$$ 之手）及当前梯度 $$\left|g_{t}\right|^{2}$$ 成反比地放大了梯度：


$$
v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right)\left|g_{t}\right|^{2}
$$


我们可以将其泛化到 $$\ell_{p}$$ 范数的更新上，注意 Kingma 和 Ba 也将 $$\beta_{2}$$ 参数化成了 $$\beta_{2}^{p}$$ ：


$$
v_{t}=\beta_{2}^{p} v_{t-1}+\left(1-\beta_{2}^{p}\right)\left|g_{t}\right|^{p}
$$


大 $$p$$ 值的范数数值不稳定，这也是为什么实践中 $$\ell_{1}$$ 和 $$\ell_{2}$$ 范数用的最多。但 $$\ell_{\infty}$$ 一般表现也很稳定，有鉴于此，[Kingma 等，2015](https://arxiv.org/pdf/1412.6980) 提出了 AdaMax 并显示带 $$\ell_{\infty}$$ 的 $$v_t$$ 能收敛到下列更稳定的值。为避免和 Adam 搞混，我们用 $$u_t$$ 表示无穷范数约束下的 $$v_t$$：



$$
\begin{aligned}
u_{t} &=\beta_{2}^{\infty} v_{t-1}+\left(1-\beta_{2}^{\infty}\right)\left|g_{t}\right|^{\infty} \\
&=\max \left(\beta_{2} \cdot v_{t-1},\left|g_{t}\right|\right)
\end{aligned}
$$



把这个插到 Adam 更新公式里，把 $$\sqrt{\hat{v}_{t}}+\epsilon$$ 换成 $$u_t$$ 就得到了 AdaMax 的更新公式：



$$
\theta_{t+1}=\theta_{t}-\frac{\eta}{u_{t}} \hat{m}_{t}
$$


注意，因为 $$u_t$$ 依赖于 $$\max$$ 运算，它并不像 Adam 里的 $$m_t$$ 和 $$v_t$$ 那样容易偏向 0，这也解释了我们为何不用算 $$u_t$$ 的偏差修正值。不错的默认值还是设 $$\eta=0.002$$，$$\beta_{1}=0.9$$ 而 $$\beta_{2}=0.999$$。

#### Nadam

如前文所见，Adam 可以看作是 RMSprop 和动量法的结合：RMSprop 贡献了指数衰减的过往梯度平方均值 $$v_t$$，而 Momentum 捐赠了指数衰减的过往梯度均值 $$m_{t}$$ 。我们也知道了 Nesterov 梯度加速法（NAG）要比一般动量法强。

[Dozat，2016](https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf) 设计的 Nesterov 加速自适应矩估计（Nesterov-accelerated Adaptive Moment Estimation，Nadam）就这样把 Adam 和 NAG 结合在了一起。要想将 NAG 插到 Adam 里，需要改改动量项 $$m_t$$。

先来回顾下动量法更新公式，用我们当前记号约定表示：


$$
\begin{aligned}
g_{t} &=\nabla_{\theta_{t}} J\left(\theta_{t}\right) \\
m_{t} &=\gamma m_{t-1}+\eta g_{t} \\
\theta_{t+1} &=\theta_{t}-m_{t}
\end{aligned}
$$


其中 $$J$$ 是目标函数，$$\gamma$$ 是动量衰减因子，而 $$\eta$$ 是步长。把第三个方程展开：


$$
\theta_{t+1}=\theta_{t}-\left(\gamma m_{t-1}+\eta g_{t}\right)
$$



这再次说明动量法会往之前动量向量的方向迈一步，并往当前梯度方向迈一步。

NAG 使我们能在梯度方向上更精准迈步，算梯度 _之前_ 用动量的一步更新下参数。所以我只用调整梯度 $$g_t$$ 即可得到 NAG：


$$
\begin{aligned}
g_{t} &=\nabla_{\theta_{t}} J\left(\theta_{t}-\gamma m_{t-1}\right) \\
m_{t} &=\gamma m_{t-1}+\eta g_{t} \\
\theta_{t+1} &=\theta_{t}-m_{t}
\end{aligned}
$$

Dozat 按下列方式对 NAG 进行了改造：不再两次运用动量移步——一次更新梯度 $$g_{t}$$ 而另一次更新参数 $$\theta_{t+1}$$——现在直接用前瞻动量向量更新当前参数：


$$
\begin{aligned}
g_{t} &=\nabla_{\theta_{t}} J\left(\theta_{t}\right) \\
m_{t} &=\gamma m_{t-1}+\eta g_{t} \\
\theta_{t+1} &=\theta_{t}-\left(\gamma m_{t}+\eta g_{t}\right)
\end{aligned}
$$


注意，相较于像上面动量更新展开式中那样使用先前动量向量 $$m_{t-1}$$，现在我们用的是当前动量 $$m_{t}$$ 来做前瞻估计。为了把 Nesterov 动量加到 Adam 上，我们可以由此类似地用当前动量替换先前向量。先来回忆下 Adam 更新公式（注意我们不需要修改 $$\hat{v}_{t}$$）：


$$
\begin{aligned}
m_{t} &=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\
\hat{m}_{t} &=\frac{m_{t}}{1-\beta_{1}^{t}} \\
\theta_{t+1} &=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} \hat{m}_{t}
\end{aligned}
$$


用 $$\hat{m}_{t}$$ 和 $$m_{t}$$ 的定义式展开第二个方程：


$$
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}\left(\frac{\beta_{1} m_{t-1}}{1-\beta_{1}^{t}}+\frac{\left(1-\beta_{1}\right) g_{t}}{1-\beta_{1}^{t}}\right)
$$



其中 $$\frac{\beta_{1} m_{t-1}}{1-\beta_{1}^{t}}$$ 就是上一时步动量向量的偏差修正估计值，于是我们可以将其用 $$\hat{m}_{t-1}$$ 替换；


$$
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}\left(\beta_{1} \hat{m}_{t-1}+\frac{\left(1-\beta_{1}\right) g_{t}}{1-\beta_{1}^{t}}\right)
$$


为简单起见，我们忽略了分母是 $$1-\beta_{1}^{t}$$ 而非 $$1-\beta_{1}^{t-1}$$，反正下一步我们就要换掉分母了。这个方程看起来和上面展开的动量更新式很像，现在我们可以像之前做的那样把 Nesterov 动量加上，简单地把上一时步动量的偏差修正估计 $$\hat{m}_{t-1}$$ 换成当前动量向量 $$\hat{m}_{t}$$ 的偏差修正估计，于是得到 Nadam 更新式：


$$
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}+\epsilon}}\left(\beta_{1} \hat{m}_{t}+\frac{\left(1-\beta_{1}\right) g_{t}}{1-\beta_{1}^{t}}\right)
$$



#### AMSGrad

随着自适应学习率方法成为训练神经网络的范式，人们发现有些场合下，比如目标识别（[Huang 等，2017](https://ieeexplore.ieee.org/document/8099726)）或机器翻译（[Johnson 等，2016](https://arxiv.org/pdf/1611.04558)），这些方法收敛不到最优值而且性能也比不过只是用动量的 SGD。

[Sashank 等，2018](https://arxiv.org/pdf/1904.09237) 正式提出了这一问题，并查明自适应学习率方法泛化能力差劲的原因在于过往梯度平方的指数型变动的均值。想来引入指数型均值的理由还是很充分的：防止学习率在训练过程中变得微乎其微的小，这也正是 Adagrad 算法的致命缺陷。但是这种梯度的短期记忆在其他场景下变成了阻碍。

在 Adam 收敛到次优解的场景中，人们发现一些最小批提供了重要、富含信息量的梯度，但因为这些最小批只出现了少有的几次，指数型均值消解了他们的影响，导致收敛的很差。作者给了一个简单凸优化问题的例子，从中也能看到一样的 Adam 行径。

为了修正这一行为，作者提出了一个新的名为 AMSGrad 的算法，使用过往梯度平方项的最大值 $$v_{t}$$ 而不是指数型均值来更新参数。$$v_{t}$$ 的定义和上面 Adam 一样：


$$
v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}
$$


相较于直接使用 $$v_{t}$$（或者偏差修正版 $$\hat{v}_{t}$$），现在如果上一步的 $$v_{t-1}$$ 比当前的大就用上一步的：


$$
\hat{v}_{t}=\max \left(\hat{v}_{t-1}, v_{t}\right)
$$



这样 AMSGrad 就有了非持续增长的步长，避免了 Adam 所具有的问题。简单起见，作者还删掉了我们在 Adam 中提到的纠偏步骤。完整且不带纠偏估计的 AMSGrad 形式如下：


$$
\begin{aligned}
m_{t} &=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\
v_{t} &=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \\
\hat{v}_{t} &=\max \left(\hat{v}_{t-1}, v_{t}\right) \\
\theta_{t+1} &=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} m_{t}
\end{aligned}
$$



作者发现相比 Adam，AMSGrad 在小数据集和 CIFAR-10 上性能有所提升。但[其他实验](https://fdlm.github.io/post/amsgrad/)上和 Adam 水平相近甚至更糟，AMSGrad 能否一直压过 Adam 还有待现实检验。

#### AdamW

权重衰减是训练神经网络时用到的一种技术，目的在于保持权重值够小。直觉上大权重会导致过拟合，权重想大理由要充分。技术实现是通过在损失函数中加一个权重值的函数，这样大权重会显著增大全局损失。最流行的权重衰减形式莫过于 L2 正则化了，它对权重值的平方进行惩罚，能方便的同时处理正、负权重，可微性良好。[Ilya 等，2017](https://openreview.net/forum?id=rk6qdGgCZ) 设计的 AdamW 修改了 Adam 里权重衰减正则化的经典实现，将权重衰减和梯度更新解耦。尤其是，Adam 里 L2 正则化通常是带着下列调整实现的，其中 $$w_t$$ 是 $$t$$ 时权重衰减比率：


$$
g_{t}=\nabla f\left(\theta_{t}\right)+w_{t} \theta_{t}
$$



而 AdamW 则转头把权重衰减项挪到了梯度更新式里：



$$
\theta_{t+1, i}=\theta_{t, i}-\eta\left(\frac{1}{\sqrt{\varepsilon_{t+1, i}^{g o g}+\varepsilon}} \cdot \mathcal{E}_{t+1, i}^{g}+w_{t, i} \theta_{t, i}\right), \quad \forall t
$$



最终在实践中该方法取得了一定的效果，有被机器学习社区的一些人采纳。就这么一点小细节可以对性能产生那么大的影响，很有趣不是吗？

#### QHAdam

[Ma 等，2018](https://arxiv.org/pdf/1810.06801) 设计的 QHAdam (Quasi-Hyperbolic Adam) 是对 QHM（Quasi-Hyperbolic Momentum）在自适应学习率上的一种扩展。QHM 下面会讲，其目的在于用拟双曲动量替换 Adam 中的两个动量估计。即是说，QHAdam 在更新权重时，将动量与当前梯度解耦，将梯度平方均值与当前梯度平方解耦。换句话说，它就是个加权动量均值配普通的 SGD，用当前折扣因子 $$\nu_{1}$$ 决定当前梯度权重，除以一个梯度平方均值和当前梯度平方的加权平均值，用当前折扣因子 $$\nu_{2}$$ 决定当前梯度平方的权重。该拟双曲表达式能复原 Adam 和 NAdam 等。注意式子里 $$\mathcal{E}_{t+1}^{g}$$ 和 $$\mathcal{E}_{t+1}^{g \circ g}$$ 的定义与 Adam 一致。如果有点困扰，看一眼 QHAdam 算法下的更新公式：


$$
\theta_{t+1, i}=\theta_{t, i}-\eta\left[\frac{\left(1-\nu_{1}\right) \cdot g_{t}+\nu_{1} \cdot \hat{\mathcal{E}}_{t+1}^{g}}{\sqrt{\left(1-\nu_{2}\right) g_{t}^{2}+\nu_{2} \cdot \hat{\mathcal{E}}_{t+1}^{g \circ g}}+\epsilon}\right], \quad \forall t
$$


QHAdam 的超参选择看情况，一开始可以令 $$\nu_{2}=1$$ 而 $$\beta_{2}$$ 和 Adam 里一样。

#### YellowFin

[Zhang 等，2017](https://arxiv.org/pdf/1706.03471)提出的 YellowFin 是一个学习率与动量调节器，着眼于鲁棒性和二次目标分析。对二次目标，优化器会调节学习率和动量以保持超参处于某一区域内，域内收敛速度是个和根动量一样的常数。该想法被实验性地移植到非凸目标上，每轮迭代时，YellowFin 会优化超参数使局部二次优化的结果最小。感兴趣的读者可以看论文了解其中细节。这里要讲的是 _未经_ 调整的 YellowFin 就可以和 Adam 和动量优化器一较高下。

#### AggMo

选动量参数是个大问题。按常规选择 0，0.9 或 0.99 赋给动量参数 $$\beta$$，取得的效果差异明显，而且也很难提前预测性能效果。如果任务的 $$\beta$$ 选的太大了，会出现震荡并减缓收敛速度。另一方面，如果任务的 $$\beta$$ 选的太小了，训练时间会很长，性能也会掉。[Lucas 等，2018](https://arxiv.org/pdf/1804.00325) 设计的累积动量（Aggregated momentum，AggMo）是种自适应动量算法，通过对多个缓存动量的线性组合解决了这一问题。它会维护 $$K$$ 个动量，每一个都带着不同的比例因子，取它们的平均值进行更新。$$K$$ 个动量对应比例因子 $$\beta \in \mathbb{R}^{K}$$，整体更新公式为：


$$
\begin{array}{l}
\left(\mathcal{E}_{t+1}^{g}\right)^{(i)}=\beta^{(i)} \cdot\left(\mathcal{E}_{t}^{g}\right)^{(i)}+g_{t}, \quad \forall i \in[1, K] \\
\theta_{t+1, i}=\theta_{t, i}-\eta\left[\frac{1}{K} \cdot \sum_{i=1}^{K}\left(\mathcal{E}_{t+1}^{g}\right)^{(i)}\right], \quad \forall t
\end{array}
$$



实验显示在玩具级样例上该方法做到了保证加速的同时减少了震荡。

#### QHM

[Ma J.等，2018](https://arxiv.org/pdf/1810.06801) 提出了另一个自适应动量算法——拟双曲动量（Quasi-Hyperbolic Momentum，QHM），该方法将更新权重时的动量项与当前梯度解耦。换句话说，它是动量和普通 SGD 的加权平均，当前梯度权重由时下比例因子 $$\nu$$ 决定。注意该表达式能涵盖 [ Nesterov Momentum](http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf)，[Nesterov 合成变体](https://arxiv.org/pdf/1408.3595)，[accSGD](https://arxiv.org/pdf/1704.08227) 和其他方法。QHM 采用更新公式：


$$
\begin{array}{c}
\mathcal{E}_{t+1}^{g}=\beta \cdot \mathcal{E}_{t}^{g}+(1-\beta) \cdot g_{t} \\
\theta_{t+1, i}=\theta_{t, i}-\eta\left[(1-\nu) \cdot g_{t}+\nu \cdot \mathcal{E}_{t+1}^{g}\right], \quad \forall t
\end{array}
$$


参数选择上，作者推荐从 $$\nu=0.7$$，$$\beta=0.999$$ 开始不错。

#### Demon

[Chen 等，2019](https://arxiv.org/pdf/1910.04952) 提出了名为衰减动量（Decaying Momentum，Demon）的方法，这是个动量衰减规则，受启发于学习率线性衰减模型可以降低梯度对当前和未来更新的影响。通过动量参数衰减，梯度对一切未来更新的整体贡献也会减少。梯度 $$g_{t}$$ 对未来更新贡献的所有“能量”为 $$\eta \sum_{i} \beta^{i}$$ ，求和值是几何增长的，$$\sum_{i=1}^{\infty} \beta^{i}=\beta \sum_{i=0}^{\infty} \beta^{i}=\beta /(1-\beta)$$。Demon 算法中该值会衰减，假如初始 $$\beta$$ 值为 $$\beta_{\text {init}}$$，总共 $$T$$ 步，那在当前 $$t$$ 步时 $$\beta_{t}$$ 的衰减过程如下：


$$
\frac{\beta_{t}} {1-\beta_{t}}=\frac{(1-\frac{t}{T}) \beta_{\text {init }}}{1-\beta_{\text {init}}}
$$



其中 $$1-\frac{t}{T}$$ 表示迭代残留比例。注意 Demon 一般不需要超参数调试，因为它通常在 $$T$$ 步会衰减到 0 或一个小负值。延缓衰减可以改善性能，Demon 方法可以用到任何带动量参数的梯度下降方法中。

#### 动态演示

下面是两幅 [Alec Radford](https://twitter.com/alecrad) 绘制的动态图，可以直观感受一下上面多数方法的优化行为。

<img src="https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-01-06_contours_evaluation_optimizers.gif" style="zoom:50%;" />

*图 5  损失轮廓线上的 SGD 优化*

上图显示的是损失表面（[Beale 函数](https://www.sfu.ca/~ssurjano/beale.html)）上随时间推移不同方法的行为表现。可以看到 Adagrad，Adadelta 和 RMSprop 几乎是立刻转向正确方向并快速在相似位置收敛，而 Momentum 和 NAG 则偏离了轨道，感觉像是要往坡下滚的球。但由于有前瞻获得的增量反馈， NAG 很快进行了修正并达到最小值。

下面的图 6 则展示了算法在鞍点上的表现，即一个维度是正斜坡（率），另有维度是负斜坡（率）的点。上面我们已经说了这对 SGD 是个麻烦，可以看到 SGD，Momentum 和 NAG 很难打破均衡，尽管后两者最终还是设法离开了鞍点。而 Adagrad，RMSprop 和 Adadelta 很快就走向了负斜坡（率）。

<img src="https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-01-06_saddle_point_evaluation_optimizers.gif" style="zoom:50%;" />

*图 6  鞍点上的 SGD 优化*

如我们所见，自适应学习率方法，即 Adagrad，Adadelta，RMSprop 和 Adam 是最适合且能得到最佳收敛结果的方法。

> 如果对这些或其他最优化方法的可视化感兴趣，可以看这篇[教程](http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/)

#### 选哪个优化器

所以该选哪个优化器呢？如果输入数据稀疏，那选一个自适应学习率方法大概率效果不错。这样做的额外一点好处在于不需要调学习率，用默认值就能取得最佳效果。

总的来讲，RMSprop 是对 Adagrad 的扩展，解决了后者学习率快速消失的问题。Adadelta 也一样，只是 Adadelta 是在分母更新中用参数的 RMS 进行更新。最后 Adam 给 RMSprop 加上了动量和偏差修正。至此，RMSprop，Adadelta 和 Adam 是在相似环境下差别不大的算法。[Kingma 等，2015](https://arxiv.org/pdf/1412.6980) 指出随着梯度变得愈加稀疏，偏差修正能帮 Adam 在抵达优化终点时略胜 RMSprop 一头。

换个角度讲，如果并没有给超参调试留下预算，从性能上看 YellowFin 是个不错的选择，它自己会调整学习率和动量参数。如果留有超参调试的预算，可以考虑自适应学习率算法，也就是最近的 AdamW 或 QHAdam，配上 Demon 会有显著提升。

最后，如果获取最佳性能是你唯一关心的，那带 Demon 的 Momentum 可能仍值得考虑，也可以试一下 QHM 或 AggMo。

### 并行和分布式 SGD

考虑到无处不在的大数据解决方案，加上易得的廉价集群，将 SGD 做分布式处理来进行加速就成了一个在自然不过的选择。

SGD 本身实际上是顺序性的：一步接着一步，不断向最小值进发。直接用它收敛效果会很好但在大规模数据集上可能慢的离谱。相反，异步 SGD 更快，但节点间的沟通可能导致收敛效果不佳。此外，我们也可以在一台机器上使 SGD 并行化，而不需要大规模计算集群。下面介绍的是一些用于优化并行与分布式 SGD 的算法和框架。

#### Hogwild！

[Niu 等，2011](https://papers.nips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf) 设计了一套名为 Hogwild！的更新方案，可以在 CPU 上实现 SGD 更新并行化。处理器可以访问共享内存，不用给参数上锁。该方法只在输入数据稀疏时有效，因为每次更新只修改一部分的参数。作者证明稀疏输入的条件下，Hogwild！更新方案达到几乎是最好的收敛速度，因为处理器不太可能覆盖掉有用信息。

#### Downpour SGD

骤雨式 SGD（Downpour SGD）是 SGD 的异步版，是 [Dean 等，2012](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf) 在谷歌内部 DistBelief 框架（Tensorflow 前身）上采用的一套方案。它会复制多份模型一起跑，每个模型的数据都是训练数据的一个子集。这些模型将更新结果传给一个参数服务器，服务器分散在许多机器上。每个机器负责存储和更新一部分模型参数，但因为复制品之间并不会通过共享参数或更新机制互相交流，它们的参数一直会有出现分歧的风险，从而阻碍收敛。

#### SGD 的延迟忍耐算法

[McMahan 和 Streeter，2014](http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf) 将 AdaGrad 扩展到并行计算环境下，其采用的延迟忍耐算法不仅适用于过往梯度，还可用于延迟更新。实践证实该方法确实有效。

#### TensorFlow

[TensorFlow](https://www.tensorflow.org/) 是谷歌的开源框架，能实现和部署大规模机器学习模型。它汲取了 DistBelief 的经验，并已经在内部用于大规模移动设备和分布式系统上的计算。为了分布式执行，计算图被分成每个设备上的子图，并通过 Send/Receive 结点对进行通信。

#### 弹性均衡 SGD

[Zhang 等，2015](https://arxiv.org/abs/1412.6651) 提出了弹性均衡 SGD（Elastic Averaging SGD，EASGD） 方案，靠着一股弹力将异步 SGD 结点上的参数连起来，也就是通过参数服务器存一个中心变量。这可以让局部变量基于中心变量进行更大幅度的波动，理论上可以在参数空间进行更充分的探索。作者在实验上证明该方法所带来的探索能力可以找到新的局部最优，进而提高性能。

### 优化 SGD 的其他策略

#### 混淆与课程学习

#### 批归一化

#### 早停

#### 梯度噪声

$$N\left(0, \sigma_{t}^{2}\right)$$


$$
g_{t, i}=g_{t, i}+N\left(0, \sigma_{t}^{2}\right)
$$

$$
\sigma_{t}^{2}=\frac{\eta}{(1+t)^{\gamma}}
$$

### 总结

