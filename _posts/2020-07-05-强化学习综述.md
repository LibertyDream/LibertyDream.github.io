---
layout:		post
title:  	强化学习综述
subtitle:   
date:       2020-07-05
author:     一轩明月
header-img: img/post-bg-unix-linux.jpg
catalog:    true
tags:
    - math
    - Reinforcement Learning
excerpt:    本文对强化学习进行了简短回顾，从基础概念到经典算法。希望本番回顾能帮助初入行的新人免于迷失在专有名词和术语当中。
---

> 编译自：A (Long) Peek into Reinforcement Learning，[Lilian Weng](https://lilianweng.github.io/lil-log/)

近些年人工智能（ Artificial Intelligence，AI）可谓大红大紫。AlphaGo 打败了最优秀的人类围棋选手，很快延拓算法 AlphaGo Zero 就打了 AlphaGo 一个 100：0，而且没有采取借人类经验进行监督学习的策略。在 DOTA 1v1 对抗赛中职业选手输给了 OpenAI 开发的机器人。知道这些以后，很难不对那些隐藏在背后的神奇算法感到好奇——强化学习（Reinforcement Learning，RL）。本文是对该领域的简短回顾，首先介绍几个基本概念，然后讲几个解决 RL 问题的经典方法，希望对入门新手有所帮助。

### 什么是强化学习

假设现在有个代理人，他不了解所处环境，但能从与环境的交互过程中得到一些回报，而他要做的就是想办法使累计收益最大。现实中的场景可能是一个机器人在打游戏希望获得高分，或着是试图通过物理手段解决现实任务，当然场景也不会仅限于此。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_RL_illustration.png)

*图 1  代理人和环境交互，努力采取明智行动使累计回报最大*

RL 的目标是期望代理人能从试探性实验和得到的相对简单的反馈中学到好策略。有了最优策略，代理人就能主动适应环境，保证未来回报最大。

#### 关键概念

下面正式介绍一下 RL 中的关键概念。

代理人的所有行为都处于一个**环境（environment）**之中。环境会对对特定行为作出怎样的反应是由一个**模型（model）**决定的，我们不一定能了解该模型。代理人能停留于某个环境**状态（state）** ($$s \in \mathcal{S}$$)  上，并选择一种行动 ($$a \in \mathcal{A}$$) 从一个状态转移到另一个状态。具体转移到哪个状态有状态间的转移概率 ($$P$$) 决定。一旦采取行动，环境会给予**回报（reward）**($$r \in \mathcal{R}$$) 作为反馈 。

模型决定了回报函数和转移概率。我们不一定清楚模型是怎样工作的，这就要分两种情况

- **了解模型**：信息完美，照此做规划；搞基于模型的 RL。如果充分了解环境，可以通过[动态规划（Dynamic Programming，DP）](https://en.wikipedia.org/wiki/Dynamic_programming)找到最优方案。你是否还记得算法课上“最长上升子序列”或“销售员问题”？哈哈，这并非本文关注重点。
- **不了解模型**：基于不完备的信息进行学习；搞无模型 RL 或者学个模型权当算法的一部分。下面的内容绝大部分都是模型未知的场景。

<span style="color: #e01f1f;">**想最大化总体回报**</span>，模型**策略（policy）**$$\pi(s)$$ 能告知特定状态下的最优行动。每个状态都关联着一个**价值（value）**函数 $$V(s)$$ ，预估当前状态下如果采用相应策略未来的期望收益水平。换句话说，价值函数就是在量化某个状态有多好，策略和价值函数也正是在强化学习中我们试图学习的内容。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_RL_algorithm_categorization.png)

*图 2  RL 方法总结，看是否想对价值、策略或环境建模（图片来源：[网络](https://youtu.be/2pWv7GOvuf0)）*

 $$t=1, 2, \dots, T$$.  t as $$S_t$$, $$A_t$$,  $$R_t$$, $$S_T$$:
$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$


#### -- 模型：转移和回报

 $$P$$ $$R$$.

 $$\mathbb{P}$$ 
$$
P(s', r \vert s, a)  = \mathbb{P} [S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a]
$$
$$P(s', r \vert s, a)$$:
$$
P_{ss'}^a = P(s' \vert s, a)  = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

$$
R(s, a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s' \in \mathcal{S}} P(s', r \vert s, a)
$$


#### -- 策略

-  $$\pi(s) = a$$.
- $$\pi(a \vert s) = \mathbb{P}_\pi [A=a \vert S=s]$$.

#### -- 价值函数

 $$G_t$$ 
$$
G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
 $$\gamma \in [0, 1]$$ 

$$S_t = s$$:
$$
V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]
$$

$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert S_t = s, A_t = a]
$$

$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$


#### -- 最优值和策略



$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$



$$
\pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

$$V_{\pi_{*}}(s)=V_{*}(s)$$  $$Q_{\pi_{*}}(s, a) = Q_{*}(s, a)$$.

#### 马尔可夫决策过程


$$
\mathbb{P}[ S_{t+1} \vert S_t ] = \mathbb{P} [S_{t+1} \vert S_1, \dots, S_t]
$$


![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_agent_environment_MDP.png)

 $$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$$,

- $$\mathcal{S}$$ - 
- $$\mathcal{A}$$ - 
- $$P$$ - 
- $$R$$ - 
- $$\gamma$$ - 
   $$P$$ and $$R$$.



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_mdp_example.jpg)

#### 贝尔曼方程

$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$

$$
\begin{aligned}
Q(s, a) 
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s, A_t = a] \\
&= \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{a\sim\pi} Q(S_{t+1}, a) \mid S_t = s, A_t = a]
\end{aligned}
$$



#### -- 期望方程

 $$\pi$$.

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_bellman_equation.png)
$$
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \big) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi} (s', a')
\end{aligned}
$$


#### -- 最优化方程

$$V_*$$ and $$Q_*$$ 
$$
\begin{aligned}
V_*(s) &= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\
V_*(s) &= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}
$$
 $$P_{ss'}^a$$ or $$R(s, a)$$, 

### 常见方法

#### 动态规划

#### -- 策略评估

 $$V_\pi$$  $$\pi$$:
$$
V_{t+1}(s) 
= \mathbb{E}_\pi [r + \gamma V_t(s') | S_t = s]
= \sum_a \pi(a \vert s) \sum_{s', r} P(s', r \vert s, a) (r + \gamma V_k(s'))
$$

#### -- 策略改进

 $$\pi' \geq \pi$$ 
$$
Q_\pi(s, a) 
= \mathbb{E} [R_{t+1} + \gamma V_\pi(S_{t+1}) \vert S_t=s, A_t=a]
= \sum_{s', r} P(s', r \vert s, a) (r + \gamma V_\pi(s'))
$$

#### -- 策略迭代

$$
\pi_0 \xrightarrow[]{\text{evaluation}} V_{\pi_0} \xrightarrow[]{\text{improve}}
\pi_1 \xrightarrow[]{\text{evaluation}} V_{\pi_1} \xrightarrow[]{\text{improve}}
\pi_2 \xrightarrow[]{\text{evaluation}} \dots \xrightarrow[]{\text{improve}}
\pi_* \xrightarrow[]{\text{evaluation}} V_*
$$

 $$\pi$$ $$\pi'(s) = \arg\max_{a \in \mathcal{A}} Q_\pi(s, a)$$
$$
\begin{aligned}
Q_\pi(s, \pi'(s))
&= Q_\pi(s, \arg\max_{a \in \mathcal{A}} Q_\pi(s, a)) \\
&= \max_{a \in \mathcal{A}} Q_\pi(s, a) \geq Q_\pi(s, \pi(s)) = V_\pi(s)
\end{aligned}
$$

#### 蒙特卡罗法

 $$V(s) = \mathbb{E}[ G_t \vert S_t=s]$$. $$G_t$$,  $$S_1, A_1, R_2, \dots, S_T$$ $$G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$$ 
$$
V(s) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s]}
$$
 $$\mathbb{1}[S_t = s]$$ 
$$
Q(s, a) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_MC_control.png)

1. $$\pi(s) = \arg\max_{a \in \mathcal{A}} Q(s, a)$$.
2.  $$\pi$$  [ε-greedy]({{ site.baseurl }}{% post_url 2018-01-23-the-multi-armed-bandit-problem-and-its-solutions %}#%CE%B5-greedy-algorithm) 
3.  $$q_\pi(s, a) = \frac{\sum_{t=1}^T \big( \mathbb{1}[S_t = s, A_t = a] \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1} \big)}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}$$

#### 时空差异学习

#### -- 引导式

#### -- 价值估计

 $$V(S_t)$$  $$R_{t+1} + \gamma V(S_{t+1})$$  α:
$$
\begin{aligned}
V(S_t) &\leftarrow (1- \alpha) V(S_t) + \alpha G_t \\
V(S_t) &\leftarrow V(S_t) + \alpha (G_t - V(S_t)) \\
V(S_t) &\leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
\end{aligned}
$$

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
$$

#### -- SARSA：策略性 TD 控制

 $$\dots, S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \dots$$

1.  $$S_t$$ , $$A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)$$; ε-greedy
2.  $$A_t$$,  $$R_{t+1}$$ $$S_{t+1}$$.
3. $$A_{t+1} = \arg\max_{a \in \mathcal{A}} Q(S_{t+1}, a)$$.
4. $$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $$.

#### -- Q 学习：非策略 TD 控制

1.  $$S_t$$  $$A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)$$; ε-greedy 
2.  $$A_t$$  $$R_{t+1}$$  $$S_{t+1}$$.
3.  $$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t)) $$.
4. 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_sarsa_vs_q_learning.png)

#### -- 深度 Q 网络

 $$Q_*(.)$$   $$\theta$$  $$Q(s, a; \theta)$$.

-  $$e_t = (S_t, A_t, R_t, S_{t+1})$$   $$D_t = \{ e_1, \dots, e_t \}$$. $$D_t$$ 


$$
\mathcal{L}(\theta) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \Big[ \big( r + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta) \big)^2 \Big]
$$
 $$U(D)$$   $$\theta^{-}$$

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_DQN_algorithm.png)

#### TD 和 MC 组合学习

$$G_t^{(n)}, n=1, \dots, \infty$$,

| $$n$$        | $$G_t$$                                                      | Notes         |
| ------------ | ------------------------------------------------------------ | ------------- |
| $$n=1$$      | $$G_t^{(1)} = R_{t+1} + \gamma V(S_{t+1})$$                  | TD learning   |
| $$n=2$$      | $$G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})$$ |               |
| ...          |                                                              |               |
| $$n=n$$      | $$ G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n}) $$ |               |
| ...          |                                                              |               |
| $$n=\infty$$ | $$G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-t-1} R_T + \gamma^{T-t} V(S_T) $$ | MC estimation |


$$
V(S_t) \leftarrow V(S_t) + \alpha (G_t^{(n)} - V(S_t))
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_TD_lambda.png)

 $$n$$ $$n$$  $$G_t^{(n)}$$  λ with n, $$\lambda^{n-1}$$;  (n → ∞) (1-λ),
$$
\begin{aligned}
\text{let } S &= 1 + \lambda + \lambda^2 + \dots \\
S &= 1 + \lambda(1 + \lambda + \lambda^2 + \dots) \\
S &= 1 + \lambda S \\
S &= 1 / (1-\lambda)
\end{aligned}
$$
$$G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)}$$. T

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_TD_MC_DP_backups.png)

#### 策略梯度

 $$\theta$$, $$\pi(a \vert s; \theta)$$.
$$
\mathcal{J}(\theta) = V_{\pi_\theta}(S_1) = \mathbb{E}_{\pi_\theta}[V_1]
$$
e $$S_1$$ i
$$
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) V_{\pi_\theta}(s) = \sum_{s \in \mathcal{S}} \Big( d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s, \theta) Q_\pi(s, a) \Big)
$$
 $$d_{\pi_\theta}(s)$$  $$\pi_\theta$$.

#### -- 策略梯度理论

 $$J(\theta)$$ 
$$
\frac{\partial \mathcal{J}(\theta)}{\partial \theta_k} \approx \frac{\mathcal{J}(\theta + \epsilon u_k) - \mathcal{J}(\theta)}{\epsilon}
$$

$$
\mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} [r] = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) R(s, a)
$$
 $$d(.)$$ with $$d_\pi(.)$$):
$$
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) Q_\pi(s, a) \propto \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) Q_\pi(s, a)
$$

$$
\begin{aligned}
\mathcal{J}(\theta) &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) Q_\pi(s, a) \\
\nabla \mathcal{J}(\theta) &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \nabla \pi(a \vert s; \theta) Q_\pi(s, a) \\
&= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) \frac{\nabla \pi(a \vert s; \theta)}{\pi(a \vert s; \theta)} Q_\pi(s, a) \\
& = \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) \nabla \ln \pi(a \vert s; \theta) Q_\pi(s, a) \\
& = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s; \theta) Q_\pi(s, a)]
\end{aligned}
$$

$$
\nabla \mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s, \theta) Q_\pi(s, a)]
$$

#### -- REINFORCE

 $$Q_\pi(s, a)$$, $$\theta$$.

 $$G_t$$   $$A(s, a) = Q(s, a) - V(s)$$ 

1.  θ
2.  $$S_1, A_1, R_2, S_2, A_2, \dots, S_T$$
3.  t=1, 2, ... , T:
   1. $$\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla \ln \pi(A_t \vert S_t, \theta)$$

#### -- 演员-评论员

-  $$Q(a \vert s; w)$$ or state-value $$V(s; w)$$.
- $$\pi(a \vert s; \theta)$$.

1.  $$a \sim \pi(a \vert s; \theta)$$.
2.  t = 1… T:
   1.  $$r_t  \sim R(s, a)$$ and next state $$s' \sim P(s' \vert s, a)$$.
   2.  $$a' \sim \pi(s', a'; \theta)$$.
   3. : $$\theta \leftarrow \theta + \alpha_\theta Q(s, a; w) \nabla_\theta \ln \pi(a \vert s; \theta)$$.
   4.  t: <br/>
      $$G_{t:t+1} = r_t + \gamma Q(s', a'; w) - Q(s, a; w)$$ <br/>
       <br/>
      $$w \leftarrow w + \alpha_w G_{t:t+1} \nabla_w Q(s, a; w) $$.
   5.  $$a \leftarrow a'$$ $$s \leftarrow s'$$.

$$\alpha_\theta$$ and $$\alpha_w$$ 

#### -- A3C

 $$V(s; w)$$, 

$$\mathcal{J}_v (w) = (G_t - V(s; w))^2$$ 

1.  θ' and w'.
2. $$t_\text{start}$$  $$s_t$$.
3.  ($$s_t \neq \text{TERMINAL}$$)  ($$t - t_\text{start} <= t_\text{max}$$):
   1.  $$a_t \sim \pi(a_t \vert s_t; \theta')$$  $$r_t$$  $$s_{t+1}$$.
4. $$R = \begin{cases} 
   0 & \text{if } s_t \text{ is TERMINAL} \\
   V(s_t; w') & \text{otherwise}
   \end{cases}$$.
5.  $$i = t-1, \dots, t_\text{start}$$:
   1. $$R \leftarrow r_i + \gamma R$$; here R is a MC measure of $$G_i$$.
   2. Accumulate gradients w.r.t. θ': $$d\theta \leftarrow d\theta + \nabla_{\theta'} \log \pi(a_i \vert s_i; \theta')(R - V(s_i; w'))$$;<br/>
      Accumulate gradients w.r.t. w': $$dw \leftarrow dw + \nabla_{w'} (R - V(s_i; w'))^2$$.

#### 演化策略

 $$\theta$$   $$\theta$$  $$\mu$$  $$\sigma^2I$$.  $$F(\theta)$$ 
$$
\begin{aligned}
& \nabla_\theta \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} F(\theta) \\
=& \nabla_\theta \int_\theta F(\theta) \Pr(\theta) && \text{Pr(.) is the Gaussian density function.} \\
=& \int_\theta F(\theta) \Pr(\theta) \frac{\nabla_\theta \Pr(\theta)}{\Pr(\theta)} \\
=& \int_\theta F(\theta) \Pr(\theta) \nabla_\theta \log \Pr(\theta) \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} [F(\theta) \nabla_\theta \log \Pr(\theta)] && \text{Similar to how we do policy gradient update.} \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \nabla_\theta \log \Big( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(\theta - \mu)^2}{2 \sigma^2 }} \Big) \Big] \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \nabla_\theta \Big( -\log \sqrt{2\pi\sigma^2} - \frac{(\theta - \mu)^2}{2 \sigma^2} \Big) \Big] \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \frac{\theta - \mu}{\sigma^2} \Big]
\end{aligned}
$$
 $$\theta$$  $$\theta$$  $$\theta$$ , $$\epsilon \sim N(0, I)$$  $$\theta + \epsilon \sigma \sim N(\theta, \sigma^2)$$. $$\epsilon$$ 
$$
\nabla_\theta \mathbb{E}_{\epsilon \sim N(0, I)} F(\theta + \sigma \epsilon) = \frac{1}{\sigma} \mathbb{E}_{\epsilon \sim N(0, I)} [F(\theta + \sigma \epsilon) \epsilon]
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_EA_RL_parallel.png)

### 已知问题

#### 探索利用窘境

#### 致命三件套

### 案例研究：AlphaGo Zero



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_go_config.png)


$$
(p, v) = f_\theta(s)
$$

- $$s$$: 
- $$p$$: 
- $$v$$: 

$$\pi \sim p(.)$$   $$a_t$$ $$z_t$$  $$(s_t, \pi_t, z_t)$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_alphago-zero-selfplay.png)
$$
\mathcal{L} = (z - v)^2 - \pi^\top \log p + c \| \theta \|^2
$$
 $$c$$