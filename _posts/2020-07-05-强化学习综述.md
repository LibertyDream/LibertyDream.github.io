---
layout:		post
title:  	强化学习入门
subtitle:   万字综述回顾 RL
date:       2020-07-05
author:     一轩明月
header-img: img/post-bg-unix-linux.jpg
catalog:    true
tags:
    - math
    - Reinforcement Learning
excerpt:    本文对强化学习进行了简短回顾，从基础概念到经典算法。希望能帮助初入行的新人免于迷失在专有名词和术语当中。
---

> 编译自：A (Long) Peek into Reinforcement Learning，[Lilian Weng](https://lilianweng.github.io/lil-log/)

近些年人工智能（ Artificial Intelligence，AI）可谓大红大紫。AlphaGo 打败了最优秀的人类围棋选手，很快延拓算法 AlphaGo Zero 就打了 AlphaGo 一个 100：0，而且用的不是要靠人类经验进行的监督学习。在 DOTA 1v1 对抗赛中职业选手输给了 OpenAI 开发的机器人。听到这些轶事，很难不对那些隐藏在背后的神奇算法感到好奇——强化学习（Reinforcement Learning，RL）。本文对该领域进行了简短回顾，首先介绍相关基本概念，然后讲几个解决 RL 问题的经典方法，希望对新手有所帮助。

### 什么是强化学习

假设现在有个代理人，他不了解所处环境，但能从与环境的交互中获得一些回报，而他要做的就是想办法使累计收益最大。现实中的场景可能是一个机器人希望在游戏中获得高分，或着是试图通过物理手段解决现实任务，当然场景也不会仅限于此。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_RL_illustration.png)

*图 1  代理人和环境交互，努力采取明智行动使累计回报最大*

RL 的目标是期望代理人能从试探性实验和得到的相对简单的反馈中学到好策略。有了最优策略，代理人就能主动适应环境，保证未来回报最大。

#### 核心概念

下面正式介绍一下 RL 中的核心概念。

代理人的所有行为背后都有一个**环境（environment）**。环境会对对特定行为作出怎样的反应是由一个**模型（model）**决定的，我们不一定清楚该模型。代理人能待在某个环境**状态（state）** ($$s \in \mathcal{S}$$)  上，并选择一种行动 ($$a \in \mathcal{A}$$) 从一个状态转移到另一个状态。具体转移到哪个状态有状态间的转移概率 ($$P$$) 决定。一旦采取行动，环境会给予**回报（reward）**($$r \in \mathcal{R}$$) 作为反馈 。

模型决定了回报函数和转移概率。我们不一定清楚模型是怎样工作的，所以要分两种情况：

- **了解模型**：信息完备，照此做规划；搞基于模型的 RL。如果充分了解环境，可以通过[动态规划（Dynamic Programming，DP）](https://en.wikipedia.org/wiki/Dynamic_programming)确定最优方案。你是否还记得算法课上“最长上升子序列”或“销售员问题”？哈哈，这并非本文关注的重点。
- **不了解模型**：基于不完备的信息进行学习；搞无模型 RL 或者学个模型当作算法的一部分。下面的内容绝大部分都是模型未知的场景。

<span style="color: #e01f1f;">**向着最大化总体回报的目标**</span>，模型**策略（policy）**$$\pi(s)$$ 会告诉我们特定状态下的最优行动。每个状态都伴随着一个**价值（value）**函数 $$V(s)$$ ，评估当前状态下如果采用相应策略未来的期望收益水平。换句话说，价值函数就是在量化某个状态有多好，策略和价值函数也正是在强化学习中我们试图学习的内容。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_RL_algorithm_categorization.png)

*图 2  RL 方法总结，要看是否想对价值、策略或环境建模（图片来源：[网络](https://youtu.be/2pWv7GOvuf0)）*

代理人和环境的交互涉及一系列行动和即时回报， $$t=1, 2, \dots, T$$。这一过程中，代理人逐步积累环境知识，学习最优策略，并判断下一步要采取什么行动才能学的高效。将 $$t$$  时刻的状态、行动和回报分别记为 $$S_t$$, $$A_t$$ 和 $$R_t$$ ，从而交互序列完全能用一个**事件（episode）**表示，也称试验或轨迹，序列终止于最终状态 $$S_T$$：
$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$

此外，在进一步了解不同类别 RL 算法的时候可能还会遇到些术语：

- **基于模型（model-based）**：依赖环境模型；要么模型已知，要么算法明确地学习过
- **非模型（model-free）**：学习期间不依赖模型
- **策略性（on-policy）**：使用选定策略得出的决策结果或样本训练算法
- **非策略性（off-policy）**：在转移或事件分布上训练，转移或事件由一个不同的行为策略而非选定策略生成

#### -- 模型：转移和回报

模型算是环境描述符，有模型就能学习或推测环境会怎样与代理人交互并提供反馈。模型有两个主要部分，转移概率函数 $$P$$ 和回报函数 $$R$$。

假设我们处于状态 $$s$$，决定采取行动前往状态 $$s'$$ 获取回报 $$r$$，这就是一步**转移（transition）**，可以用一个元组$$(s,a,s',r)$$表示。

采取行动并获得回报后，转移函数 $$P$$ 会将状态 $$s$$ 到 $$s'$$ 的转移概率记下来。用 $$\mathbb{P}$$ 作为“可能性”标识。
$$
P(s', r \vert s, a)  = \mathbb{P} [S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a]
$$
状态-转移函数可以定义为 $$P(s', r \vert s, a)$$:
$$
P_{ss'}^a = P(s' \vert s, a)  = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

回报函数 $$R$$ 会预估某个行动能带来的回报：
$$
R(s, a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s' \in \mathcal{S}} P(s', r \vert s, a)
$$

#### -- 策略

策略，即代理人的行为函数 $$\pi$$，能指导我们在状态 $$s$$ 下该怎么行动。它是状态 $$s$$ 和行动 $$a$$ 间的映射，不确定是确定的还是随机的：

-  确定的：$$\pi(s) = a$$.
- 随机的：$$\pi(a \vert s) = \mathbb{P}_\pi [A=a \vert S=s]$$.

#### -- 价值函数

价值函数会通过预测未来回报来评估状态“品质”，判断环境会怎样奖励状态或行为。未来的回报也称**收益(return)** ，是未来折扣奖励的总和。从 $$t$$ 时刻开始计算收益 $$G_t$$ ：
$$
G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
折扣因子 $$\gamma \in [0, 1]$$ 会对未来奖励予以惩罚，因为：

- 未来收益可能存在高度不确定性，比如股票市场
- 未来回报不会提供即时好处，作为人类，可能更喜欢今日欢愉而非推到 5 年以后
- 折扣利于数学计算，不需要步步追踪来计算收益
- 不需要担心状态转移图中的无限循环

$$t$$ 时刻状态 $$s$$ 的**状态价值（state-value）**就是期望收益，$$S_t = s$$:
$$
V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]
$$

类似的，可以定义一对状态-行动的**行动价值（action-value）**（“Q 价值”，我猜 Q 代表质量(Quality)？）：
$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert S_t = s, A_t = a]
$$

此外，因为我们尊崇目标策略 $$\pi$$，可以利用行动和 Q 值的概率分布表示状态价值：
$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$

行动价值和状态价值的差就是行动的**优势（advantage）**函数（“A 价值”）
$$
A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)
$$

#### -- 最优值和策略

最优价值函数带来的收益最大：

$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

最优策略能得到最优价值函数：

$$
\pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

当然，会有 $$V_{\pi_{*}}(s)=V_{*}(s)$$，以及 $$Q_{\pi_{*}}(s, a) = Q_{*}(s, a)$$.

#### 马尔可夫决策过程

更专业点说，几乎所有 RL 问题都可归结为**马尔可夫决策过程（Markov Decision Processes，MDPs）**。MDP 中的所有状态都具备马尔可夫性，就是说未来怎样只看当前状态，与历史无关：

$$
\mathbb{P}[ S_{t+1} \vert S_t ] = \mathbb{P} [S_{t+1} \vert S_1, \dots, S_t]
$$

换句话说，在既定现实下过去和未来是**条件独立（conditionally independent）**的，因为当前状态已经有了预测未来所需的所有统计信息。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_agent_environment_MDP.png)

*图 3  马尔可夫决策过程中的代理人-环境互动（图片来源：[Sutton & Barto (2017)](http://incompleteideas.net/book/bookdraft2017nov5.pdf) Sec. 3.1)*

马尔可夫过程有五个部分 $$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$$，符号含义和上面介绍关键概念时给出的一致，和 RL 问题情境完美对应：

- $$\mathcal{S}$$ - 状态集
- $$\mathcal{A}$$ - 行动集
- $$P$$ - 转移概率函数
- $$R$$ - 回报函数
- $$\gamma$$ - 未来回报的折扣因子。不确定环境下，我们并不十分清楚 $$P$$ 和 $$R$$

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_mdp_example.jpg)

*图 4  马尔可夫决策过程的有趣案例：工作日常（图片来源：[网络](https://randomant.net/reinforcement-learning-concepts/)）*

#### 贝尔曼方程

贝尔曼方程是一组方程，能将价值函数拆解成即时回报和折算未来价值两部分。
$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$

相应的对 Q 值有：
$$
\begin{aligned}
Q(s, a) 
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s, A_t = a] \\
&= \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{a\sim\pi} Q(S_{t+1}, a) \mid S_t = s, A_t = a]
\end{aligned}
$$

#### -- 贝尔曼期望方程

状态价值和行动价值函数上的方程通过递归可以进一步拆解。随着行动步数增长，遵照策略 $$\pi$$ 选择性拓展 $$V$$ 和 $$Q$$ 。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_bellman_equation.png)

*图 5  贝尔曼期望方程更新状态价值和行动价值函数图示*
$$
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \big) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi} (s', a')
\end{aligned}
$$


#### -- 贝尔曼最优方程

如果只对最优值感兴趣，相较于遵从策略计算期望，可以在选择性更新过程中不用策略直接求最大收益。最优值 $$V_*$$ 和 $$Q_*$$ 是能得到的最好结果，[前面](#---最优值和策略)定义过：
$$
\begin{aligned}
V_*(s) &= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\
V_*(s) &= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}
$$
不出所料，和贝尔曼期望方程看起来很像。

如果环境信息完备，这就变成了规划问题，可通过 DP 求解。不幸的是，绝大多数情况下我们并不知道 $$P_{ss'}^a$$ 或 $$R(s, a)$$，所以不能直接用贝尔曼方程求解 MDP，但它却是很多 RL 算法的理论基础。

### 常见方法

现在该看看解决 RL 问题的主要方法和经典算法了。

#### 动态规划

当模型完全已知，按贝尔曼方程可以用动态规划迭代价值函数并改进策略。

#### -- 策略评估

策略评估就是要计算给定策略 $$\pi$$ 时的状态价值 $$V_\pi$$ ：
$$
V_{t+1}(s) 
= \mathbb{E}_\pi [r + \gamma V_t(s') | S_t = s]
= \sum_a \pi(a \vert s) \sum_{s', r} P(s', r \vert s, a) (r + \gamma V_t(s'))
$$

#### -- 策略改进

在价值函数的基础上，策略改进会贪婪地寻找更优策略 $$\pi' \geqslant \pi$$ 
$$
Q_\pi(s, a) 
= \mathbb{E} [R_{t+1} + \gamma V_\pi(S_{t+1}) \vert S_t=s, A_t=a]
= \sum_{s', r} P(s', r \vert s, a) (r + \gamma V_\pi(s'))
$$

#### -- 策略迭代

*广义策略迭代（Generalized Policy Iteration,GPI）*算法就是结合策略评估与改进的迭代过程，为完善策略一路狂奔。
$$
\pi_0 \xrightarrow[]{\text{evaluation}} V_{\pi_0} \xrightarrow[]{\text{improve}}
\pi_1 \xrightarrow[]{\text{evaluation}} V_{\pi_1} \xrightarrow[]{\text{improve}}
\pi_2 \xrightarrow[]{\text{evaluation}} \dots \xrightarrow[]{\text{improve}}
\pi_* \xrightarrow[]{\text{evaluation}} V_*
$$

GPI 中，价值函数会不断逼近当前策略的实际价值，同时策略也在不断改进中靠向最优结果。策略迭代过程有效而且总能收敛到最优，这是为什么？

比方说，现有策略 $$\pi$$ 通过贪婪选择变成改进版 $$\pi'$$，$$\pi'(s) = \arg\max_{a \in \mathcal{A}} Q_\pi(s, a)$$。改进过的 $$\pi'$$ 保证更好，因为：
$$
\begin{aligned}
Q_\pi(s, \pi'(s))
&= Q_\pi(s, \arg\max_{a \in \mathcal{A}} Q_\pi(s, a)) \\
&= \max_{a \in \mathcal{A}} Q_\pi(s, a) \geqslant Q_\pi(s, \pi(s)) = V_\pi(s)
\end{aligned}
$$

#### 蒙特卡罗法

既已知 $$V(s) = \mathbb{E}[ G_t \vert S_t=s]$$，蒙特卡罗（Monte-Carlo，MC）法用了个讨巧的思路：从原本经历事件中学习，不对环境动态建模，将观测到的平均收益作为期望收益的近似。为了计算期望收益 $$G_t$$，MC 方法需要从<span style="color: #e01f1f;">**完备**</span>事件组 $$S_1, A_1, R_2, \dots, S_T$$ 中算出 $$G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$$ 同时要求所有事件必须有个最终结果。

状态 $$s$$ 的经验平均收益为：
$$
V(s) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s]}
$$
这里 $$\mathbb{1}[S_t = s]$$ 是一个二值指示函数。可以在每次造访状态 $$s$$ 时计数，由此可能在一个事件中存在对某个状态的多次访问（“每每造访”），或者只在某个事件中首次抵达一个状态时计数（“首次造访”）。借助统计 $$(s,a)$$ 的数量，这种近似方式可以轻易拓展到行动价值函数上。
$$
Q(s, a) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}
$$
想通过 MC 学习最优策略，要用类似 GPI 的思想进行迭代。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_MC_control.png)

1. 基于当前价值函数贪婪地改进策略：$$\pi(s) = \arg\max_{a \in \mathcal{A}} Q(s, a)$$.
2.  用新策略 $$\pi$$ 得到新事件（就是说使用像 ε-greedy 的算法帮助平衡探索与利用间的关系）
3.  从新事件中估计 Q：

$$
q_\pi(s, a) = \frac{\sum_{t=1}^T \big( \mathbb{1}[S_t = s, A_t = a] \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1} \big)}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}
$$

#### 时差学习

类似于蒙特卡罗法，时差（Temporal-Difference，TD）学习也是不用模型从所经历事件中学习。但 TD 学习能从<span style="color: #e01f1f;">**不完备**</span>事件组中学习，所以也就不用一直跟踪到事件结束。TD 学习非常重要， Sutton 和 Barto 在他们的 RL 书籍中将其描述为“对强化学习来讲既新颖又核心......的一个思想”。

#### -- 引导式

TD 学习法是通过现有猜测来更新目标值，而不像 MC 方法只依靠实际回报和完备收益。这种方法也叫**引导式（bootstraping）**。

#### -- 价值估计

TD 学习的核心思想就是面向预估收益 $$R_{t+1} + \gamma V(S_{t+1})$$ （也叫“ **TD 目标**”）更新价值函数 $$V(S_t)$$ ，具体要在多大程度上更新价值函数由超参数学习率 $$\alpha$$ 控制:
$$
\begin{aligned}
V(S_t) &\leftarrow (1- \alpha) V(S_t) + \alpha G_t \\
V(S_t) &\leftarrow V(S_t) + \alpha (G_t - V(S_t)) \\
V(S_t) &\leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
\end{aligned}
$$

类似的，也可以估计行动价值：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
$$

下面就来看看怎么找到 TD 学习的最佳策略（也称“TD 控制”）。友情提示，下面你将看到许多经典知名算法。

#### -- SARSA：策略性 TD 控制

“SARSA” 是指遵照序列 $$\dots, S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \dots$$ 更新 Q 值的过程，其思想和 GPI 一致：

1.  在 $$t$$ 时刻，从状态 $$S_t$$ 开始并按 Q 值选择行动 $$A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)$$；这里常会用到 ε-greedy 算法
2.  采取行动 $$A_t$$ ，得到回报 $$R_{t+1}$$ 并进入下一状态 $$S_{t+1}$$
3. 用和第一步相同的方式选择接下来的行动 $$A_{t+1} = \arg\max_{a \in \mathcal{A}} Q(S_{t+1}, a)$$.
4. 更新行动价值函数：$$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $$
5.  令 $$ t = t+1$$ ，回到第一步重复

SARSA 的每次更新都要按当前策略为两个步骤两次选定行动方案（步骤一和三）

#### -- Q 学习：非策略 TD 控制

Q 学习（[Watkins & Dayan, 1992](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)）的出现无疑是早期强化学习的重大突破

1.  在 $$t$$ 时刻，从状态 $$S_t$$ 开始并按 Q 价值选择行动 $$A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)$$；这里常会用到 ε-greedy  算法
2.  采取行动 $$A_t$$ ，得到回报 $$R_{t+1}$$ 并进入下一状态 $$S_{t+1}$$
3.  更新行动价值函数：$$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t)) $$.
4.  令 $$ t = t+1$$ ，回到第一步重复

头两步和 SARSA 一样。到第三步 Q 学习就不按当前策略选择行动了，而是独立估计当前策略下的最优值 $$Q_*$$。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_sarsa_vs_q_learning.png)

*图 6  Q 学习与  SARSA*

#### -- 深度 Q 网络

理论上讲，Q 学习时可以记下所有状态-行动对的 $$Q_*(.)$$，存一张巨大的表，但当状态和行动空间变大时这样很快就会达到算力瓶颈。所以人们用函数（也就是机器学习模型）来近似 Q 值，这叫**函数近似（function approximation）** 。例如，想用参数为 $$\theta$$ 的函数计算 Q 值，可记作 $$Q(s, a; \theta)$$。

不幸的是，用非线性 Q 值函数做近似并结合引导式的时候，Q 学习可能不稳定而且发散（见[问题二](# 死亡三项问题)）。

深度 Q 网络（“DQN”；[Mnih 等，2015](https://www.cs.swarthmore.edu/~meeden/cs63/s15/nature15b.pdf)）引入了两个创新机制，期望大幅改善和稳定 Q 学习训练过程：

-  **经历回放（Expreience Replay）**：所有事件序列 $$e_t = (S_t, A_t, R_t, S_{t+1})$$  都存在一段记忆中 $$D_t = \{ e_1, \dots, e_t \}$$。$$D_t$$ 内有着许多事件的经历元组。Q 学习更新期间，随机从回放记忆中抽取样本，自然一个样本可能被多次使用。经历回放改善了数据有效性，去掉了观测序列中的相关性，并使数据分布的起伏变化变得平滑。
-  **定期更新目标（Periodically Updated Target）**：Q 面向目标值优化，但目标值只会定期更新。每 $$C$$ 步（$$C$$ 是超参数）内复制并冻结 Q 网络将其作为优化目标。这一改动克服了短期波动，使训练更稳定。

损失函数如下：

$$
\mathcal{L}(\theta) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \Big[ \big( r + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta) \big)^2 \Big]
$$
回放记忆 $$D$$ 服从均匀分布 $$U(D)$$；$$\theta^{-}$$ 是冻住的 Q 网络的参数。

此外还发现将误差裁剪到 [-1, 1] 内会有帮助。（对参数裁剪总是心情复杂，虽然从经验上看很多研究展示了其有效性，但也让数学表达丑陋不堪）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_DQN_algorithm.png)

*图 7  DQN 算法，带经历回放和间断冻结优化目标。预处理序列是  Atari 游戏图像的输入处理结果，别太操心这些，就当作输入的特征向量即可（图片来源：[Mnih 等，2015](https://www.cs.swarthmore.edu/~meeden/cs63/s15/nature15b.pdf)）*

为改善原始设计有了很多 DQN 衍生版本，比如带竞争架构的 DQN（[Wang 等，2016](https://arxiv.org/pdf/1511.06581.pdf) ），它通过共享网络参数估计状态价值函数 $$V(s)$$ 和优势函数 $$A(s,a)$$

#### TD 和 MC 组合学习

在前面讲 TD 学习的部分，我们在计算 TD 目标的时候只顺着行动链条跟进了一步，任谁都能照着执行多步来估算收益。

将走 $$n$$ 步得到的期望收益记为 $$G_t^{(n)}, n=1, \dots, \infty$$，那么：

| $$n$$        | $$G_t$$                                                      | Notes         |
| ------------ | ------------------------------------------------------------ | ------------- |
| $$n=1$$      | $$G_t^{(1)} = R_{t+1} + \gamma V(S_{t+1})$$                  | TD learning   |
| $$n=2$$      | $$G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})$$ |               |
| ...          | ...                                                          |               |
| $$n=n$$      | $$ G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n}) $$ |               |
| ...          | ...                                                          |               |
| $$n=\infty$$ | $$G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-t-1} R_T + \gamma^{T-t} V(S_T) $$ | MC estimation |

n 步 TD 学习过程中值函数更新形式不变：

$$
V(S_t) \leftarrow V(S_t) + \alpha (G_t^{(n)} - V(S_t))
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_TD_lambda.png)

随自己喜欢随便选 $$n$$，要问的是哪个 $$n$$ 最好？哪个 $$G_t^{(n)}$$ 能带来最大收益？常见但明智的做法是对所有可能的 $$n$$ 步 TD 目标值加权求和而不是单选一个最好的 $$n$$。按权重因子 $$\lambda$$ 衰减，其与 $$n$$ 有关，$$\lambda^{n-1}$$；这里的思想类似于为什么在计算收益的时候要给未来回报打个折扣：看得未来越远，也就越没信心。为使所有权重项 $$(n \rightarrow \infin)$$ 总和为 1，给每一项都乘上$$(1-\lambda)$$，因为：
$$
\begin{aligned}
\text{let } S &= 1 + \lambda + \lambda^2 + \dots \\
S &= 1 + \lambda(1 + \lambda + \lambda^2 + \dots) \\
S &= 1 + \lambda S \\
S &= 1 / (1-\lambda)
\end{aligned}
$$
这样的 $$n$$ 步收益的加权和称为 $$\lambda$$ 收益 $$G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)}$$。用 $$\lambda$$ 收益更新值的 TD 学习记为 **TD(λ)**。我们介绍的[原始版本](#---价值估计)等价于 **TD(0)**

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_TD_MC_DP_backups.png)

*图 8  蒙特卡罗，时差学习和动态规划状态价值函数对比*

#### 策略梯度

上面讲的所有方法都是为了学习状态/行动价值函数，然后凭此选择行动。策略梯度法则选择用带参函数 $$\pi(a \vert s; \theta)$$，$$\theta$$ 是函数参数，直接学习策略。将回报函数（与损失函数相对）定义为*期望收益*，算法训练目标就是最大化回报函数，我的[另一篇](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)文章介绍了为什么策略梯度理论有效（证明）并介绍了若干策略梯度算法。

离散空间内：
$$
\mathcal{J}(\theta) = V_{\pi_\theta}(S_1) = \mathbb{E}_{\pi_\theta}[V_1]
$$
其中 $$S_1$$ 是起始状态。

连续空间内：
$$
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) V_{\pi_\theta}(s) = \sum_{s \in \mathcal{S}} \Big( d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s, \theta) Q_\pi(s, a) \Big)
$$
其中 $$\pi_\theta$$ 的马尔可夫链服从固定分布  $$d_{\pi_\theta}(s)$$。如果你并不熟悉什么叫“固定分布”，可以参考[文章](https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/)。

通过 *梯度上升法* 可以找到能带来最大收益的 $$\theta_*$$。不难看出基于策略的算法在连续空间更有用，其中有着无数行动和状态要进行价值估计，基于价值的方法也因此在计算开销上要大得多。

#### -- 策略梯度理论

通过在第 $$k$$ 维上用一个较小的 $$\epsilon$$ 震动 $$\theta$$ 可以 *定量* 计算梯度，甚至当 $$J(\theta)$$ 不可微时该方法同样有效，但不出所料这算起来会很慢。
$$
\frac{\partial \mathcal{J}(\theta)}{\partial \theta_k} \approx \frac{\mathcal{J}(\theta + \epsilon u_k) - \mathcal{J}(\theta)}{\epsilon}
$$

或者写成 *分析* 的形式
$$
\mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} [r] = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) R(s, a)
$$
实际上对将 $$d(.)$$ 换成 $$d_\pi(.)$$ 我们有很好的理论支撑：
$$
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) Q_\pi(s, a) \propto \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) Q_\pi(s, a)
$$

可以看 [Sutton & Barto (2017)](http://incompleteideas.net/book/bookdraft2017nov5.pdf) 章节 13.1 了解为什么如此。接着：
$$
\begin{aligned}
\mathcal{J}(\theta) &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) Q_\pi(s, a) \\
\nabla \mathcal{J}(\theta) &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \nabla \pi(a \vert s; \theta) Q_\pi(s, a) \\
&= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) \frac{\nabla \pi(a \vert s; \theta)}{\pi(a \vert s; \theta)} Q_\pi(s, a) \\
& = \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) \nabla \ln \pi(a \vert s; \theta) Q_\pi(s, a) \\
& = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s; \theta) Q_\pi(s, a)]
\end{aligned}
$$

这就是“策略梯度理论”，是各种策略梯度算法的理论基础
$$
\nabla \mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s, \theta) Q_\pi(s, a)]
$$

#### -- REINFORCE

REINFORECE 又称蒙特卡罗策略梯度，依靠 $$Q_\pi(s, a)$$ 更新策略参数 $$\theta$$，即用 MC 方法处理事件样本取得的预期收益作为更新依据。

一个常用 REINFORCE 变体是从收益 $$G_t$$ 中减去基准值，以降低梯度估计的方差同时保证偏差不变。比如，状态价值就是一个常用基准，如果照此做，在梯度上升更新中会有 $$A(s, a) = Q(s, a) - V(s)$$ 

1.  随机初始化 $$\theta$$
2.  生成一个事件 $$S_1, A_1, R_2, S_2, A_2, \dots, S_T$$
3.  对 $$t=1, 2, \ldots , T$$:
   1. 估计自 $$t$$ 时刻起的收益 $$G_t$$
   2. $$\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla \ln \pi(A_t \vert S_t, \theta)$$

#### -- 演员-评论员

如果价值函数是与策略一起学习，就得到了演员-评论员算法。

-  **评论员（Critic）**：更新价值函数参数 $$w$$ ，看算法，可以是行动价值 $$Q(a \vert s; w)$$ 或是状态价值 $$V(s; w)$$
- **演员（Actor）**：按评论员给出的方向更新策略参数 $$\theta$$，$$\pi(a \vert s; \theta)$$

来看一下行动价值演员-评论员算法的工作流程：

1.  随机初始化 $$s,\theta$$；采样得到 $$a \sim \pi(a \vert s; \theta)$$
2.  对 $$t = 1,\ldots, T$$:
   1.  对样本回报 $$r_t  \sim R(s, a)$$ 和接下来的状态 $$s' \sim P(s' \vert s, a)$$ 采样
   2.  采样得到下一行动 $$a' \sim \pi(s', a'; \theta)$$
   3. 更新策略参数： $$\theta \leftarrow \theta + \alpha_\theta Q(s, a; w) \nabla_\theta \ln \pi(a \vert s; \theta)$$.
   4.  计算 $$t$$ 时刻行动价值的修正量: <br/>
      $$G_{t:t+1} = r_t + \gamma Q(s', a'; w) - Q(s, a; w)$$ <br/>
       用其更新价值函数参数：<br/>
      $$w \leftarrow w + \alpha_w G_{t:t+1} \nabla_w Q(s, a; w) $$
   5.  更新 $$a \leftarrow a'$$ $$s \leftarrow s'$$

$$\alpha_\theta$$ 和 $$\alpha_w$$ 分别是更新策略和价值函数时的学习率。

#### -- A3C

**异步优势演员-评论员（Asynchronous Advantage Actor-Critic）**方法，简写为 A3C （[Mnih 等, 2016](http://proceedings.mlr.press/v48/mniha16.pdf)），是经典的专注于并行训练的策略梯度方法。

A3C 中，在评论员学习状态价值函数 $$V(s; w)$$ 的时候，同时有多个演员在并行训练并不时用全局参数进行同步。因此一般 A3C 很适合并行训练，也就是说适合在有多核 CPU 的机器上跑。

状态价值的损失函数是最小化均方误差 $$\mathcal{J}_v (w) = (G_t - V(s; w))^2$$ ，用梯度下降法寻找最优 $$w$$。状态价值函数是策略梯度更新的基准。

算法概貌如下：

1.  有全局参数 $$\theta$$ 和 $$w$$；相应有线程参数 $$\theta'$$ 和 $$w'$$
2.  初始化时间 $$t=1$$
3.  当 $$T \leqslant T_{MAX}$$:
    1.  重置梯度：$$\mathrm{d}\theta=0$$，$$\mathrm{d}w=0$$
    2.  全局和线程参数同步：$$\theta'=\theta,w'=w$$
    3.  $$t_\text{start} = t$$ 并得到 $$s_t$$
    4.  当 $$s_t \neq \text{TERMINAL}$$ 且 $$t - t_\text{start} \leqslant t_\text{max}$$ ：
        1.  选择行动 $$a_t \sim \pi(a_t \vert s_t; \theta')$$ 取得回报 $$r_t$$ 和新状态 $$s_{t+1}$$
        2.  更新 $$t=t+1$$，$$T= T+1$$
    5.  更新记录收益估计的变量 $$R = \begin{cases} 
        0 &  s_t \text{ 为 TERMINAL} \\
        V(s_t; w') & \text{其他}
        \end{cases}$$
    6.  对 $$i = t-1, \dots, t_\text{start}$$：
        1.  $$R \leftarrow r_i + \gamma R$$; 这里 $$R$$ 是 $$G_i$$ 的 MC 度量
        2.  累计梯度 $$w.r.t. θ'$$：$$\mathrm{d}\theta \leftarrow \mathrm{d}\theta + \nabla_{\theta'} \log \pi(a_i \vert s_i; \theta')(R - V(s_i; w'))$$;<br/>累计梯度 $$w.r.t. w'$$： $$\mathrm{d}w \leftarrow \mathrm{d}w + \nabla_{w'} (R - V(s_i; w'))^2$$
    7.  同步更新，用 $$\mathrm{d}\theta$$ 更新$$\theta$$，用 $$\mathrm{d}w$$ 更新 $$w$$

A3C 支持多代理并行训练，6.2 步的梯度累计可以看作对最小批随机梯度更新的改良：每个训练线程在各自方向上都独立对 $$w$$ 或 $$\theta$$  的值做出了一点修正

#### 演化策略

[演化策略](https://en.wikipedia.org/wiki/Evolution_strategy)（Evolution Strategies，ES）是一类模型无关的最优化方法，通过模仿达尔文自然选择的物种演化理论学习最优方案。用 ES 有两个前提条件：1）方案能与环境自由交互并验证是否能解决问题；2）能每个方案都能算出一个**适宜度（fitness）**得分表示其质量。想解决问题不必了解环境条件。

假设有一批随机方案，所有方案都能与环境交互并且只有适宜度高的方案能留下（*有限资源竞争中只有最适和的能幸存下来*）。通过对高适宜度幸存者的配置重组（*基因突变*）得到新一代方案。不断重复这一过程直到结果足够好。

和前面介绍的基于 MDP 的方法非常不同的一点在于，ES 学习策略参数 $$\theta$$ 的时候不做近似估计，假设参数 $$\theta$$ 服从[同向](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic)多元高斯分布，均值为 $$\mu$$，固定的协方差 $$\sigma^2I$$。可计算梯度 $$F(\theta)$$ ：
$$
\begin{aligned}
& \nabla_\theta \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} F(\theta) \\
=& \nabla_\theta \int_\theta F(\theta) \Pr(\theta) && \text{Pr(.) 为高斯密度函数} \\
=& \int_\theta F(\theta) \Pr(\theta) \frac{\nabla_\theta \Pr(\theta)}{\Pr(\theta)} \\
=& \int_\theta F(\theta) \Pr(\theta) \nabla_\theta \log \Pr(\theta) \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} [F(\theta) \nabla_\theta \log \Pr(\theta)] && \text{类似策略梯度更新的做法} \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \nabla_\theta \log \Big( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(\theta - \mu)^2}{2 \sigma^2 }} \Big) \Big] \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \nabla_\theta \Big( -\log \sqrt{2\pi\sigma^2} - \frac{(\theta - \mu)^2}{2 \sigma^2} \Big) \Big] \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \frac{\theta - \mu}{\sigma^2} \Big]
\end{aligned}
$$
可以按照“平均”参数 $$\theta$$ 重写公式（和上面的 $$\theta$$ 不同，这个 $$\theta$$ 是将来突变的基础基因） $$\epsilon \sim N(0, I)$$ ，所以 $$\theta + \epsilon \sigma \sim N(\theta, \sigma^2)$$。 $$\epsilon$$ 控制着生成变异时应该加入高斯噪声的比例：
$$
\nabla_\theta \mathbb{E}_{\epsilon \sim N(0, I)} F(\theta + \sigma \epsilon) = \frac{1}{\sigma} \mathbb{E}_{\epsilon \sim N(0, I)} [F(\theta + \sigma \epsilon) \epsilon]
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_EA_RL_parallel.png)

*图 9  一个简单的基于演化策略的 RL 算法。并行算子共享随机种子以便能用很小的通信带宽重构高斯噪声（图片来源：[Salimans 等， 2017](https://arxiv.org/pdf/1703.03864.pdf)）*

ES 作为一种黑箱式最优化算法，算是解决 RL 问题的另一种方法。它的很多特性使其训练既快速又简单：

- ES 不需要价值函数近似
- ES 不靠梯度反向传播
- ES 对延迟/长期回报不动摇
- ES 用很少的数据通信量实现了高度可并行化

### 已知问题

#### 探索利用窘境

探索利用困境广泛存在于我们的生活中，比方说你特别喜爱街角那家餐馆，如果你每天都去那里吃，你对你能得到的东西会有十分自信的预期，但也就失去了发现更优选项的机会。如果你总是尝试新馆子，很有可能你会不时吃到糟心的食物。类似的，在线顾问试图在已知的最吸引人的广告和可能会更成功的广告间进行权衡。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-08-31_exploration_vs_exploitation.png)

*图 10  探索利用困境的现实案例：去哪吃？（图片来源：[网络课件](http://ai.berkeley.edu/slides/Lecture%2011%20--%20Reinforcement%20Learning%20II/SP14%20CS188%20Lecture%2011%20--%20Reinforcement%20Learning%20II.pptx)）*

如果已经获取有关环境的所有信息，即使采用有些暴力的方式我们也能找到最优策略，更别提其他聪明方法了。困境来自于信息不完备：我们需要获取足够多的信息来制定最优决策同时保证风险可控。利用，可以充分发挥当下最优选项的潜力。探索，就要承担风险，收集未知选项的信息。最优长期策略可能免不了短期牺牲。比如，一次探索实验可能是完全失败的，但它会警告我们以后别这么做。

当 RL 面对未知环境，该问题就成了找到良好解决方案的关键：不充分探索，就不能充分了解环境；不充分利用，就不能完成回报最大化的任务。

不同的 RL 算法选择了不同方式平衡探索与利用。MC 方法、Q 学习或许多策略性算法，探索通常是靠 ε-greedy 算法实现的；对 ES，通过策略参数扰动进行探索。开发新 RL 算法的时候请记住这一点。

#### 死亡三项问题

我们一直在追求引导式 TD 方法的效率和灵活性，但当一个 RL 算法具备非策略性，非线性函数近似和引导式三个条件的时候，训练过程可能会不稳定而且很难收敛。该问题称为**死亡三项（deadly triad）**，[Sutton & Barto, 2017](http://incompleteideas.net/book/bookdraft2017nov5.pdf)。人们也提出了很多深度学习模型架构来解决这一问题，其中就包括用经历回放和间歇冻结目标网络稳定训练过程的 DQN。

### 案例研究：AlphaGo Zero

直到近些年，围棋一直都是人工智能领域里的一个难题。AlphaGo 和 AlphaGo Zero 是两个由 DeepMind 团队开发的程序，两者都采用了深度卷积网络（Convolutional Neural Networks，CNN）和蒙特卡罗树搜索（Monte Carlo Tree Search，MCTS)，现已证实两个程序都具备人类职业棋手的下棋水平。和 AlphaGo 从人类专家行动中进行监督学习不同，AlphaGo Zero 只用强化学习，不用人类知识就在基本规则之上自己和自己博弈。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_go_config.png)

*图 11  围棋棋盘。两名选手分执黑白子在 19 x 19 的棋盘上交替落子。一组棋子必须至少有一个开口（一个交叉点，称为 “气 ”）才能留在棋盘上，同时必须有至少有两个或更多的闭气（叫 “眼 ”）来保 “活 ”。不能在前一位置重复落子。*

有了上面 RL 的知识，可以看一下 AlphaGo Zero 是怎么做的。主要部分是一个基于棋盘布局的深度 CNN（准确地讲是一个带批归一化和 ReLU 的 ResNet）。该网络输出两个值

$$
(p, v) = f_\theta(s)
$$

- $$s$$: 游戏棋盘布局，19 x 19 x 17 的堆叠特征面；各位置有 17 个特征，8 个当前选手的过往布局（包括当前的）+ 8 个对手的过往布局 + 1 个颜色指示特征（1 = 黑，0 = 白）。因为神经网络是自己和自己下，而且当前选手和对手的的棋子颜色是交替变换的，所以要对颜色编码
- $$p$$: 在 $$19^2 +1$$ 个候选项中选定一步的概率
- $$v$$: 当前布置下的获胜概率

自我博弈期间，MCTS 会进一步改进行动概率分布 $$\pi \sim p(.)$$ 然后从改良策略中选择行动 $$a_t$$ 。回报 $$z_t$$ 是一个二值量，指明当前选手是否 *最终* 获胜。每一步都会生成一个事件元组  $$(s_t, \pi_t, z_t)$$ 并会存在回放记忆中。MCTS 的细节超出本文范围，感兴趣可以看原[论文](https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0)。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_alphago-zero-selfplay.png)

*图 11  AlphaGo Zero 自行训练的同时 MCTS 每步都在进一步改良策略*

网络用回放记忆中的样本训练，目标是最小化损失：
$$
\mathcal{L} = (z - v)^2 - \pi^\top \log p + c \| \theta \|^2
$$
其中 $$c$$ 是控制 L2 惩罚强度的超参数，加入惩罚项是防过拟合。

AlphaGo Zero 通过去掉监督学习，将分离的策略与价值网络合二为一简化了 AlphaGo。结果是 AlphaGo Zero 用更少的训练时间达到了更高的性能水平。建议比对阅读这两篇论文 [[1]](https://pdfs.semanticscholar.org/1740/eb993cc8ca81f1e46ddaadce1f917e8000b5.pdf) [[2]](https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0)，很有趣。

### 后记：ε-Greedy 算法

ε-greedy 算法多数时候会选择最佳行动，但会偶尔进行随机探索。根据过往经历，计算目标行动到目前为止取得的平均收益，以此判断其价值（到当前时刻 $$t$$）：
$$
\hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{\tau=1}^t r_\tau \mathbb{1}[a_\tau = a]
$$
其中 $$\mathbb{1}$$ 是二值指示函数， $$N_t(a)$$ 是到目前为止选择了多少次行动 a， $$N_t(a) = \sum_{\tau=1}^t \mathbb{1}[a_\tau = a]$$

ε-greedy 算法中会有小概率 $$\epsilon$$ 采取随机行动，其他时候（这占多数，概率 $$1-\epsilon$$）选择已学到的最佳行动： $$\hat{a}^{*}_t = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a)$$