---
layout:		post
title:  	强化学习入门
subtitle:   万字综述回顾 RL
date:       2020-07-05
author:     一轩明月
header-img: img/post-bg-unix-linux.jpg
catalog:    true
tags:
    - math
    - Reinforcement Learning
excerpt:    本文对强化学习进行了简短回顾，从基础概念到经典算法。希望本番回顾能帮助初入行的新人免于迷失在专有名词和术语当中。
---

> 编译自：A (Long) Peek into Reinforcement Learning，[Lilian Weng](https://lilianweng.github.io/lil-log/)

近些年人工智能（ Artificial Intelligence，AI）可谓大红大紫。AlphaGo 打败了最优秀的人类围棋选手，很快延拓算法 AlphaGo Zero 就打了 AlphaGo 一个 100：0，而且没有采取借人类经验进行监督学习的策略。在 DOTA 1v1 对抗赛中职业选手输给了 OpenAI 开发的机器人。知道这些以后，很难不对那些隐藏在背后的神奇算法感到好奇——强化学习（Reinforcement Learning，RL）。本文是对该领域的简短回顾，首先介绍几个基本概念，然后讲几个解决 RL 问题的经典方法，希望对入门新手有所帮助。

### 什么是强化学习

假设现在有个代理人，他不了解所处环境，但能从与环境的交互过程中得到一些回报，而他要做的就是想办法使累计收益最大。现实中的场景可能是一个机器人在打游戏希望获得高分，或着是试图通过物理手段解决现实任务，当然场景也不会仅限于此。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_RL_illustration.png)

*图 1  代理人和环境交互，努力采取明智行动使累计回报最大*

RL 的目标是期望代理人能从试探性实验和得到的相对简单的反馈中学到好策略。有了最优策略，代理人就能主动适应环境，保证未来回报最大。

#### 关键概念

下面正式介绍一下 RL 中的关键概念。

代理人的所有行为都处于一个**环境（environment）**之中。环境会对对特定行为作出怎样的反应是由一个**模型（model）**决定的，我们不一定能了解该模型。代理人能停留于某个环境**状态（state）** ($$s \in \mathcal{S}$$)  上，并选择一种行动 ($$a \in \mathcal{A}$$) 从一个状态转移到另一个状态。具体转移到哪个状态有状态间的转移概率 ($$P$$) 决定。一旦采取行动，环境会给予**回报（reward）**($$r \in \mathcal{R}$$) 作为反馈 。

模型决定了回报函数和转移概率。我们不一定清楚模型是怎样工作的，这就要分两种情况

- **了解模型**：信息完美，照此做规划；搞基于模型的 RL。如果充分了解环境，可以通过[动态规划（Dynamic Programming，DP）](https://en.wikipedia.org/wiki/Dynamic_programming)找到最优方案。你是否还记得算法课上“最长上升子序列”或“销售员问题”？哈哈，这并非本文关注重点。
- **不了解模型**：基于不完备的信息进行学习；搞无模型 RL 或者学个模型权当算法的一部分。下面的内容绝大部分都是模型未知的场景。

<span style="color: #e01f1f;">**想最大化总体回报**</span>，模型**策略（policy）**$$\pi(s)$$ 能告知特定状态下的最优行动。每个状态都关联着一个**价值（value）**函数 $$V(s)$$ ，预估当前状态下如果采用相应策略未来的期望收益水平。换句话说，价值函数就是在量化某个状态有多好，策略和价值函数也正是在强化学习中我们试图学习的内容。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_RL_algorithm_categorization.png)

*图 2  RL 方法总结，看是否想对价值、策略或环境建模（图片来源：[网络](https://youtu.be/2pWv7GOvuf0)）*

代理人和环境的交互涉及一系列行动和即时回报， $$t=1, 2, \dots, T$$。这一过程中，代理人逐步积累环境知识，学习最优策略，并判断下一步要采取什么行动方便高效学习最优策略。将 $$t$$  时刻的状态、行动和回报分别记为 $$S_t$$, $$A_t$$ 和 $$R_t$$ ，从而交互序列完全能用一个**事件（episode）**表示，也称试验或轨迹，序列终止于最终状态 $$S_T$$：
$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$

深入了解不同类别的 RL 算法的时候还会遇到些术语：

- **基于模型（model-based）**：依赖环境模型；要么模型已知，要么算法明确地学习过
- **非模型（model-free）**：学习期间不依赖模型
- **策略性（on-policy）**：使用目标策略的决策结果或样本训练算法
- **非策略性（off-policy）**：在转移或事件分布上训练，转移或事件由一个不同的行为策略而非目标策略生成

#### -- 模型：转移和回报

模型算是环境描述符，有模型就能学习或推测环境会怎样与代理人交互并提供反馈。模型有两个主要部分，转移概率函数 $$P$$ 和回报函数 $$R$$。

假设我们处于状态 $$s$$，决定采取行动前往状态 $$s'$$ 获取回报 $$r$$，这就是一步**转移（transition）**，可以用一个元组$$(s,a,s',r)$$表示。

采取行动并获得回报后，转移函数 $$P$$ 会将状态 $$s$$ 到 $$s'$$ 的转移概率记下来。用 $$\mathbb{P}$$ 作为“可能性”标识。
$$
P(s', r \vert s, a)  = \mathbb{P} [S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a]
$$
状态-转移函数可以定义为 $$P(s', r \vert s, a)$$:
$$
P_{ss'}^a = P(s' \vert s, a)  = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

回报函数 $$R$$ 会预估某个行动能带来的回报：
$$
R(s, a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s' \in \mathcal{S}} P(s', r \vert s, a)
$$

#### -- 策略

策略，即是代理人的行为函数 $$\pi$$，会告诉我们状态 $$s$$ 下要怎么行动。它是状态 $$s$$ 和行动 $$a$$ 间的映射，是确定的还是随机的不一定：

-  确定的：$$\pi(s) = a$$.
- 随机的：$$\pi(a \vert s) = \mathbb{P}_\pi [A=a \vert S=s]$$.

#### -- 价值函数

价值函数会通过预测未来回报来评估状态“品质”，或是会怎样奖励状态或行为。未来的回报也称**收益(return)** ，是未来折扣奖励的总和。从 $$t$$ 时刻开始计算收益 $$G_t$$ ：
$$
G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
折扣因子 $$\gamma \in [0, 1]$$ 会对未来奖励予以惩罚，因为：

- 未来收益可能存在高度不确定性，比如股票市场
- 未来回报不会提供即时好处，作为人类，可能更喜欢今日欢愉而非推到 5 年以后
- 折扣利于数学计算，我们不需要永远追着脚步计算收益
- 不需要担心状态转移图中的无限循环

$$t$$ 时刻状态 $$s$$ 的**状态价值（state-value）**就是期望收益，记 $$S_t = s$$:
$$
V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]
$$

类似的，可以定义一对状态-行动的**行动价值（action-value）**（“Q 价值”，我猜 Q 代表质量(Quality)？）：
$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert S_t = s, A_t = a]
$$

此外，因为我们尊崇目标策略 $$\pi$$，可以利用行动和 Q 值的概率分布表示状态价值：
$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$

行动价值和状态价值间的差异就是行动的**优势（advantage）**函数（“A 价值”）
$$
A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)
$$

#### -- 最优值和策略

最优价值函数带来的收益最大：

$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

最优策略能得到最优价值函数：

$$
\pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

当然，会有 $$V_{\pi_{*}}(s)=V_{*}(s)$$，以及 $$Q_{\pi_{*}}(s, a) = Q_{*}(s, a)$$.

#### 马尔可夫决策过程

更专业点说，几乎所有 RL 问题都可归结为**马尔可夫决策过程（Markov Decision Processes，MDPs）**。MDP 中的所有状态都具备马尔可夫性，就是说未来怎样只看当前状态，而非历史：

$$
\mathbb{P}[ S_{t+1} \vert S_t ] = \mathbb{P} [S_{t+1} \vert S_1, \dots, S_t]
$$

换句话说，在既定现实下过去和未来是**条件独立（conditionally independent）**的，因为当前状态已经有了预见未来所需的所有统计信息。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_agent_environment_MDP.png)

*图 3  马尔可夫决策过程中的代理人-环境互动（图片来源：[Sutton & Barto (2017)](http://incompleteideas.net/book/bookdraft2017nov5.pdf) Sec. 3.1)*

马尔可夫过程有五个部分 $$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$$，符号含义和上面关键概念中的相同，和 RL 问题情境完美对应：

- $$\mathcal{S}$$ - 一组状态
- $$\mathcal{A}$$ - 一组行动
- $$P$$ - 转移概率函数
- $$R$$ - 回报函数
- $$\gamma$$ - 未来回报的折扣因子。不确定环境下，我们并不十分了解 $$P$$ 和 $$R$$

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_mdp_example.jpg)

*图 4  马尔可夫决策过程的有趣案例：工作日常（图片来源：[网络](https://randomant.net/reinforcement-learning-concepts/)）*

#### 贝尔曼方程

贝尔曼方程是一组方程，能将价值函数拆解成即时回报和带折扣的未来价值两部分。
$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$

相应的对 Q 值有：
$$
\begin{aligned}
Q(s, a) 
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s, A_t = a] \\
&= \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{a\sim\pi} Q(S_{t+1}, a) \mid S_t = s, A_t = a]
\end{aligned}
$$

#### -- 贝尔曼期望方程

 递归更新过程能进一步拆解出构建在状态价值和行动价值函数上的方程。随着行动步数增长，遵照策略 $$\pi$$ 选择性拓展 $$V$$ 和 $$Q$$ 。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_bellman_equation.png)

*图 5  贝尔曼期望方程更新状态价值和行动价值函数图示*
$$
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \big) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi} (s', a')
\end{aligned}
$$


#### -- 贝尔曼最优方程

如果只对最优值感兴趣，相较于遵从策略计算期望，可以在选择性更新过程中不用策略直接求最大收益。最优值 $$V_*$$ 和 $$Q_*$$ 是能得到的最好结果，[上面](#---最优值和策略)定义过：
$$
\begin{aligned}
V_*(s) &= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\
V_*(s) &= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}
$$
不出所料，和贝尔曼期望方程看起来很像。

如果有环境的完备信息，这就变成了规划问题，可通过 DP 求解。不幸的是，绝大多数情况下我们并不知道 $$P_{ss'}^a$$ 或 $$R(s, a)$$，所以不能直接用贝尔曼方程求解 MDP，但它却是很多 RL 算法的理论基础。

### 常见方法

现在该看看解决 RL 问题的主要方法和经典算法了。未来可能会对每种方法撰文细讲。

#### 动态规划

当模型完全已知，按贝尔曼方程可以用动态规划迭代价值函数并改进策略。

#### -- 策略评估

策略评估就是要计算给定策略 $$\pi$$ 时的状态价值 $$V_\pi$$ ：
$$
V_{t+1}(s) 
= \mathbb{E}_\pi [r + \gamma V_t(s') | S_t = s]
= \sum_a \pi(a \vert s) \sum_{s', r} P(s', r \vert s, a) (r + \gamma V_t(s'))
$$

#### -- 策略改进

在价值函数的基础上，策略改进能靠贪婪行为得到更好的策略 $$\pi' \geqslant \pi$$ 
$$
Q_\pi(s, a) 
= \mathbb{E} [R_{t+1} + \gamma V_\pi(S_{t+1}) \vert S_t=s, A_t=a]
= \sum_{s', r} P(s', r \vert s, a) (r + \gamma V_\pi(s'))
$$

#### -- 策略迭代

*广义策略迭代（Generalized Policy Iteration,GPI）*算法就是结合策略评估与改进的迭代过程，就为完善策略
$$
\pi_0 \xrightarrow[]{\text{evaluation}} V_{\pi_0} \xrightarrow[]{\text{improve}}
\pi_1 \xrightarrow[]{\text{evaluation}} V_{\pi_1} \xrightarrow[]{\text{improve}}
\pi_2 \xrightarrow[]{\text{evaluation}} \dots \xrightarrow[]{\text{improve}}
\pi_* \xrightarrow[]{\text{evaluation}} V_*
$$

GPI 中，价值函数会不断逼近当前策略的真实价值，同时策略也在不断改进中靠向最优结果。策略迭代过程有效而且总能收敛到最优，这是为什么？

比方说，现有策略 $$\pi$$ 通过贪婪行为得到改进版 $$\pi'$$，$$\pi'(s) = \arg\max_{a \in \mathcal{A}} Q_\pi(s, a)$$。改进过的 $$\pi'$$ 保证更好，因为：
$$
\begin{aligned}
Q_\pi(s, \pi'(s))
&= Q_\pi(s, \arg\max_{a \in \mathcal{A}} Q_\pi(s, a)) \\
&= \max_{a \in \mathcal{A}} Q_\pi(s, a) \geqslant Q_\pi(s, \pi(s)) = V_\pi(s)
\end{aligned}
$$

#### 蒙特卡罗法

想一下 $$V(s) = \mathbb{E}[ G_t \vert S_t=s]$$，蒙特卡罗（Monte-Carlo，MC）法用了个讨巧的思路：从原本经历事件中学习，不对环境动态建模，将观测到的平均收益作为期望收益的近似。为了计算期望收益 $$G_t$$，MC 方法需要从<span style="color: #e01f1f;">**完备**</span>事件组 $$S_1, A_1, R_2, \dots, S_T$$ 中算出 $$G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$$ 同时要求所有事件必须有个最终结果。

状态 $$s$$ 的经验平均收益为：
$$
V(s) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s]}
$$
这里 $$\mathbb{1}[S_t = s]$$ 是一个二进制指示函数。可以每次在造访状态 $$s$$ 时计数，进而可能在一个事件中存在对某个状态的多次访问（“每每造访”），或者只在某个事件中首次抵达一个状态时计数（“首次造访”）。借助统计 $$(s,a)$$ 对的数量，这种近似方式可以轻易拓展到行动价值函数上。
$$
Q(s, a) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}
$$
想通过 MC 学习最优策略，要用类似 GPI 的思想进行迭代。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_MC_control.png)

1. 基于当前价值函数贪婪地改进策略：$$\pi(s) = \arg\max_{a \in \mathcal{A}} Q(s, a)$$.
2.  用新策略 $$\pi$$ 得到新事件（就是说使用像 [ε-greedy]({{ site.baseurl }}{% post_url 2018-01-23-the-multi-armed-bandit-problem-and-its-solutions %}#%CE%B5-greedy-algorithm) 的算法帮助平衡探索与利用间的关系）
3.  从新事件中估计 Q：

$$
q_\pi(s, a) = \frac{\sum_{t=1}^T \big( \mathbb{1}[S_t = s, A_t = a] \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1} \big)}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}
$$

#### 时间差异学习

类似于蒙特卡罗法，时间差异（Temporal-Difference，TD）学习也是不用模型从经历事件中学习。但 TD 学习能从<span style="color: #e01f1f;">**不完备**</span>事件组中学习，所以也就不用一直跟踪到事件结束。TD 学习如此重要以至于 Sutton 和 Barto 在他们的 RL 书籍中将其描述为“对强化学习来讲既新颖又核心......的一个想法”。

#### -- 引导式

TD 学习法是通过现有估计更新目标，而不像 MC 方法只依靠实际回报和完备收益。这种方法也叫**引导式（bootstraping）**。

#### -- 价值估计

TD 学习的核心思想就是面向预估收益 $$R_{t+1} + \gamma V(S_{t+1})$$ （也叫“ **TD 目标**”）更新价值函数 $$V(S_t)$$ ，具体要在多大程度上更新价值函数由超参数学习率 $$\alpha$$ 控制:
$$
\begin{aligned}
V(S_t) &\leftarrow (1- \alpha) V(S_t) + \alpha G_t \\
V(S_t) &\leftarrow V(S_t) + \alpha (G_t - V(S_t)) \\
V(S_t) &\leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
\end{aligned}
$$

类似的，也可以估计行动价值：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
$$

下面就来看看怎么找到 TD 学习的最佳策略（也称“TD 控制”）。友情提示，下面你将遇到许多经典知名算法。

#### -- SARSA：策略性 TD 控制

“SARSA” 是指遵照序列 $$\dots, S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \dots$$ 更新 Q 值的过程，其思想和 GPI 一致：

1.  在 $$t$$ 时刻，从状态 $$S_t$$ 开始并按 Q 值选择行动 $$A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)$$；常会用到 ε-greedy 算法
2.  采用行动 $$A_t$$ 后就能看到回报 $$R_{t+1}$$ 并进入下一状态 $$S_{t+1}$$
3. 用和第一步相同的方式选择接下来的行动 $$A_{t+1} = \arg\max_{a \in \mathcal{A}} Q(S_{t+1}, a)$$.
4. 更新行动价值函数：$$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $$
5.  令 $$ t = t+1$$ ，回到第一步重复

SARSA 的每次更新都要按当下策略给两个步骤两次选定行动方案（步骤一和三）

#### -- Q 学习：非策略 TD 控制

Q 学习（[Watkins & Dayan, 1992](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)）的出现无疑是早期强化学习的重大突破

1.  在 $$t$$ 时刻，从状态 $$S_t$$ 开始并按 Q 价值选择行动 $$A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)$$；常会用到 ε-greedy  算法
2.  采用行动 $$A_t$$ 后就能看到回报 $$R_{t+1}$$ 并进入下一状态 $$S_{t+1}$$
3.  更新行动价值函数：$$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t)) $$.
4.  令 $$ t = t+1$$ ，回到第一步重复

头两步和 SARSA 一样。到第三步 Q 学习就不按当前策略选择行动了，而是独立估计当前策略下的最优 Q 值 $$Q_*$$。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_sarsa_vs_q_learning.png)

*图 6  Q 学习与  SARSA 辅助理解图*

#### -- 深度 Q 网络

理论上讲，Q 学习时可以记下所有状态-行动对的 $$Q_*(.)$$，存一张巨大的表，但当状态和行动空间变大时很快就会达到算力瓶颈。所以人们用函数（也就是机器学习模型）来近似 Q 值，这叫**函数近似（function approximation）** 。例如，想用参数为 $$\theta$$ 的函数计算 Q 值，可记作 $$Q(s, a; \theta)$$。

不幸的是，用非线性 Q 值函数近似和引导式结合的时候，Q 学习可能不稳定而且发散（见[问题二](# 致命三件套)）。

深度 Q 网络（“DQN”；[Mnih 等，2015](https://www.cs.swarthmore.edu/~meeden/cs63/s15/nature15b.pdf)）引入两个创新性机制期望大幅改善和稳定 Q 学习训练过程：

-  **经历重放（Expreience Replay）**：所有事件序列 $$e_t = (S_t, A_t, R_t, S_{t+1})$$  都存在一段记忆中 $$D_t = \{ e_1, \dots, e_t \}$$。$$D_t$$ 内有着许多事件的经历元组。Q 学习更新期间，随机从回放记忆中抽取样本，自然一个样本可能被多次使用。经历重放改善了数据有效性，去掉了观测序列中的相关性，并使数据分布的起伏变化变得平滑。
-  **定期更新目标（Periodically Updated Target）**：Q 面向周期性更新的目标值进行优化。每 $$C$$ 步（$$C$$ 是超参数）内复制并冻结 Q 网络作为优化目标。这一改动克服了短期波动，使训练更稳定。

损失函数如下：

$$
\mathcal{L}(\theta) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \Big[ \big( r + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta) \big)^2 \Big]
$$
回放记忆 $$D$$ 服从均匀分布 $$U(D)$$；$$\theta^{-}$$ 是冻住的 Q 网络的参数。

此外还发现将误差裁剪到 [-1, 1] 内会有帮助。（对参数裁剪总是心情复杂，一方面从经验上很多研究展示了其有效性，但也让数学表达丑陋不堪）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_DQN_algorithm.png)

*图 7  DQN 算法，带经历重放和间断冻结优化目标。预处理序列是  Atari 游戏图像的输入处理结果，别太操心这些，就当作输入的特征向量即可（图片来源：[Mnih 等，2015](https://www.cs.swarthmore.edu/~meeden/cs63/s15/nature15b.pdf)）*

为改善原始设计有了很多 DQN 衍生版本，比如带竞争架构的 DQN（[Wang 等，2016](https://arxiv.org/pdf/1511.06581.pdf) ）它通过共享网络参数估计状态价值函数 $$V(s)$$ 和优势函数 $$A(s,a)$$

#### TD 和 MC 组合学习

在前面讲 TD 学习的部分，我们在计算 TD 目标的时候只顺着行动链条跟进了一步，谁都可以顺着执行多步来估算收益。

将走 $$n$$ 步得到的期望收益记为 $$G_t^{(n)}, n=1, \dots, \infty$$，那么：

| $$n$$        | $$G_t$$                                                      | Notes         |
| ------------ | ------------------------------------------------------------ | ------------- |
| $$n=1$$      | $$G_t^{(1)} = R_{t+1} + \gamma V(S_{t+1})$$                  | TD learning   |
| $$n=2$$      | $$G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})$$ |               |
| ...          | ...                                                          |               |
| $$n=n$$      | $$ G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n}) $$ |               |
| ...          | ...                                                          |               |
| $$n=\infty$$ | $$G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-t-1} R_T + \gamma^{T-t} V(S_T) $$ | MC estimation |

生成的 n 步 TD 学习过程仍旧是一样的价值函数更新形式：

$$
V(S_t) \leftarrow V(S_t) + \alpha (G_t^{(n)} - V(S_t))
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_TD_lambda.png)

随自己喜欢随便选 $$n$$，要问的是哪个 $$n$$ 最好？哪个 $$G_t^{(n)}$$ 能带来最大收益？常见但明智的做法是对所有可能的 $$n$$ 步 TD 目标值加权求和而不是单选一个最好的 $$n$$。权重按和 $$n$$ 相关的因子 $$\lambda$$ 衰减，$$\lambda^{n-1}$$；其思想类似于为什么在计算收益的时候要给未来回报打个折扣：看得未来越远，也就越没信心。为使所有权重项 $$(n \rightarrow \infin)$$ 总和为 1，给每一项都乘上$$(1-\lambda)$$，因为：
$$
\begin{aligned}
\text{let } S &= 1 + \lambda + \lambda^2 + \dots \\
S &= 1 + \lambda(1 + \lambda + \lambda^2 + \dots) \\
S &= 1 + \lambda S \\
S &= 1 / (1-\lambda)
\end{aligned}
$$
这样的 $$n$$ 步收益的加权和称为 $$\lambda$$ 收益 $$G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)}$$。用 $$\lambda$$ 收益更新值的 TD 学习记为 **TD(λ)**。我们介绍的[原始版本](#---价值估计)等价于 **TD(0)**

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_TD_MC_DP_backups.png)

*图 8  蒙特卡罗，时差学习和动态规划状态价值函数对比*

#### 策略梯度

上面讲的所有方法都是为了学习状态/行动价值函数，然后凭此选择行动。策略梯度法则选择用带参函数 $$\pi(a \vert s; \theta)$$，$$\theta$$ 是函数参数，直接学习策略。将回报函数（与损失函数相对）定义为*期望收益*，算法训练目标就是最大化回报函数，我的[另一篇](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)文章介绍了为什么策略梯度理论有效（证明）并介绍了若干策略梯度算法。

离散空间内：
$$
\mathcal{J}(\theta) = V_{\pi_\theta}(S_1) = \mathbb{E}_{\pi_\theta}[V_1]
$$
其中 $$S_1$$ 是起始状态。

连续空间内：
$$
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) V_{\pi_\theta}(s) = \sum_{s \in \mathcal{S}} \Big( d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s, \theta) Q_\pi(s, a) \Big)
$$
其中 $$\pi_\theta$$ 的马尔可夫链服从固定分布  $$d_{\pi_\theta}(s)$$。如果你并不熟悉什么叫“固定分布”，可以参考[文章](https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/)。

通过 *梯度上升法* 可以找到能带来最大收益的最佳 $$\theta$$。因为连续空间中有着无数行动和（或）状态要进行价值估计，自然会期望基于策略的算法在连续空间更有用，基于价值的方法也因此在计算开销上要大得多。

#### -- 策略梯度理论

可以在第 $$k$$ 维上通过一个较小的 $$\epsilon$$ 震动 $$\theta$$ 来 *定量* 计算梯度，甚至当 $$J(\theta)$$ 不可微时该方法同样有效，但不出所料这算起来会很慢。
$$
\frac{\partial \mathcal{J}(\theta)}{\partial \theta_k} \approx \frac{\mathcal{J}(\theta + \epsilon u_k) - \mathcal{J}(\theta)}{\epsilon}
$$

或者写成 *分析* 的形式
$$
\mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} [r] = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) R(s, a)
$$
实际上对将 $$d(.)$$ 换成 $$d_\pi(.)$$ 我们有很好的理论支撑：
$$
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) Q_\pi(s, a) \propto \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) Q_\pi(s, a)
$$

可以看 [Sutton & Barto (2017)](http://incompleteideas.net/book/bookdraft2017nov5.pdf) 章节 13.1 了解为什么这样。接着：
$$
\begin{aligned}
\mathcal{J}(\theta) &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) Q_\pi(s, a) \\
\nabla \mathcal{J}(\theta) &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \nabla \pi(a \vert s; \theta) Q_\pi(s, a) \\
&= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) \frac{\nabla \pi(a \vert s; \theta)}{\pi(a \vert s; \theta)} Q_\pi(s, a) \\
& = \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) \nabla \ln \pi(a \vert s; \theta) Q_\pi(s, a) \\
& = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s; \theta) Q_\pi(s, a)]
\end{aligned}
$$

该成果名为“策略梯度理论”，是各种策略梯度算法的理论基础
$$
\nabla \mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s, \theta) Q_\pi(s, a)]
$$

#### -- REINFORCE

 $$Q_\pi(s, a)$$, $$\theta$$.

 $$G_t$$   $$A(s, a) = Q(s, a) - V(s)$$ 

1.  θ
2.  $$S_1, A_1, R_2, S_2, A_2, \dots, S_T$$
3.  t=1, 2, ... , T:
   1. $$\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla \ln \pi(A_t \vert S_t, \theta)$$

#### -- 演员-评论员

-  $$Q(a \vert s; w)$$ or state-value $$V(s; w)$$.
- $$\pi(a \vert s; \theta)$$.

1.  $$a \sim \pi(a \vert s; \theta)$$.
2.  t = 1… T:
   1.  $$r_t  \sim R(s, a)$$ and next state $$s' \sim P(s' \vert s, a)$$.
   2.  $$a' \sim \pi(s', a'; \theta)$$.
   3. : $$\theta \leftarrow \theta + \alpha_\theta Q(s, a; w) \nabla_\theta \ln \pi(a \vert s; \theta)$$.
   4.  t: <br/>
      $$G_{t:t+1} = r_t + \gamma Q(s', a'; w) - Q(s, a; w)$$ <br/>
       <br/>
      $$w \leftarrow w + \alpha_w G_{t:t+1} \nabla_w Q(s, a; w) $$.
   5.  $$a \leftarrow a'$$ $$s \leftarrow s'$$.

$$\alpha_\theta$$ and $$\alpha_w$$ 

#### -- A3C

 $$V(s; w)$$, 

$$\mathcal{J}_v (w) = (G_t - V(s; w))^2$$ 

1.  θ' and w'.
2. $$t_\text{start}$$  $$s_t$$.
3.  ($$s_t \neq \text{TERMINAL}$$)  ($$t - t_\text{start} <= t_\text{max}$$):
   1.  $$a_t \sim \pi(a_t \vert s_t; \theta')$$  $$r_t$$  $$s_{t+1}$$.
4. $$R = \begin{cases} 
   0 & \text{if } s_t \text{ is TERMINAL} \\
   V(s_t; w') & \text{otherwise}
   \end{cases}$$.
5.  $$i = t-1, \dots, t_\text{start}$$:
   1. $$R \leftarrow r_i + \gamma R$$; here R is a MC measure of $$G_i$$.
   2. Accumulate gradients w.r.t. θ': $$d\theta \leftarrow d\theta + \nabla_{\theta'} \log \pi(a_i \vert s_i; \theta')(R - V(s_i; w'))$$;<br/>
      Accumulate gradients w.r.t. w': $$dw \leftarrow dw + \nabla_{w'} (R - V(s_i; w'))^2$$.

#### 演化策略

 $$\theta$$   $$\theta$$  $$\mu$$  $$\sigma^2I$$.  $$F(\theta)$$ 
$$
\begin{aligned}
& \nabla_\theta \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} F(\theta) \\
=& \nabla_\theta \int_\theta F(\theta) \Pr(\theta) && \text{Pr(.) is the Gaussian density function.} \\
=& \int_\theta F(\theta) \Pr(\theta) \frac{\nabla_\theta \Pr(\theta)}{\Pr(\theta)} \\
=& \int_\theta F(\theta) \Pr(\theta) \nabla_\theta \log \Pr(\theta) \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} [F(\theta) \nabla_\theta \log \Pr(\theta)] && \text{Similar to how we do policy gradient update.} \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \nabla_\theta \log \Big( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(\theta - \mu)^2}{2 \sigma^2 }} \Big) \Big] \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \nabla_\theta \Big( -\log \sqrt{2\pi\sigma^2} - \frac{(\theta - \mu)^2}{2 \sigma^2} \Big) \Big] \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \frac{\theta - \mu}{\sigma^2} \Big]
\end{aligned}
$$
 $$\theta$$  $$\theta$$  $$\theta$$ , $$\epsilon \sim N(0, I)$$  $$\theta + \epsilon \sigma \sim N(\theta, \sigma^2)$$. $$\epsilon$$ 
$$
\nabla_\theta \mathbb{E}_{\epsilon \sim N(0, I)} F(\theta + \sigma \epsilon) = \frac{1}{\sigma} \mathbb{E}_{\epsilon \sim N(0, I)} [F(\theta + \sigma \epsilon) \epsilon]
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_EA_RL_parallel.png)

### 已知问题

#### 探索利用窘境

#### 致命三件套

### 案例研究：AlphaGo Zero



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_go_config.png)


$$
(p, v) = f_\theta(s)
$$

- $$s$$: 
- $$p$$: 
- $$v$$: 

$$\pi \sim p(.)$$   $$a_t$$ $$z_t$$  $$(s_t, \pi_t, z_t)$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-05-19_alphago-zero-selfplay.png)
$$
\mathcal{L} = (z - v)^2 - \pi^\top \log p + c \| \theta \|^2
$$
 $$c$$