---
layout:		post
title:  	机器学习算法调试难点和解决办法
subtitle:   
date:       2021-02-20
author:     一轩明月
header-img: img/post-bg-future.jpg
catalog:    true
tags:
    - math
    - optimization
excerpt:    
---

> 编译自：Why are machine learning algorithms hard to tune and how to fix it， [Jonas Degrave, Ira Korshunova](https://engraved.ghost.io/why-machine-learning-algorithms-are-hard-to-tune/)

机器学习领域中，线性组合起来的损失遍地都是。实际上，这通常就是标准路径，尽管路上全是坑，尤其是考虑到这些线性组合让你的算法有多难调试。由此引出本文如下观点：

- 很多机器学习问题本该当作多目标问题对待，但却没有
- 多目标对待举措的缺失加大了这些机器学习算法超参调试难度
- 这些问题发生的时候几乎无法察觉，也就更难加以解决了
- 有办法可以解决这一问题，用不了几行代码。后文会介绍其中一种

本文没什么新奇内容，你可能早已知晓了我们想说的一切。但我印象中绝大多数机器学习教程并没有深入探讨优化方法（起码我的没有），结果就是梯度下降“一招鲜，吃遍天”。一般的教训在于，如果某个算法对你的问题无效，你需要花费更多的时间为问题调试超参数。

后文会介绍一种基于 NIPS‘88 的论文解决方案，其介绍了修正乘数微分法。希望本文能帮你消除一些疑惑，以更基础和有原则的方式解决问题。

### 无处不在的线性组合损失

尽管有单目标问题，一般也要加上额外的正则化。我们从整个机器学习领域中选了一个这样的优化目标集合。

首先是正规项，权重衰减和 lasso 损失。显而易见，当你加上这些正则化手段时，也就为问题创建了一个多目标损失函数。毕竟，你真正在意的是原损失 $$L_0$$ 和正则损失都要够低。为此，你会用参数 $$\lambda$$ 在二者间取得平衡。


$$
\begin{array}{l}
L(\theta)=L_{0}(\theta)+\lambda \sum|\theta| \\
L(\theta)=L_{0}(\theta)+\lambda \sum \theta^{2}
\end{array}
$$



结果就是像 VAE 这样的损失函数实质上是多目标的，首先是要尽可能覆盖数据样本，其次是要尽量贴合先验分布。此时，偶尔会配合 KL 散度，通过可调参数 $$\beta$$ 处理多目标损失。


$$
L(\theta)=\mathbb{E}_{q_{\phi}(z \mid x)}\left[\log p_{\theta}(x \mid z)\right]-\beta D_{K L}\left(q_{\phi}(z \mid x) \| p(z)\right)
$$



强化学习中，你也能看到这种多目标问题。不仅许多情景下的任务就是对局部目标回报的简单求和，策略损失通常也是线性组合的方式。以 PPO，SAC 和 MPO 的策略损失为例，采用带可调参数 $$\alpha$$ 的熵正则法。


$$
\begin{array}{c}
L(\pi)=-\sum_{t} \mathbb{E}_{\left(s_{t}, a_{t}\right)}\left[r\left(s_{t}, a_{t}\right)+\alpha \mathcal{H}\left(\cdot, s_{t}\right)\right] \\
L(\pi)=-\sum_{t} \mathbb{E}_{\left(s_{t}, a_{t}\right)}\left[\mathbb{E}_{\pi}\left(Q\left(s_{t}, a_{t}\right)\right)-\alpha D_{K L}(q \| \pi)\right]
\end{array}
$$



更不用说 GAN 损失，那就是辨别器和生成器损失的和：


$$
L(\theta)=-\mathbb{E}_{x}\left[\log D_{\theta}(x)\right]-\mathbb{E}_{z}\left[\log \left(1-D_{\theta}\left(G_{\theta}(z)\right)\right]\right.
$$



所有这些损失有个共同点，他们实质上都在尝试同时优化多个目标，并认为最优解是在这些常起冲突的力量中取得平衡。有些时候，所求和更加特殊并带有超参数来调节各部分权重。有时候对于损失为何这样组织有着清晰的理论基础，并且不用超参数来调节各部分间的平衡。

本文希望向你说明，这种损失组合方法听上去可能诱人，但这种线性组合实际上既不稳定又不可靠，所谓的平衡艺术更像是在走钢丝。

### 玩具样例

来考虑一种简单情况，试着优化一个线性组合损失，采用全局优化的方式。通过梯度下降我们观测到如下行为。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_optimising_a_multi-objective_loss_convex.gif)

*图 1  多目标损失优化*

写成 Jax 代码会像这样：

```python
def loss(θ):
  return loss_1(θ) + loss_2(θ)
loss_derivative = grad(loss)
for gradient_step in range(200):
  gradient = loss_derivative(θ)
  θ = θ - 0.02 * gradient
```

和通常一样，我们并没有立刻取得损失上的平衡，所以我们给第二个损失加了个比例系数 $$\alpha$$ 并运行下列代码：

```python
def loss(θ, α):
  return loss_1(θ) + α*loss_2(θ)
loss_derivative = grad(loss)
for gradient_step in range(200):
  gradient = loss_derivative(θ, α=0.5)
  θ = θ - 0.02 * gradient
```

我们希望看到的是调节 $$ \alpha$$ 后，我们可以在两个损失中取得均衡，并选择那个对应用来讲最满意的点。实际上我们会用一个超参数调节循环，人工选取一个 $$\alpha$$，执行最优化程序，判断第二个损失该再小点，调大 $$\alpha$$ 并重复整个最优化过程。几轮迭代后，我们敲定了最终方案，继续写我们的论文。

但这种情形并不总会发生，观测到的行为有时候会像这样：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_optimising_for_different_alphas_concave.gif)

*图 2  不同 $$\alpha$$ 下的损失优化表现*

好像不管则么调 $$\alpha$$，都不能在两个损失间取得较好的平衡。

我们可以看到两类方案，一个忽略了第一项损失，另一个忽略了第二项损失。但这些方案对大多数应用来讲都没啥用，多数情况下更能平衡两种损失的点会是我们更喜欢的方案。

实际上，人们几乎从没画过两个损失在训练过程中的变化图，所以动态图示所展示出的问题也就鲜有人提及。我们只看着训练曲线描绘整体损失，可能会得出超参需要进一步调试的结论，因为它看起来很敏感。或者我们可以用早停方法保证论文中的数字有效，毕竟评议员喜欢数据效率。

是哪里出了岔子？为什么这种方法有时有效，又为什么有时给不了你一个可调参数？为此，我们需要进一步看下两个图像间的差异。

两个图像是由同一问题得出的，一样的损失，一样的损失优化方法。所以不会是这些方面带来的差异。变化的条件是模型，换句话讲，模型参数 $$\theta$$ 对模型结果带来的影响是不同的。

所以，我们 *做个弊*，来把一些通常不可见的内容可视化出来，画出两个优化方案的帕累托边界。这是我们模型能取得的所有解的集合。换句话讲，这是能达到的损失的集合，不存在一个点使所有损失更优。不论你怎么调整两个损失，更好的方案永远落在帕累托边界上。通过调节超参数，一般只是在同一边界上找到了另一个不同的点。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_optimising_for_different_alphas_convex_pareto.gif)

*图 3  不同 $$\alpha$$ 下损失优化的帕累托边界（凸）*

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_optimising_for_different_alphas_concave_pareto.gif)

*图 4  不同 $$\alpha$$ 下损失优化的帕累托边界（凹）*

两条帕累托曲线的差异在于第一种情况下结果很好，但变了模型后就变得很糟。结果就是当帕累托边界是凸的，我们可以通过调节 $$\alpha$$ 参数达到任意均衡点，但是假如帕累托边界是凹的，同样的方法可能就不再有效了。

### 失效原因分析

我们可以画图看下为何梯度下降会对凹的帕累托边界失效，在三维世界中观察整体损失，损失用梯度下降优化。下图中我们将有关各损失的全局损失平面画了出来，尽管我们实际是在按各参数梯度在平面上向下走，我们走的参数下降的每一步在朝平面下方走时也都是必要的。你可以将梯度下降优化过程看作是在平面上放了个球形鹅卵石，让其随重力滚动直到停止。

优化过程停止的点就是优化结果，这里用星星表示。正如下图所示，无论你怎么摆动平面，最终都会在最优点停止。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_loss_optimization.png)

*图 5  损失优化图示*

通过调节 $$\alpha$$，空间仍为一个平面，毕竟改变 $$\alpha$$ 只是改了平面的斜率。如你所见，在凸条件下，帕累托曲线上的任意解都可通过调整 $$\alpha$$ 达到。$$\alpha$$ 大点就将星星拉向左侧，小点则拉到右侧。所有优化起点最终都会收敛于相同结果，这对任意 $$\alpha$$ 值都成立。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_visualising_the_convex_case.gif)

*图 6  凸条件下的优化过程*

但是，如果是用凹帕累托曲线对问题建模，它就显然成了我们问题的来源。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_visualising_the_concave_case.gif)

*图 7  凹条件下的优化过程*

试想我们的鹅卵石顺着该平面梯度滚动：有时会滚向左侧，有时是右侧，但总还是向下的。显然它最终会在两个角落之一处停下，要么是红星星，要么是蓝星星。调节 $$\alpha$$ 的时候，平面倾斜情况和凸条件下一模一样，但鉴于帕累托边界的形状，只能达到边界上的两个点，亦即凹曲线的两端。曲线上你实际想达到的 X 点，靠梯度下降法是找不到的。为什么？因为那是个鞍点。

还要多留意一下在我们调节 $$\alpha$$ 的时候发生了什么。可以看到我们调整了许多起点，最终都收敛于二解之一，可就是不能调整收敛到帕累托边界上的其他解上。

### 线性组合带来的问题

简单总结一下，这种线性组合方式会有如下问题：

- 首先，即使不用超参数平衡损失，**说梯度下降会在相互冲突的力量间进行权衡也不正确**。这要看你模型所能取得的解，随模型初始化的不同，它可能完全忽略某个损失而聚焦于另一个上，反之亦然。
- 其次，即使加上超参数，**超参数也是调的试着看**，完整跑一轮优化，看是否满意，然后再微调超参数。重复该优化循环直到满意为止。这是个费时费力的法子，一般都要跑梯度下降进行多次迭代。
- 第三，**无法调整超参数取得所有最优解**。不论你怎么折腾和微调，你也听不在你感兴趣的中间结果上，这倒并不是因为它们不存在，多数时候它们确实存在，而是因为选了差劲的损失组合方法。
- 第四，对实际应用得强调一下，**帕累托边界是否是凸的以及由此导致的损失权重是否可调总是未知的**。超参数好不好完全看你怎么对模型做的参数化，并且这又怎样影响到了帕累托曲线。但想可视化或分析所有实际应用的帕累托曲线是不可能的。可视化比原优化问题还要难得多。所以如果问题已经发生了，它也不会被注意到。
- 最后，如果你确实想用线性权重进行权衡，你需要显式证明**所用特定模型的整个帕累托曲线是凸的**。所以只是用对模型结果为凸的损失函数并不能避免问题发生。如果参数空间很大，涉及神经网络参数的话常常如此，你可能会忘记去试着证明一下。得强调一下，基于某些中间变量对损失所展示出的帕累托曲线凸性并不足以说明存在可调参数。凸性强烈依赖于参数空间，以及可取得解的帕累托边界的样子。

注意，绝大多数情况下帕累托边界既不凸又不凹，而是二者的混合。这使问题变得更加复杂。举个例子，凹段夹杂在凸段中间，如此一来不仅每个凹段会使梯度下降无解，它还会把参数初始空间分成两部分，一个会在一边的凸段上找到解，一个则只在另一段上能找到解。凹段越多问题越严重，如下图所示。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_visualising_the_mixed_case.gif)

*图 8  混合条件下的优化过程*



$$
\text { Minimize } L_{0}(\theta) \text { subject to } L_{1}(\theta)<\epsilon
$$

$$
L(\theta, \lambda)=L_{0}(\theta)-\lambda\left(\epsilon-L_{1}(\theta)\right)
$$


```python
def loss(θ, λ, ε):
  return loss_1(θ) - λ*(ε - loss_2(θ))

loss_derivative = grad(loss)
ε = 0.3 
λ = solve_dual(ε)  # The crux

for gradient_step in range(200):
  gradient = loss_derivative(θ, λ, ε)
  θ = θ - 0.02 * gradient
```





![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_the_solve-the-dual_method_convex_alpha.gif)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_the_solve-the-dual_method_concave_alpha.gif)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_the_solve-the-dual_method_concave_pareto.gif)

```python
def constraint(θ, ε):
  return ε - loss_2(θ)

optimization_derivative = grad(loss_1)
constraint_derivative = grad(constraint)

ε = 0.7

for gradient_step in range(200):
  while constraint(θ, ε) < 0:
    # maximize until the constraint is positive again
    gradient = constraint_derivative(θ, ε)
    θ = θ + 0.02 * gradient
    
  gradient = optimization_derivative(θ)
  θ = θ - 0.02 * gradient
```



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_the_hard_constraint_first_method_concave_pareto.gif)



```python
def lagrangian(θ, λ, ε):
 return loss_1(θ) - λ*(ε - loss_2(θ))

derivative = grad(lagrangian, (0,1))
ε = 0.7
λ = 0.0

for gradient_step in range(200):
  gradient_θ, gradient_λ = derivative(θ,λ,ε)
  θ = θ - 0.02 * gradient_θ  # Gradient descent
  λ = λ + gradient_λ  # Gradient ascent!
  if λ < 0:
    λ = 0
```



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_the_basic_differential_multiplier_method_convex_pareto.gif)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_the_basic_differential_multiplier_method_concave_pareto.gif)

```python
def lagrangian(θ, λ, ε):
 damp = 10 * stop_gradient(ε-loss_2(θ))
 return loss_1(θ) - (λ-damp) * (ε-loss_2(θ))

derivative = grad(lagrangian, (0,1))
ε = 0.7
λ = 0.0

for gradient_step in range(200):
  gradient_θ, gradient_λ = derivative(θ, λ, ε)
  θ = θ - 0.02 * gradient_θ
  λ = λ + gradient_λ
  if λ < 0:
    λ = 0
```





![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_the_modified_differential_method_of_multipliers_convex_pareto.gif)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-02-08_the_modified_differential_method_of_multipliers_concave_pareto.gif)