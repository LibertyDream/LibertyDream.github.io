---
layout:		post
title:  	解析神经网络嵌入机制
subtitle:   给我一个向量，还你一个世界
date:       2019-10-12
author:     一轩明月
header-img: img/post-bg-code.jpg
catalog: 	 true
tags:
    - NLP
    - embedding
---

> 文章编译自：
>
> https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526

神经网络的应用近两年突飞猛进，从图像分割到自然语言处理再到时序预测。嵌入（embedding）就是取得良好应用效果的典范，其常用于将离散变量转换为连续向量。这一技术已经得到了实际应用，词嵌入用于机器翻译，实体嵌入用于类别变量。

本文会解释什么是嵌入，为什么使用和怎么得到嵌入，并通过一个问题情境将这些内容串起来——将维基百科上的所有书籍用向量表示以用于构建推荐系统。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-12_tsne_embedding.png)

## 嵌入

嵌入就是从离散型类别变量到连续型数值向量的映射结果。在神经网络的语境下，嵌入是通过学习得到的低维连续向量，用于表征离散变量。神经网络嵌入十分有用，一来可以降低离散变量的维度，二来能在映射空间中表示类别信息。

神经网络嵌入的功用有三：

1. 寻找嵌入空间中的最近邻。这有助于按照我们的兴趣或类簇搭建推荐系统
2. 作为有监督机器学习模型的输入
3. 概念可视化和类别间关系的可视化

这意味着在图书项目中，借助嵌入机制，维基百科上 37,000 个图书条目变成了 37,000 个 50 维的向量。此外，在学得嵌入的过程中，彼此相似的图书在嵌入空间中的离得更近了。

嵌入的诞生是为了突破传统独热编码具有的两个局限。

## 独热编码的局限

对类别变量进行独热处理（one-hot) 实际上也是一种嵌入，一个类别一个向量。这一操作将离散实体和每次观测映射成大部分为值为 0 ，只有一个值为 1 用于指定类别的向量。

独热编码主要缺点有二：

1. 对于那些聚集度高，即取值选择多的变量，转换后维度过高且稀疏
2. 这种映射本身没有带来任何信息量，“相似”的类别在嵌入空间中没有走得更近

第一个问题很好理解：每多一个类别（实体），就必须在独热向量中加上一个值。如果有 37,000 本书，每一本都要被表示为一个 37,000 维的向量，任何机器学习模型都望洋兴叹。

第二个问题也是同样的：独热编码没有使相似的实体在向量空间中彼此距离更短。如果使用余弦相似度，独热编码后任意两个实体间的相似度都为 0。

这意味着如果使用独热编码，《战争与和平》和《安娜卡列尼娜》（都是列夫托尔斯泰的著作）间的相似性并不会强于《战争与和平》和《太空漫游指南》。

```python
# One Hot Encoding Categoricals
books = ["War and Peace", "Anna Karenina", 
          "The Hitchhiker's Guide to the Galaxy"]
books_encoded = [[1, 0, 0],
                 [0, 1, 0],
                 [0, 0, 1]]
Similarity (dot product) between First and Second = 0
Similarity (dot product) between Second and Third = 0
Similarity (dot product) between First and Third = 0
```

考虑到这两点，理想的类别变量表示应当维数比实体种数少同时相似实体会变得更相似。

```python
# Idealized Representation of Embedding
books = ["War and Peace", "Anna Karenina", 
          "The Hitchhiker's Guide to the Galaxy"]
books_encoded_ideal = [[0.53,  0.85],
                       [0.60,  0.80],
                       [-0.78, -0.62]]
Similarity (dot product) between First and Second = 0.99
Similarity (dot product) between Second and Third = -0.94
Similarity (dot product) between First and Third = -0.97
```

为了构建更好的类别实体表征，可以使用嵌入神经网络并设计监督学习任务学习嵌入表征。

## 学习嵌入

独热编码的主要问题是转换过程不依赖任何监督。通过有监督的神经网络训练可以大幅改善嵌入表现。嵌入构成了神经网络的参数（权重），要靠其最小化任务损失。最终得到的嵌入向量不仅表征实体且相似类别间距离更小。

比如说，我们手头有一个从电影评论中得到的 50,000 个词的词表，我们可以通过嵌入神经网络对每个词学习一个 100 维的嵌入向量，用这些向量来预测评论带有的情感【[实例](https://colab.research.google.com/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=embeddings-colab&hl=en)】。词表中类似"brillant"、"excellent"的积极词汇在嵌入空间走得更近了，因为神经网络学到了这些词都和正向评论相关。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-12_sentiment_pred.png)

上文图书的例子中，设定的监督学习任务可以是“确认一本书是不是列夫托尔斯泰写的”，得到的嵌入向量会把托尔斯泰的作品放在一起。**获取嵌入最难的地方在于如何设计监督学习任务来生成相关表示**

## 实现

维基百科图书项目中【[notebook](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb)】的监督学习任务是预测一个图书条目中是否存在着指向给定维基百科页面的链接。混合积极、消极数据对后，将`(书名,链接)`训练数据样本送入模型。这一设置的先验假设是链接到相似维基百科页面的两本书相似度更高。最终的嵌入向量也应当将相似的书在向量空间内放的更近。

我所使用的网络有两个并行的嵌入层，负责将书籍和百科链接映射成独立的 50 维向量，一个点积层负责整合嵌入向量得到预测输出。嵌入向量是网络参数，或者说权重，训练时不断修正以最小化损失。

下面是 Keras 代码（如果不甚了解代码，跳过看图即可）

```python
# Both inputs are 1-dimensional
book = Input(name = 'book', shape = [1])
link = Input(name = 'link', shape = [1])

# Embedding the book (shape will be (None, 1, 50))
book_embedding = Embedding(name = 'book_embedding',
                           input_dim = len(book_index),
                           output_dim = embedding_size)(book)

# Embedding the link (shape will be (None, 1, 50))
link_embedding = Embedding(name = 'link_embedding',
                           input_dim = len(link_index),
                           output_dim = embedding_size)(link)

# Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))
merged = Dot(name = 'dot_product', normalize = True, axes = 2)([book_embedding, link_embedding])

# Reshape to be a single number (shape will be (None, 1))
merged = Reshape(target_shape = [1])(merged)

# Output neuron
out = Dense(1, activation = 'sigmoid')(merged)
model = Model(inputs = [book, link], outputs = out)

# Minimize binary cross entropy
model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
```

一般情况下监督学习的目标是对新数据做预测，但在该嵌入模型中，预测只是达到目的的一种手段。我们想要的是嵌入权重，图书和链接连续向量化的表征。

嵌入向量本身没那么有趣，就是一些简单的数字向量

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-12_vector_embedding.png)

但是嵌入向量可以应用于上文提到的三个方向。对于这个项目来说，我们感兴趣的是基于近邻进行书籍推荐。为了计算相似度，我们给出待查询书籍的向量并在其与所有其他书籍向量的点积结果中寻找（如果嵌入向量经过了正则化，点积结果就是余弦相似度，范围从 -1，完全不相关，到 1，十分相似。此外我们还可以使用欧式距离计算相似度）。

```
Books closest to War and Peace.
Book: War and Peace              Similarity: 1.0
Book: Anna Karenina              Similarity: 0.79
Book: The Master and Margarita   Similarity: 0.77
Book: Doctor Zhivago (novel)     Similarity: 0.76
Book: Dead Souls                 Similarity: 0.75
```

向量与向量自身的余弦相似度为 1。经过维度缩减（见下文），可以得到类似下图的结果：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-12_dim_reduction.png)

可以清晰地看到学得的嵌入向量值，现在我们就对每本书构建起了 50 维的表征，相似的书彼此距离更小。

## 可视化嵌入

嵌入很棒的一点是他们可以用于可视化类似“相对其他内容是否是小说”这样的概念。这需要更进一步的维度缩减技术来讲维度降到 2 或 3 维。最流行的降维技术本身就是一种嵌入方法：t分布随机近邻嵌入（t-Distributed Stochastic Neighbor Embedding ，TSNE)

我们可以将原始的记录所有维基百科图书的 37,000 维特征，通过神经网络嵌入映射成 50 维，再经过 TSNE 映射到 2 维。结果如下：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-12_TSNE_2_dim.png)

TSNE 是一种流形学习技术，其将高维数据映射到低维流形并创建一个嵌入向量以记录数据的局部结构特征。它几乎专为可视化而生，因为结果是随机的且不支持转换新数据。最新的替代技术是均匀流行近似投影技术（ [Uniform Manifold Approximation and Projection, UMAP](https://github.com/lmcinnes/umap)），其速度更快且支持将新数据转换至嵌入空间。

缩减本身不是很有用，可一旦我们根据不同书籍类别进行上色后意义骤现。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-12_embed_colored.png)

可以清晰的看到同一类别书籍组成的群组。这并不完美，但仍然令人动容，我们只用两个数字就表示了所有书籍且依然保有类别间差异性。