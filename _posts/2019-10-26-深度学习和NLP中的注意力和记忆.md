---
layout:		post
title:  	深度学习和 NLP 中的注意力与记忆
subtitle:   
date:       2019-10-26
author:     一轩明月
header-img: img/post-bg-future.jpg
catalog: 	 true
tags:
    - NLP
excerpt:   本文以神经翻译机为例探讨了注意力（attention）机制想解决的问题，同时指出了该机制引入的高额计算代价，其次在注意力工作方式的启发下尝试从“记忆模式”的角度进行了解释。
---

> 文章编译自：
>
> http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/ 
>
> 博客内含有数学公式，如果使用 Chrome 浏览器观看的话，可以添加这个[插件](https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima)

神经网络里的注意力机制和人类视觉的注意力没啥关系。人们对视觉注意力研究的很深入并有很多模型，而所有这些模型都可以归结为对特定区域“高分辨率”细细看，周围环境“低分辨率”粗略看，并随时间调整焦点。

神经网络运用注意力由来已久，特别是在图像识别方面。但直到最近注意力机制才并应用到循环神经网络架构上，尤其是 NLP 领域，这也是本文关注的内容。

# 注意力解决什么问题？

不妨以神经翻译机（Neural Machine Translation, NMT)为例看看注意力为我们做了什么。传统机器翻译依赖于在文本统计信息上进行复杂的特征工程，得到的系统复杂且搭建很费力。神经机与此不同，其会将序列映射成定长的向量表示并基于向量生成翻译。因为不依赖词袋和 n-gram 计数之类的内容，而是转向从高维空间中捕获文本信息，NMT 系统得到的序列效果通常比其他方法好。更重要的是 NMT 系统更易于搭建和训练，也不需要多少手工操作编码特征。

大多数 NMT 系统的套路都是先通过一个循环神经网络（RNN）将原序列（比如一句德文）**编码(encoding）**成一个向量，然后还是通过一个 RNN 对向量进行解码得到新序列（比如英文）

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-26_nmt_eg.png)

  图中的“Echt”, “Dicke” 和 “Kiste”  被送入**编码器(encoder)**，当一个特定信号传来（图中未显示）**解码器(decoder)**开始进行翻译，直到翻译到序列结束标识符为止。这里向量 $$h$$ 表示编码器内部的隐态。

如果你仔细看，会发现解码器应该只依赖编码器中最近的隐态（图中的$$h_3$$)进行翻译。这个 $$h_3$$ 向量必须将我们对原序列知道的一切都包含在内，更专业一点说这个向量是序列的**嵌入(embedding)**。实际上如果你用 PCA 降维，然后将不同序列的嵌入画在一张图里，你会看到语义相近的短语会彼此靠近。这是嵌入的神奇之处。

即使这样，假定能将一个可能非常长的序列编码成一个简单的向量且不损失信息，同时解码器还能仅凭它就得到一条好翻译，这似乎有点无理取闹。比如说，原序列有 50 个词，而英文翻译中的第一个词很可能与原序列中的第一个词高度相关。但这意味着解码器要能够考虑到 50 步之前的信息，同时这些信息经由某种方式编码进向量当中。众所周知循环神经网络难以应付长范围依赖。理论上 LSTM 类架构能处理这个问题，但实践中长范围依赖依旧问题多多。比如，研究员们发现倒置原序列（逆序传入编码器）可以取得十分优异的结果，因为此时解码器到相关编码器的距离变短了。相似的，将输入序列重复两次（abc->abcabc）同样有助于神经网络进行记忆。

我认为倒置序列的方法不过是奇技淫巧。实践中虽然使结果变好，但它却不是理论级的解决方法。绝大多数翻译基准都是法语或德语一类的语言，它们本身与英语就很相像（即使是中文其词语顺序也和英语十分相似）。可在有些语言（比如日语）中，句子的最后一个词可能和英文翻译的第一个词高度相关，如果此时倒置输入只会使结果更糟。所以还有其他方法吗？注意力机制。

有了注意力机制我们不再试图将所有原序列内容都编入一个定长向量中。转而尝试在解码器每次输出时能“留意”原序列的不同部分。关键地方在于，让模型**懂得**输入序列里要关注什么，以及它已经输出了什么。因此，对于对照关系良好的语言（比如英语和德语），解码器可能会挨个留意序列内容——翻译第一个英文词时看第一个词，翻译第二个看第二个等等。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-26_attention_xy.png)

这里 $$y$$ 是解码器得到的翻译文本， $$x$$ 是我们的原词语序列。上图用了一个双向循环神经网络，但这不重要你可以将反向的部分忽略不看。重要的地方是每个解码器输出都是基于**输入状态的加权组合**，而不仅仅是最近的状态。$$a$$ 是权重，代表对于每个输出各个输入值得被考量的程度。所以，如果 $$a_{3,2}$$ 的值很大，意味着解码器在生成目标序列中的第三个词时会着重参考原序列的第二个状态。$$a$$ 也通常会被正则化，使其累加总和为 1（这样他们就是输入状态上的一个分布了）。

注意力一个很棒的地方在于它让我们能解释和看到模型正在做什么。比如，在翻译一句话的时候通过可视化权重矩阵 $$a$$ 可以知道模型是怎么进行翻译的

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-26_visual_translation.png)

这里可以看到在将法文翻译成英文的过程中，网络会逐个参考每个输入。有时为了一个输出也会参考两个单词，比如将"la Syrie"译成"Syria"。

*ps:更多注意力的应用可以参考[这里](https://libertydream.github.io/2019/10/05/注意力和增强循环神经网络/)* 

# 注意力的代价

如果我们再仔细看一下注意力的计算方式不难发现注意力是有代价的。我们需要对每一组输入输出计算注意力。如果输入 50 个词的序列，同样输出 50 个词的序列，那么就有 2500 个注意力值。这也还好，但如果是字符级并且序列由数百个标记组成那么要实施注意力机制的代价就是极其高昂的。

实际上这十分反直觉。人类的注意力是应当**节约**的算力资源。聚焦于某物之上时会忽略其他事物。但上述模型显然不是这么做的，实质上我们事无巨细的审查一切细节然后决定专注于哪一部分。直观上就是输出一个译文单词，随后检索所有内部文本记忆来决定下一个输出的词是什么。这似乎是种浪费且完全不是人类做派。实际上其更接近于内存访问而不是注意力，个人觉得称之为注意力有些措辞不当。当然，这依旧无法阻挡注意力机制受欢迎同时在许多任务上表现亮眼。

注意力的替代方案是使用强化学习来预测要关注的区域，这听起来就很像人类注意力了。

# 注意力=（模糊）记忆？

注意力机制所解决的基本问题是使神经网络可以参考输入序列而不是强行将所有信息编码成一个定长向量。但如上文所述，我认为叫注意力有点用词不当。换种方式解释，注意力机制就是使神经网络可以访问内部记忆，也就是编码器中的隐态。这种解释下，相较于说选择“留意”什么，不如说神经网络是在选择读取哪段记忆。不同于典型的记忆，这里的记忆访问机制是弹性的，就是说神经网络取得的是所有记忆位置的加权组合，而非从单一位置得到的孤立值。弹性记忆的好处是可以容易地使用反向传播端到端的训练网络（也确实有些非模糊的做法，比如梯度计算选择采样法而不是反向传播）

记忆机制本身的历史更加悠久。标准 RNN 模型的隐态本身就是一种内部记忆。RNN 有梯度消失问题致使其无法学习长依赖关系。LSTM 类模型在此基础上使用门机制决定删除或更新具体的记忆内容。

向着更复杂的记忆结构演化的趋势仍在继续。端到端记忆网络使得网络可以在输出前，多次读取相同输入序列，每次更新一下记忆内容。比如说，通过对某个故事进行多段推理以回答问题。但是当网络参数权重以某种方式绑定在一起，端到端记忆网络中的记忆机制就变成了本文说的注意力机制，只是它在记忆上跳跃了多次（因为要整合多个序列信息）

神经图灵机用了相似形式的记忆机制，但加上了更复杂的取址方式——不仅有基于内容的（本文所示）还有基于位置的寻址，这使得网络可以学习寻址模式来执行简单的计算机程序，比如说排序算法

