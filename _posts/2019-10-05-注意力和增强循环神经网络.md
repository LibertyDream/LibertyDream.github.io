---
layout:     post
title:      注意力和增强循环神经网络
subtitle:   聊聊 attention
date:       2019-10-05
author:     一轩明月
header-img: img/post-bg-blue.jpg
catalog: 	 true
tags:
    - NLP
excerpt:    attention 机制是自然语言处理领域的新锐派，本文重点介绍了基于注意力的四个循环神经网络研究方向——神经图灵机，注意力接口，自适应计算耗时以及神经编程机
---

> 文章编译自：
>
> https://distill.pub/2016/augmented-rnns/

循环神经网络（RNN）是深度学习的代表之一，使神经网络可以处理文本、音频和视频之类的序列数据。常被用来对序列进行高度抽象，注解序列甚至生成全新的序列。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_RNN.png)

RNN 基本构型处理长序列有问题，但是某些变种，比如长短文记忆网络（[LSTM](https://libertydream.github.io/2019/09/21/GRU与LSTM图解/)），依旧可以胜任这类任务。这类模型十分强大在许多领域，比如翻译、语音识别和图像起标题等，都取得了令人瞩目的成绩。因而最近几年 RNN 可谓风生水起，使用广泛。

有鉴于此，我们看到了许多对增强 RNN 赋予其新特性的有益尝试。有四个方向格外突出，振奋人心：

* 神经图灵机（Neural Turing Machines，NTM）：增加了额外的记忆读写区
* 注意力接口（Attentional Interfaces）：允许 RNN 只专注于部分输入
* 自适应计算耗时（Adaptive Computation Time）：每个时步的计算量可变
* 神经编程机（Neural Programmers）：可以调用函数，运行时创建程序

特别地，这些尝试都是对 RNN 构型的有效拓展，但真正了不起的地方在于他们可以彼此组合，似乎只是更广阔空间中的一些点。同时，他们的工作都依赖于名为“注意力（attention）”的底层技巧。

预计这些强化 RNN 构型在不远的将来会写下浓墨重彩的一笔，拓展深度学习的疆界。

### 神经图灵机

神经图灵机给 RNN 加上了额外的记忆读写区。因为神经网络的语言是向量，记忆就是一组向量。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_NTM_1.png)

那它又是怎么读写的呢？难点在于怎么确认该读还是该写，特别是最好能从读和写的位置直接分辨二者。棘手之处在于记忆地址本质上是离散的。NTM 采取了一个很聪明的办法：每一步都对整个记忆区进行读写，只是比例不同。

举例来说，先看读操作。相较于指定一个单独的位置，RNN 输出一个”注意力分布“，分布表示我们对不同记忆位置的关注程度，也就是各部分的权重。这样读操作就是对记忆区的加权求和。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_NTM_read.png)

相似的，我们每次以不同的权重到处写，注意力分布表示每个位置我们”着墨多寡”。要做到这点，就要将一个位置的旧记忆内容和新写入值卷积组合，组合比例由注意力权重决定。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_NTM_write.png)

但 NTM 又是怎么确定组合比例，换句话说怎么知道该关注记忆区的哪个位置呢？NTM 实际上采用了两种方法的组合策略：基于内容的注意力和基于位置的注意力。基于内容的注意力允许 NTM 浏览整个记忆并聚焦于他们要找的内容的地方，而基于位置的注意力允许在记忆内相对移动，这样 NTM 就可以循环了。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_NTM_attention_dist.png)

1. RNN 控制器给一个查询向量，和每个记忆向量计算相似度
2. 相似度得分通过 softmax 激活，得到分布
3. 之后和旧时步的 attention 按比例融合
4. 将 attention 和移位过滤器（shift filter）进行卷积，这样就可以转移关注点了
5. 最后对 attention 锐化，突出重要的，弱化不重要的。得到反馈给读写需求的最终 attention 分布

这种读写能力使得 NTM 可以实现许多简单算法，早先神经网络是做不到的。比如，神经图灵机可以将长序列存在记忆里，然后在上面不断循环。这一过程中，我们能看到他们在哪里写，又从哪里读，从而更好地理解他们的行为：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_NTM_long_seq.png)

NTM 还可以模仿查找表，甚至给数字排序（尽管有点假）。另一方面，他们还不能做许多基本的事情，比如加法或乘法。

从初始 NTM 论文发表以来，有许多令人振奋的相似方向的进一步探索。从客观角度来看，这些模型可以完成很多类似数字相加的任务，客观上并不难。传统程序合成社区却会为此抓耳挠腮。但神经网络还胜任许多其他任务，像神经图灵机这样的模型似乎已经极大地扩展了其能力边界。

### 注意力接口

当我想翻译一个句子，我会关注在当前在翻译的词上。当我在转录音频时，我会仔细倾听我正在记录的片段。如果你让我描述我待的屋子，我会环顾四周物品。

神经网络通过 attention 可以做到相同的事，只关注所给信息中的一部分。比如，一个 RNN 工作在另一个 RNN 的输出之上，每个时步它会关注另一个 RNN 的不同位置。

我们希望注意力清晰可辨，这样我们就知道要关注哪里。为了达到这一目的，还是使用和神经图灵机里一样的把戏：哪里都看，只是程度不同。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_attentional_interfaces.png)

注意力分布通常通过基于内容的注意力产生。伴随 RNN 会给出一条查询表明它的关注点。每个条目与查询做点积得到匹配度得分。这些得分传入一个 softmax 激活并生成注意力分布。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_attention_inter_dist.png)

RNN 间的注意力一大应用是翻译。传统 seq2seq 模型会将输入简化成单个向量然后再扩展回来。attention 避免了这点，处理输入的 RNN 上报他所看到的每个词，生成输出的 RNN 由此可以专注于输出相关的词。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_translation.png)

这类 RNN 间的 attention 还有一系列其他应用。它可被用于声音识别，一个 RNN 处理音频，另一个 RNN 浏览处理结果，只关注和当前记录内容相关的部分。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_voice_recognition.png)

这类注意力的用途还有解析文本，能让模型生成解析树的时候浏览所有词。还可以用于对话模型，能让模型在产生答复的时候关注于早先谈话内容。

attention 还可以用来作为 CNN 和 RNN 的接口。这使得 RNN 每一步都能看到图片的不同位置。这类注意力的一大流行用法是给图像起标题。首先，卷积网络处理图像抽取高维特征。然后运行 RNN 生成图像描述。在生成描述中的每个字的时候，RNN 关注于卷积网络对图像相关部分的解释。我们可以清楚地看到这一过程：

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_show_attend_tell.png)

更广泛地讲，只要一个人想要与输出中存在重复结构的神经网络交互就可以使用注意力接口。

### 自适应计算耗时

标准 RNN 构型每一时步的计算量相同。这似乎并不直观，事情已经很难了还来得及多想吗？这也限制了 RNN 构型在长度为 n 的列表上只能完成 O(n) 级别的运算量。

自适应计算耗时能使 RNN 每一时步的计算量各不相同。总体思路很简单：让 RNN 每一时步做多步计算。

为了让网络学习该进行多少步运算，我们希望步骤数可以区分。我们通过先前使用的老技巧：不指定具体的离散步骤数，我们在所有计算步骤上加上一个注意力分布。各时步结果加权求和得到最终结果。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_adaptive_time.png)

上图中有些细节没有提到。下图是一个包含三步计算的完整时步计算图。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_three_step_time_step.png)

这有点复杂，让我们一步步看。从顶层来说，我们依旧在跑一个普通 RNN，输出结果是状态的加权组合。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_time_step_overview.png)

每一步的权重由“停止神经元”决定。它是一个观察着 RNN 状态的 sigmoid 神经元，给出宕机权重。权重代表着终止该步计算的概率。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_halting_neuron.png)

停止权重总和为 1，沿着上方轴线分配权重。当权重小于 epsilon 时，停止分配。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_halt_weight.png)

因为是在小于 epsilon 时停止的，所以可能还会剩下些权重。怎么处理呢？技术上讲，我们希望将其授予下一步计算，但是我们并不想执行那些计算，所以将它分配给最后一步。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_last_step.png)

在训练自适应耗时模型的时候，损失函数中要添加”思索成本“。这会对模型使用的计算量进行惩罚。该项越大，模型计算耗时越少。

### 神经编程机

神经网络在许多任务上表现出色，但在做一些基础算术上仍有困难，而这在普通实现方法中是微不足道的。如果能结合神经网络和普通编程，有机结合两个世界就太好了。

神经编程机[[1]](http://arxiv.org/pdf/1511.04834.pdf)就是这样一种尝试。它会学习创造程序以解决任务。事实上，该模型学习生成程序并不需要正确的样例程式。它将怎样生成程序视作最终完成任务的一种手段。

论文中的实际模型通过生成类似 SQL 的程序查询表，以此回答表相关的问题。但这里有些细节使这一任务稍显复杂，我们不妨先想一下更简单的模型，该模型接收一个算术表达式并生成程序执行运算。

所生成的程序是一个运算序列。每个运算都定义在先前运算的结果之上。所以某一步运算可能是”将两步前的结果和一步前的结果相加“，这更像是 Unix 管道而不是一个会分配、读取变量的程序。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_NP_operation.png)

在 RNN 控制器的操纵下，该程序每次生成一条运算指令。每一步中 RNN 控制器都会输出下一步运算形态的概率分布。比如，我们可能很确定第一步要做加法，然后就要纠结第二步是该做乘法还是除法了......

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_NP_each_output.png)

现在就可以计算最终运算类型的概率分布了。相较于每一步执行单一运算，我们还是使用注意力的一般技巧，同时尝试所有运算将结果加权平均。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-10-05_NP_final_output.png)

只要我们能对运算定义导数，程序结果就概率而言是可以区分的。随后就可以定义损失，给定正确答案开始训练网络进行编程。这种方式下，神经编程机不需要正确的样例程序就能学习编程。唯一的监督项是程序产生的结果。

这就是神经编程机的核心理念了。但论文中的询问表模型不只有算术表达式，还有一些额外的技巧：

* 多类别：神经编程机中的很多运算要处理的是标量数字之外的数据类型。一些运算会输出所查表列的集合，或是选定单元格的集合。同时只有相同类别的输出才会彼此融合
* 引用输入：给一张带有人口信息的城市表，要神经编程机回答类似”有多少城市人口大于一百万？“的问题。为了简化问题，一些运算允许网络引用所答问题中的既有内容。引用通过注意力借助指针网络(pointer networks)完成。

神经编程机并不是让神经网络编程的唯一方法。Neural Programmer-Interpreter[[2]](http://arxiv.org/pdf/1511.06279.pdf)给出了另一种有趣的方法，它能完成很多有趣的任务，只是需要给出正确的程序进行监督学习。

从传统编程到神经网络还有很长的路要走，神经编程机不会是最终解决方案，但我们相信后来人能从中借鉴不少经验。

### 总结

有论文的人某种程度上比没有的人聪明些。有数学符号的人能解决没有的人不能应对的难题。计算机的使用能使我们完成远超自身能力的壮举。

总的来说，似乎很多有趣的智力形式都是人类创造、启发性的直觉与一些更细腻细致媒介，比如语言、方程，相互作用的结果。有时媒介是物理实体，为我们存储信息，避免我们犯错，亦或者执行繁重运算。其他情况下，媒介是我们脑中运作的模型。无论哪种方式都是智力的基础。

最近的机器学习成果已经有了这种趋势，将神经网络的直觉和其它一些东西结合。其中一种方法叫启发式搜索。可以用于教神经网络玩游戏，或是借助神经网络的直觉处理数学表达式。本文提到的强化 RNN 构型是另外一种路径，将 RNN 与设计好的媒介结合拓展其一般能力。

和媒介交互自然会产生一系列行为，观测和更多的行为。这就带来了挑战：我们怎么知道下一步该怎么做？这听起来很像强化学习，也当然可以采用其方法。但强化学习实际是在挑战这类问题最难的版本，同时它的解决方案并不好用。注意力了不起的地方就在于它给出了一个更简洁的答案，不同程度的采用所有行为。这行之有效是因为我们可以设计类似 NTM 记忆区这样的媒介来做些小动作并具有差异性。强化学习要求我们选一条路并从中学习。注意力选择了所有路然后将它们融合。

注意力机制的主要弱点在于我们每一步都要尝试每一种行为。这就导致在你使用神经图灵机时，计算成本同记忆区的大小呈线性关系。你或许会说让注意力稀疏些，这样就只用访问部分记忆了。但这依然有难度，因为你可能希望基于记忆内容构建注意力分布，而这本身就在迫使你遍历整个记忆。如果真能使注意力机制运行在亚线性时间内，那可就太好了！