---
layout:		post
title:  	神经结构搜索
subtitle:   
date:       2020-11-11
author:     一轩明月
header-img: img/post-bg-blue.jpg
catalog:    true
tags:
    - reinforcement learning
excerpt:    神经结构搜素（NAS）能自动完成网络结构工程，其目的在于能针对特定任务取得最佳效果的网络拓扑。NAS 技术可以分成三块：搜索空间，搜索算法和评价策略，本文由此回顾了很多有意思的想法，它们都是为了实现更好、更快、更经济的自动神经结构搜索
---

> 编译自：Neural Architecture Search， [Lilian Weng](https://lilianweng.github.io/lil-log/)

尽管大多数既成功又流行的模型架构是人类专家设计出来的，但并不意味着我们已经穷尽了网络结构的可能性并找到了最优项。如果采用一种系统化、自动化的方式学习高性能模型架构，我们找到最佳选择的机会会更大些。

自动学习和演化网络拓扑并不是个新思想（[Stanley 和 Miikkulainen, 2002](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)）。近些年，[Zoph 和 Le 2017](https://arxiv.org/abs/1611.01578) 和 [Baker 等人, 2017](https://arxiv.org/abs/1611.02167) 在神经结构搜索（Neural Architecture Search，NAS）领域所做的开创性工作引起了人们的广泛关注。也由此推动了许多更好、更快、更经济的 NAS 方法的诞生。

要涉猎 NAS，[Elsken 等人, 2019](https://arxiv.org/abs/1808.05377) 所做的综述非常不错，他们将 NAS 拆解成由三个主要部分组成的系统，简单明了且常为其他 NAS 引用。

1. **搜索空间**：NAS 搜索空间定义了一系列操作（比如卷积、全连接，池化），并指出怎么拼接这些操作形成有效网络结构。搜索空间的设计往往需要人类经验，也就不可避免的会带有人类偏见。
2. **搜索算法**：NAS 搜索算法会在诸多网络结构候选项中进行采样。它以子模型性能标准为回报（比如准度高，延迟小），尽可能找到最佳高性能结构候选。
3. **评价策略**：要测算、估计或预测一众被举荐子模型的性能，这样才能得到搜索算法学习所需的反馈。候选人评估过程可能会开销很大，人们也已经设计了很多新方法节省时间或计算资源。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NAS-high-level.png)

*图 1  神经结构搜索（NAS）的三个主要部分（图片来源：[Elsken 等, 2019](https://arxiv.org/abs/1808.05377))*

### 搜索空间

NAS 搜索空间定义了一系列网络基础操作，并规定怎么组合操作来构建有效网络结构。

#### 顺次层向操作

构建神经网络结构搜索空间的最原始方式是描绘网络拓扑，要么 CNN 要么 RNN，带着一系列 *顺次层向操作（sequential layer-wise operations）*，可参考早期工作 [Zoph 与 Le 2017](https://arxiv.org/abs/1611.01578) 和 [Baker 等人, 2017](https://arxiv.org/abs/1611.02167)。网络表示的序列需要大量专业知识，因为每个操作都与不同层的参数关联，这样的联结需要硬编码。比如，`conv` 操作之后模型应该输出核大小，步幅等信息；又或者在 `FC` 操作之后我们需要知道下一轮预测的单元数量。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NAS-search-space.png)

*图 2  （上）CNN 的序列表示（下）循环单元树结构的序列表示（图片来源：[Zoph 和 Le 2017](https://arxiv.org/abs/1611.01578)）*

要保证得到的结构有效，还得加点规则（[Zoph 和 Le 2017](https://arxiv.org/abs/1611.01578)）：

- 如果某层没与任何输入层相连，那就将其作为输入层；
- 在最后一层，把所有没被连接的输出结果连起来；
- 如果某一层有多个输入层，那将所有输入层按深度连接起来
- 如果待连接输入层大小不一，较小的层用 0 填充使尺寸统一

用类似注意力的机制，跳跃连接关系也能预测。在 $$i$$ 层会在锚点上加上 $$i-1$$ 个基于内容的 sigmoid 信息，以此表示与前面哪些层有关联。每个 sigmoid 函数以当前节点 $$h_i$$ 的隐态和之前 $$i-1$$ 个节点 $$h_j,j=1,\dots,i-1$$ 作为输入。


$$
P(\text{j 层是 i 层的输入}) = \text{sigmoid}(v^\top \tanh(\mathbf{W}_\text{prev} h_j + \mathbf{W}_\text{curr} h_i))
$$



顺次搜索空间的表示能力很强，但它太大了，要详尽覆盖搜索空间要花费巨量计算资源。[Zoph 和 Le 2017](https://arxiv.org/abs/1611.01578) 的实验用了 800 块 GPU 并行跑了 28 天，[Baker 等人, 2017](https://arxiv.org/abs/1611.02167) 对搜索空间加了点限制，最多不超过 2 个 `FC` 层。

#### 单元表示

受已有建树的视觉模型架构（如 Inception，ResNet）启发，特别是复用组件这一设计思路影响，*NASNet 搜索空间*（[Zoph 等 2018](https://arxiv.org/abs/1707.07012)）设计了一种卷积网络结构，相同单元重复许多次，每个单元里面包含若干 NAS 算法给出的运算操作。设计良好的组件单元具备数据集间的可迁移性。通过调整单元数量也很容易调小或增大模型。

准确地讲，NASNet 搜索空间要学两类单元来构建网络：

1. *一般单元（Normal Cell）*：输入输出的特征维数相同
2. *缩放单元（Reduction Cell）*：输出特征的宽高减半

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NASNet-search-space.png)

*图 3  NASNet 搜索空间构建了由一叠重复单元组成的结构。单元框架通过 NAS 算法优化（图片来源：[Zoph 等 2018](https://arxiv.org/abs/1707.07012)）*

每个单元的预测被分组归入 $$B$$  个区块（NASNet 论文里 $$B=5$$），每个区块里有 5 步预测，预测由 5 个互不相同的 softmax 分类器给出，代表区块提供的几种选择。注意 NASNet 搜索空间在单元间不存在残差连接，模型仅在区块内部学习跳跃连接关系。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_cell-prediction-steps.png)

*图 4  （a）每个单元包括 $$B$$ 个区块，每个区块由 5 个独立决策决定（b）每步决策都能选什么操作的具体例子*

实验中人们发现了一个 [*DropPath*](https://arxiv.org/abs/1605.07648) 变种，起名叫 *ScheduledDropPath*，这显著改善了最终 NASNet 实验性能。DropPath 会以固定概率随机丢掉一些路径（也就是 NASNet 里通过操作连接的边）。ScheduledDropPath 会随时间线性增大路径丢弃概率。

[Elsken 等人，2019](https://arxiv.org/abs/1808.05377) 指出 NASNet 搜索空间主要有三大优势：

1. 大幅缩减了空间规模
2. 基于[样式](https://en.wikipedia.org/wiki/Network_motif)的架构很容易用到不同数据集上
3. 证明架构工程中重复堆叠模组这一设计模式确实有用。比如，可以在 CNN 里堆叠残差块，在 Transformer 里堆叠多头注意力块来构建强力模型

#### 层次结构

要想利用已知的精心设计过的网络[样式](https://en.wikipedia.org/wiki/Network_motif)，NAS 搜索空间可以强制转为层次结构，就像在*分层 NAS（Hierarchical NAS，HNAS）*（[Liu 等人，2017](https://arxiv.org/abs/1711.00436)） 中一样。起初是一小撮初始运算，包括卷积，池化，单位阵等在内的单独操作，然后由初始运算构成的小子图（或“样式”）递归地构建上层计算图。

在 $$\ell=1, \dots, L$$  层的计算样式可以表示为 $$(G^{(\ell)}, \mathcal{O}^{(\ell)})$$，其中：

- $$\mathcal{O}^{(\ell)}$$ 是一组操作，$$\mathcal{O}^{(\ell)} = \{ o^{(\ell)}_1, o^{(\ell)}_2, \dots \}$$
- $$G^{(\ell)}$$ 是邻接矩阵，条目 $$G_{ij}=k$$ 表示结点 $$i$$ 和 $$j$$ 间的操作为 $$o^{(\ell)}_k$$，结点索引遵循 DAG 中的[拓扑顺序](https://en.wikipedia.org/wiki/Topological_sorting)，索引从 $$1$$ 开始，最大的索引对应沉寂点

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_hierarchical-NAS-search-space.png)

*图 5  （上）三个 1 级初始运算构成一个 2 级样式（下）将三个 2 级样式插入基础网络集成为一个 3 级样式（图片来源：[Liu 等人，2017](https://arxiv.org/abs/1711.00436)）*

要想按层级结构搭个网络，要从最低的 $$\ell=1$$ 级开始，递归地定义 $$\ell$$ 级的第 $$m$$ 个样式：


$$
o^{(\ell)}_m = \text{assemble}\Big( G_m^{(\ell)}, \mathcal{O}^{(\ell-1)} \Big)
$$


层次表示变成 $$\Big( \big\{ \{ G_m^{(\ell)} \}_{m=1}^{M_\ell} \big\}_{\ell=2}^L, \mathcal{O}^{(1)} \Big), \forall \ell=2, \dots, L$$，其中 $$\mathcal{O}^{(1)}$$ 里是一系列初始操作。

$$\text{assemble}()$$ 过程等价于顺次计算结点 $$i$$ 的特征映射，按拓扑顺序把它之前的结点 $$j$$ 的所有特征映射聚集起来：


$$
x_i = \text{merge} \big[ \{ o^{(\ell)}_{G^{(\ell)}_{ij}}(x_j) \}_{j < i} \big], i = 2, \dots, \vert G^{(\ell)} \vert
$$


论文里 $$\text{merge}[]$$ 是按深度连接的。

和 NASNet 一样，[Liu 等人，2017](https://arxiv.org/abs/1711.00436)的实验目的在于在预先定义好的带重复组件的”宏“结构中找到较好的单元结构，实验显示就算用简单的搜索方法（比如随机搜索或演化算法），精心设计过的搜索空间也能显著增强其性能。

[Cai 等人，2018b](https://arxiv.org/abs/1806.02639)提出了一种树结构的搜索空间，用上了路径层网络转换。树中每个结点都设有给子结点划拨输入的 *分配（allocation）* 计划，和组合子结点结果的 *整合（merge）* 计划。如果某一层加了或连了整合计划，路径层网络转换能将其划成一个多叉样式。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_path-level-network-transformations.png)

*图 6  用路径层转换将单一层转换为树结构样式（图片来源：[Cai 等人，2018b](https://arxiv.org/abs/1806.02639)）*

#### 记忆库表示

前馈网络的记忆库表示方法是[Brock 等人，2017](https://arxiv.org/abs/1708.05344) 在 [SMASH](# 从预测出发) 中提出的。相较于运算图，作者将神经网络视作有多个可读写的记忆块的系统。每层操作被设计为：（1）从记忆块子集中读取；（2）计算结果；（3）将结果写入另一子集。比如在序列模型中，会不断从单个记忆块中读取与覆盖。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NAS-memory-bank-view-representation.png)

*图 7  几个流行网络架构块的记忆库表示（图片来源： [Brock 等人，2017](https://arxiv.org/abs/1708.05344)）*

### 搜索算法

NAS 搜索算法会在一众子网络中采样，以子模型性能度量结果为回报，学着去得到高性能架构候选项。与超参数搜索里的做法如出一辙。

#### 随机搜索

随机搜索是最原始的法子了，其 *随机* 从搜索空间中抽取有效结构，且其中不涉及学习模型。随机搜索的绝佳效果已经在超参数搜索中得到了验证（[Bergstra 与 Bengio 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)），如果搜索空间设计好的坏，随机搜索定下的性能底线可是很能打的。

#### 强化学习

最开始的 **NAS**（[Zoph 和 Le 2017](https://arxiv.org/abs/1611.01578)） 设计中有一个基于 RL 的控制器，决定选用哪个子模型架构来评估。控制器是个 RNN，输出一个变长标识序列来配置网络架构。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NAS.png)

*图 8  NAS 的高层视角，有一个 RNN 控制器和一条评估子模型的管道（图片来源：[Zoph 和 Le 2017](https://arxiv.org/abs/1611.01578)）*

控制器的训练是当作一个强化学习任务来看的，用 [REINFORCE](https://libertydream.github.io/2020/07/19/策略梯度算法/#reinforce)。

- **行动空间**：行动空间就是一个标识列表，这些标识定义了由控制器给出的子网络（见[上文](# 顺次层向操作)）是个什么样子。控制器会输出*行动（action）* $$a_{1:T}$$，其中 $$T$$ 是标识总数
- **回报**：收敛得到的子网络准确率就是训练控制器的回报 $$R$$
-  **损失**：NAS 用 REINFORCE 损失来优化控制器参数 $$\theta$$ 。我们希望用下面给出的梯度使期望收益最大化（高准确率）。这用策略梯度的好处在于即使回报不可微算法仍旧有效。


$$
\nabla_{\theta} J(\theta) = \sum_{t=1}^T \mathbb{E}[\nabla_{\theta} \log P(a_t \vert a_{1:(t-1)}; \theta) R ]
$$



**MetaQNN** [Baker 等人，2017](https://arxiv.org/abs/1611.02167)会训练一个代理，用 [Q 学习](https://libertydream.github.io/2020/07/05/强化学习综述/#-q-学习非策略-td-控制)法有序选择 CNN 层，还用上了 ε-greedy 探索策略和经历回放。回报就是验证准确率。



$$
Q^{(t+1)}(s_t,  a_t) = (1 - \alpha)Q^{(t)}(s_t, a_t) + \alpha (R_t + \gamma \max_{a \in \mathcal{A}} Q^{(t)}(s_{t+1},  a'))
$$


其中状态 $$s_t$$ 是个层操作和相关参数构成的元组。行动 $$a$$ 决定了操作间的连接关系。Q 值大小与我们对能带来高准确度的两个关联操作的信心成正比。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_MetaQNN.png)

*图 9  MetaQNN 概览——用 Q 学习设计 CNN（图片来源：[Baker 等人，2017](https://arxiv.org/abs/1611.02167)）*

#### 演化算法

**增强拓扑神经演化**（**NEAT**，*NeuroEvolution of Augmenting Topologies*）是种用[遗传算法(GA)](https://en.wikipedia.org/wiki/Genetic_algorithm)演化神经网络拓扑的方法，在 2002 年由 [Stanley 和 Miikkulainen](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf) 提出。NEAT 会将连接权重和网络拓扑一起推演。每段基因内都会编入配置网络的全部信息，包括结点权重和边。目标总体会随着权重和连接的变异，双亲基因的交叉而增多。更多有关神经演化的内容可以看这篇深度综述 [Stanley 等人，2019](https://www.nature.com/articles/s42256-018-0006-z)。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_NEAT-mutations.png)

*图 10  NEAT 算法中的突变（图片来源：[Stanley 和 Miikkulainen，2002](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)）*

[Real 等人，2018](https://arxiv.org/abs/1802.01548) 用演化算法（EA）来搜索高性能网络结构，命名为 **AmoebaNet**。作者采用了[锦标赛选举](https://en.wikipedia.org/wiki/Tournament_selection)法，该方法每轮会从一组随机样本中选择最好的那一项并将其变异后代放回目标总体。若锦标赛规模为 $$1$$，就等价于随机选择。

AmoebaNet 改进了锦标赛选举规则，使其对较年轻的基因型更友好，同时每轮都要丢掉最老的模型。该方法叫 *陈化演进（aging evolution）*，使 AmobebaNet 覆盖和探索更多的搜索空间，而不是过早的缩小到好性能模型上。

更准确地讲，陈化演进下的每轮锦标赛选举中：

1.  从目标总体中采样 $$S$$ 个模型，准确率最高的当作*亲代（parent）*
2.  通过亲代变异获得 *后代（child）* 模型
3.  接着训练、评估子模型并将其加入到目标总体中
4.  从目标总体中删除最老的模型

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_aging-evolution-algorithm.png)

*图 11  陈化演进算法（图片来源：[Real 等人，2018](https://arxiv.org/abs/1802.01548)）*

用到的变异有两类：

1. *隐态变异（Hidden state mutation）*：随机选一个结对组合并重设随机一端使图中无环
2. *操作变异（Operation mutation）*：随机将一个既有操作随机换成另一个

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_AmoebaNet-mutations.png)

图 12  AmoebaNet 中的两类变异（图片来源：[Real 等人，2018](https://arxiv.org/abs/1802.01548)）

作者进行的实验中 EA 和 RL 在最终验证准确率上表现一样好，但 EA 在任意时刻上的性能会更好而且能找到更小的模型。这里在 NAS 中用 EA 计算开销还是很大，每个实验都是用 450 个 GPU 跑了 7 天。

**HNAS**（[Liu 等人，2017](https://arxiv.org/abs/1711.00436)） 也是用演化算法（原始锦标赛选举）当作他们的搜索策略。层次结构搜索空间中，每条边都是一个操作，所以他们实验中的基因型变异就是用一个不同的操作随机替换掉一条边。替换集中包括 `none` 选项，所以它能修改、删除和添加一条边。他们通过在“平凡”样式（所有单位映射，即输入=输出）上进行大量随机变异构建初始基因型组。

#### 渐进决策过程

构建一个模型结构是个有序过程，加入的每个运算或层都会带来额外的复杂度。如果我们引导搜索模型从简单模型开始调研，逐步演化到更复杂的架构上，这很像把“课程”引进到搜索模型的学习过程中了。

渐进 NAS（Progressive NAS，**PNAS**； [Liu 等人，2018](https://arxiv.org/abs/1712.00559)）将 NAS 的问题定为搜索模型的复杂度逐步增加的一个过程。相较于 RL 或 EA，PNAS 采用基于序列模型的贝叶斯优化（Sequential Model-based Bayesian Optimization，SMBO) 法作为搜索策略。PNAS 的工作模式类似 A* 搜索，因为它由易到难的搜索模型的同时还在学习一个指导搜索的代理函数。

> [A* 搜索算法](https://en.wikipedia.org/wiki/A*_search_algorithm)（“最佳优先搜索”）是个很流行的寻路算法。寻路问题就是在一张带权图中，找到一条从特定起点到给定终点开销最小的路径。每轮迭代，A* 都会通过最小化 $$f(n)=g(n)+h(n)$$ 来扩张路径，其中 $$n$$ 是下一个结点，$$g(n)$$ 是从起点到 $$n$$ 的开销，而 $$h(n)$$ 是负责估计从 $$n$$ 点到目标点最小开销的启发式函数。

PNAS 采用 [NASNet](# 单元表示) 搜索空间。每个块被设计成一个包含 5 元素的元组，并且 PNAS 只取元素间加法作为步骤 5 的组合运算，不包括联结。不同的是，相较于设定固定 $$B$$ 块区块，PNAS 从 $$B=1$$ 开始，模型里每个单元只有一个区块，逐步增大 $$B$$。

验证集上的表现就是 *代理（surrogate）*模型训练过程中的反馈，其负责*预估（predict）*新架构的性能。借此预测器，可以决定接下来应该先评估哪个模型。因为性能预测器应该能处理各种尺寸的输入，准确率和样本效率，作者最终用了个 RNN 模型。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_progressive-NAS-algorithm.png)

*图 13  渐进 NAS 算法（图片来源：[Liu 等人，2018](https://arxiv.org/abs/1712.00559)）*

#### 梯度下降

要用梯度下降更新结构搜索模型，需要使选择离散运算的过程可微。这些方法一般会把结构参数和网络权重的学习打包到一块扔到模型里。更多内容见下面[“单样本”方法](# 单样本法：搜索+评价)一节

### 评价策略

我们需要测量，估计或者说预测每个子模型的性能，以此获取反馈来优化搜索算法。候选项的评估过程可能开销会非常大，由此也诞生了很多新评估方法来节约算力，节省时间。评估子模型的时候，我们最关心的是它在验证集上的准确率。近期研究则开始关注模型的其他要素，比如模型大小和延迟，因为特定设备可能内存有限或需要响应够快。

#### 从头训练

最原始的做法是从头独立训练每个子网络直到 *收敛（convergence）*，然后算下它在验证集上的准确率（[Zoph 和 Le 2017](https://arxiv.org/abs/1611.01578)）。这确实能得到可靠的性能数字，但训练——收敛——评估一整圈下来只给 RL 控制器的训练贡献了一个数据样本（更别说 RL 是出了名的样本效率差）。所以就计算开销而言这么做代价太大了。

#### 代理任务性能

有些方法用代理任务的性能表现作为子网络的性能估计，这一般更经济，统计起来也更快：

- 在小规模数据集上训练
- 训练轮次更少
- 在搜索阶段训练并评估一个砍了规模的模型。比如，一旦学完了一个单元结构，我们可以大量进行单元复制或增大过滤器数量（[Zoph 等人，2018](https://arxiv.org/abs/1707.07012)）
- 预测学习曲线。[Baker 等人，2018](https://arxiv.org/abs/1705.10823) 对验证准确率的预测进行了建模，将其看作是一个时序回归问题。回归模型（$$\nu$$ 支持向量机回归；$$\nu$$-SVR）的特征包括每轮早期序列的准确率，架构参数和超参数

#### 参数共享

如果不从头挨个训练子模型，你可能会想，在他们中间构造依赖关系再找个方法复用权重怎么样？一些研究者还真这么做并成功了。

受 [Net2Net](https://arxiv.org/abs/1511.05641) 转换启发， [Cai 等人，2017](https://arxiv.org/abs/1707.04873) 提出了 *高效结构搜索（Efficient Architecture Search，**EAS**）*。EAS 有个 RL 代理，称为元控制器，负责预测想保留功能需要进行的网络转换由此扩大网络深度或加大层的宽度。因为网络是逐步增大的，之前验证过的网络的权重可以在后续探索中复用。有了继承来的权重，新建网络只需要些轻度训练即可。

 $$\omega$$   $$\theta$$:

1.  $$\theta$$   $$R(\mathbf{m}, \omega)$$ 
2.  $$\omega$$ 

#### 从预测出发

### 单样本法：搜索+评价

 $$r^{1/k}$$  $$0 < r < 1$$  $$k$$ i

 $$N$$  $$x_i$$  $$(i, j)$$  $$o^{(i,j)} \in \mathcal{O}$$  $$x_j$$  $$x_i$$
$$
x_i = \sum_{j < i} o^{(i,j)}(x_j)
$$
s $$\alpha = \{ \alpha^{(i,j)} \}$$.
$$
\bar{o}^{(i,j)}(x) = \sum_{o\in\mathcal{O}} \frac{\exp(\alpha_{ij}^o)}{\sum_{o'\in\mathcal{O}} \exp(\alpha^{o'}_{ij})} o(x)
$$
 $$\alpha_{ij}$$  $$\vert \mathcal{O} \vert$$, $$i$$ and $$j$$ 
$$
\begin{aligned}
\min_\alpha & \mathcal{L}_\text{validate} (w^*(\alpha), \alpha) \\
\text{s.t.} & w^*(\alpha) = \arg\min_w \mathcal{L}_\text{train} (w, \alpha)
\end{aligned}
$$
 $$k$$,  $$\alpha_{k−1}$$,  $$w_k$$  $$w_{k−1}$$  $$\mathcal{L}_\text{train}(w_{k−1}, \alpha_{k−1})$$ e $$\xi$$.  $$w_k$$ 
$$
J_\alpha = \mathcal{L}_\text{val}(w_k - \xi \nabla_w \mathcal{L}_\text{train}(w_k, \alpha_{k-1}), \alpha_{k-1})
$$
 $$w^∗(\alpha)$$. 
$$
\begin{aligned}
\text{Let }w'_k &= w_k - \xi \nabla_w \mathcal{L}_\text{train}(w_k, \alpha_{k-1}) & \\
J_\alpha &= \mathcal{L}_\text{val}(w_k - \xi \nabla_w \mathcal{L}_\text{train}(w_k, \alpha_{k-1}), \alpha_{k-1}) = \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) & \\
\nabla_\alpha J_\alpha 
&= \nabla_{\alpha_{k-1}} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) \nabla_\alpha \alpha_{k-1} + \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1})\nabla_\alpha w'_k & \\& \text{; multivariable chain rule}\\
&= \nabla_{\alpha_{k-1}} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) + \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) \big( - \xi \color{red}{\nabla^2_{\alpha, w} \mathcal{L}_\text{train}(w_k, \alpha_{k-1})} \big) & \\
&\approx \nabla_{\alpha_{k-1}} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) - \xi \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) \color{red}{\frac{\nabla_\alpha \mathcal{L}_\text{train}(w_k^+, \alpha_{k-1}) - \nabla_\alpha \mathcal{L}_\text{train}(w_k^-, \alpha_{k-1}) }{2\epsilon}} & \\
& \text{; apply numerical differentiation approximation}
\end{aligned}
$$
$$w_k^+ = w_k + \epsilon \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1})$$  $$w_k^- = w_k - \epsilon \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1})$$.

 $$N=7$$  $$N$$.

 $$G$$ $$G_{ij}$$  $$i$$ and $$j$$ a  $$\vert \mathcal{O} \vert$$   $$\mathcal{O} = \{ o_1, \dots \}$$. 

 $$m_\mathcal{O}(x)$$  $$\alpha$$  $$\vert \mathcal{O} \vert$$. $$\alpha$$
$$
\begin{aligned}
m^\text{one-shot}_\mathcal{O}(x) &= \sum_{i=1}^{\vert \mathcal{O} \vert} o_i(x) \\
m^\text{DARTS}_\mathcal{O}(x) &= \sum_{i=1}^{\vert \mathcal{O} \vert} p_i o_i(x) = \sum_{i=1}^{\vert \mathcal{O} \vert} \frac{\exp(\alpha_i)}{\sum_j \exp(\alpha_j)} o_i(x) \\
m^\text{binary}_\mathcal{O}(x) &= \sum_{i=1}^{\vert \mathcal{O} \vert} g_i o_i(x) = \begin{cases}
o_1(x) & \text{with probability }p_1, \\
\dots &\\
o_{\vert \mathcal{O} \vert}(x) & \text{with probability }p_{\vert \mathcal{O} \vert}
\end{cases} \\
\text{ where } g &= \text{binarize}(p_1, \dots, p_N) = \begin{cases}
[1, 0, \dots, 0] & \text{with probability }p_1, \\
\dots & \\
[0, 0, \dots, 1] & \text{with probability }p_N. \\
\end{cases}
\end{aligned}
$$

1.  $$m^\text{binary}_\mathcal{O}(x)$$. 
2.  $$\alpha$$, $$w$$, $$\alpha$$  $$\partial \mathcal{L} / \partial g_i$$   $$\partial \mathcal{L} / \partial g_i$$ 

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \alpha_i} 
&= \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial p_j} \frac{\partial p_j}{\partial \alpha_i} 
\approx \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial g_j} \frac{\partial p_j}{\partial \alpha_i} 
= \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial g_j} \frac{\partial \frac{e^{\alpha_j}}{\sum_k e^{\alpha_k}}}{\partial \alpha_i} \\
&= \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial g_j} \frac{\sum_k e^{\alpha_k} (\mathbf{1}_{i=j} e^{\alpha_j}) - e^{\alpha_j} e^{\alpha_i} }{(\sum_k e^{\alpha_k})^2}
= \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial g_j} p_j (\mathbf{1}_{i=j} -p_i)
\end{aligned}
$$

 $$\partial \mathcal{L} / \partial g_i$$ $$o_i(x)$$,  $$\vert \mathcal{O} \vert$$ 

 $$\mathbb{E}[\text{latency}] = \sum_j p_j F(o_j)$$,   $$F(.)$$ i

### 未来远景

### 附录：NAS 论文总结

<table class="info">
    <thead>
        <tr>
            <th>Model name</th>
            <th>Search space</th>
            <th>Search algorithms</th>
            <th>Child model evaluation</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><a href="http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf">NEAT (2002)</a></td>
            <td>-</td>
            <td>Evolution (Genetic algorithm)</td>
            <td>-</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1611.01578">NAS (2017)</a></td>
            <td>Sequential layer-wise ops</td>
            <td>RL (REINFORCE)</td>
            <td>Train from scratch until convergence</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1611.02167">MetaQNN (2017)</a></td>
            <td>Sequential layer-wise ops</td>
            <td>RL (Q-learning with $\epsilon$-greedy)</td>
            <td>Train for 20 epochs</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1711.00436">HNAS (2017)</a></td>
            <td>Hierarchical structure</td>
            <td>Evolution (Tournament selection)</td>
            <td>Train for a fixed number of iterations</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1707.07012">NASNet (2018)</a></td>
            <td>Cell-based</td>
            <td>RL (PPO)</td>
            <td>Train for 20 epochs</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1802.01548">AmoebaNet (2018)</a></td>
            <td>NASNet search space</td>
            <td>Evolution (Tournament selection with aging regularization)</td>
            <td>Train for 25 epochs</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1707.04873">EAS (2018a)</a></td>
            <td>Network transformation</td>
            <td>RL (REINFORCE)</td>
            <td>2-stage training</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1712.00559">PNAS (2018)</a></td>
            <td>Reduced version of NASNet search space</td>
            <td>SMBO; Progressive search for architectures of increasing complexity</td>
            <td>Train for 20 epochs</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1802.03268">ENAS (2018)</a></td>
            <td>Both sequential and cell-based search space</td>
            <td>RL (REINFORCE)</td>
            <td>Train one model with shared weights</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1708.05344">SMASH (2017)</a></td>
            <td>Memory-bank representation</td>
            <td>Random search</td>
            <td>HyperNet predicts weights of evaluated architectures.</td>
        </tr>
        <tr>
            <td><a href="http://proceedings.mlr.press/v80/bender18a.html">One-Shot (2018)</a></td>
            <td>An over-parameterized one-shot model</td>
            <td>Random search (zero out some paths at random)</td>
            <td>Train the one-shot model</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1806.09055">DARTS (2019)</a></td>
            <td>NASNet search space</td>
            <td colspan="2">Gradient descent (Softmax weights over operations)</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1812.00332">ProxylessNAS (2019)</a></td>
            <td>Tree structure architecture</td>
            <td colspan="2">Gradient descent (BinaryConnect) or REINFORCE</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1812.09926">SNAS (2019)</a></td>
            <td>NASNet search space</td>
            <td colspan="2">Gradient descent (concrete distribution)</td>
        </tr>
    </tbody>
</table>








![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_EAS-meta-controller.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_ENAS-example.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_SMASH-algorithm.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_SMASH-error-correlation.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_one-shot-model-architecture.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_one-shot-model-accuracy-correlation.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_DARTS-illustration.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_DARTS-algorithm.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_proxylessNAS-training.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_ProxylessNAS-latency.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_AutoML-zero-evaluation.png)



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-11-11_AutoML-zero-progress.png)