---
layout:		post
title:  	测试驱动机器学习开发
subtitle:   模型部署系列之七
date:       2020-05-31
author:     一轩明月
header-img: img/post-bg-future.jpg
catalog:    true
tags:
    - production
excerpt:    
---

> 编译自：Test-Driven Machine Learning Development，[Luigi](https://mlinproduction.com/author/luigi/)

机器学习部署系列中，上一篇文章介绍了模型注册怎样做到一石多鸟的，包括存储模型谱系，模型版本和配置信息。比如，可以用模型注册系统查询某个序列化模型存在哪里。注册系统同时还能指明给定模型的 _开发阶段_，可以查询了解哪个版本的训练模型处于开发阶段，又是哪个版本要在运行时生成预测。类似于新软件的部署过程，新近训练过的模型在升至 _筹划_ 乃至 _生产_  就绪阶段前，要先从 _开发_  起步。

但怎样决定何时提升模型走过这一周期？还是站在巨人的肩膀上从软件开发中偷师借鉴点东西吧。传统软件开发中，代码必须经过各式各样的测试才能上生产线。比如[单元测试]()负责检测特定代码单元，而[集成测试](https://martinfowler.com/articles/practical-test-pyramid.html#IntegrationTests)则要测试你的 app 和外围其他应用的集成度。

机器学习模型当然也要经过测试才能上线，这很重要。模型经过充分测试你就能有信心模型能如期运作，但该语境下“测试”到底意味着什么呢？本文会给出模型测试定义并介绍几种使用离线测试的模型测验方法。这些测试策略可使我们实现模型部署闭环，将 CI/CD 应用到机器学习上。

# 测试驱动开发

如 Martin Fowler 所言，[测试驱动开发（Test-Driven Development，TDD）](https://martinfowler.com/bliki/TestDrivenDevelopment.html)是一种软件建构技术，通过写测试来指导软件开发。该过程就简单的三个步骤，不断重复直到项目完成。

1. 给想添加的功能写测试
2. 写功能代码知道测试通过
3. 重构所有代码优化结构

举个简单例子，假设你想写个方法接收一个数字列表，将数字的平方数列表返回

```python
def square_list(numbers):
    '''
    接收数值列表，对数值求平方后以列表方式返回
    '''
    pass
```

实现功能前，要先写测试，_假定功能实现正确_ 其应当返回预期结果。`square_list` 方法的测试样例包括：

- 1）如果参数 `numbers` 并非列表要抛出异常
- 2）返回列表应当和 `numbers` 等长
- 3）返回列表中索引 `j` 处的数等于参数 `numbers` 中 `j` 处数的平方

这些只是可写测试的一小部分。比如，测试三假定 `numbers` 中的每一项都是数字，结合上下文，当然可以写个测试核对该条件是否成立。

指明并写好测试用例后，接着就来实现 `square_list` 方法。当方法通过了所有测试，就能肯定我们的代码实现了预期功能，同时也有了一组测试能防止代码库回退。这也正是测试驱动开发的主要优点：写出良好测试过的代码，而不仅仅是代码。

# 测试机器学习模型

和常规软件一样，机器学习模型部署前不许要进行验证，通过这些验证或测试才能保证高质量交付预测结果。预估性能差劲的模型可能会给用户和组织带来灾难性后果，低劣的歌曲推荐系统会使听众不满，自动驾驶系统中的物体探测器如果不精确则有可能招致死亡。显然部署模型到线上前应当竭尽所能提防此类错误出现。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-07-09_test-pyramid.png)

## 1. 保留数据集上的性能

首先要验证的是测算模型在保留数据集上的预估性能，并与预置最小可接受阈值进行比对