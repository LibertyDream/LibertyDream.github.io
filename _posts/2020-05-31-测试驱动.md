---
layout:		post
title:  	测试驱动机器学习开发
subtitle:   模型部署系列之七
date:       2020-05-31
author:     一轩明月
header-img: img/post-bg-future.jpg
catalog:    true
tags:
    - production
excerpt:    
---

> 编译自：Test-Driven Machine Learning Development，[Luigi](https://mlinproduction.com/author/luigi/)

机器学习部署系列中，上一篇文章介绍了模型注册怎样做到一石多鸟的，包括存储模型谱系，模型版本和配置信息。比如，可以用模型注册系统查询某个序列化模型存在哪里。注册系统同时还能指明给定模型的 _开发阶段_，可以查询了解哪个版本的训练模型处于开发阶段，又是哪个版本要在运行时生成预测。类似于新软件的部署过程，新近训练过的模型在升至 _筹划_ 乃至 _生产_  就绪阶段前，要先从 _开发_  起步。

但怎样决定何时提升模型走过这一周期？还是站在巨人的肩膀上从软件开发中偷师借鉴点东西吧。传统软件开发中，代码必须经过各式各样的测试才能上生产线。比如[单元测试]()负责检测特定代码单元，而[集成测试](https://martinfowler.com/articles/practical-test-pyramid.html#IntegrationTests)则要测试你的 app 和外围其他应用的集成度。

机器学习模型当然也要经过测试才能上线，这很重要。模型经过充分测试你就能有信心模型能如期运作，但该语境下“测试”到底意味着什么呢？本文会给出模型测试定义并介绍几种使用离线测试的模型测验方法。这些测试策略可使我们实现模型部署闭环，将 CI/CD 应用到机器学习上。

# 测试驱动开发

如 Martin Fowler 所言，[测试驱动开发（Test-Driven Development，TDD）](https://martinfowler.com/bliki/TestDrivenDevelopment.html)是一种软件建构技术，通过写测试来指导软件开发。该过程就简单的三个步骤，不断重复直到项目完成。

1. 给想添加的功能写测试
2. 写功能代码知道测试通过
3. 重构所有代码优化结构

举个简单例子，假设你想写个方法接收一个数字列表，将数字的平方数列表返回

```python
def square_list(numbers):
    '''
    接收数值列表，对数值求平方后以列表方式返回
    '''
    pass
```

实现功能前，要先写测试，_假定功能实现正确_ 其应当返回预期结果。`square_list` 方法的测试样例包括：

- 1）如果参数 `numbers` 并非列表要抛出异常
- 2）返回列表应当和 `numbers` 等长
- 3）返回列表中索引 `j` 处的数等于参数 `numbers` 中 `j` 处数的平方

这些只是可写测试的一小部分。比如，测试三假定 `numbers` 中的每一项都是数字，结合上下文，当然可以写个测试核对该条件是否成立。

指明并写好测试用例后，接着就来实现 `square_list` 方法。当方法通过了所有测试，就能肯定我们的代码实现了预期功能，同时也有了一组测试能防止代码库回退。这也正是测试驱动开发的主要优点：写出良好测试过的代码，而不仅仅是代码。

# 测试机器学习模型

和常规软件一样，机器学习模型部署前不许要进行验证，通过这些验证或测试才能保证高质量交付预测结果。预估性能差劲的模型可能会给用户和组织带来灾难性后果，低劣的歌曲推荐系统会使听众不满，自动驾驶系统中的物体探测器如果不精确则有可能招致死亡。显然部署模型到线上前应当竭尽所能提防此类错误出现。

如前文所述，TDD 主要就是在实现应用功能前先写好测试用例。但写用例首先要假定你知道什么要测试，这（一般）在确定性系统中很直观，如 `square_list` 的例子所展示的那样。但不像传统软件，机器学习模型是不确定的，这在 [ML 测试评分](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf)一文中说的清楚：

> ML 系统测试相较测试人工编码系统来的更复杂，因为 ML 系统行为高度依赖数据和模型，而这些却不能先验明确指定

所以要怎么测试我们的模型呢？要怎样规避机器学习的统计特征，回避无法预知会碰到什么未明数据的事实？来审视一下写机器学习模型测试的三种方式。尤其是有助于实现模型部署过程自动化的这些测试用例。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-07-09_test-pyramid.png)

## 1. 保留数据集上的性能

首先要验证的是测算模型在保留数据集上的预估性能，并与预定的最小可接受阈值进行比对。在[产品规划阶段](https://www.jeremyjordan.me/ml-requirements/)，产品和数据科学团队应合伙商讨确定性能标准和最低性能门槛。该标准/阈值的组合确立了一个模型在部署上线前所应达到的最低性能水平。

尽管这是最广为人知且最容易实现的模型测试，还是有几点要注意。标准/阈值组合一旦确定，数据划分策略也就随之而定了。和数据科学训练营与在线教程教授你的不同，适宜地将数据集划分成训练集、验证集和测试集所要求的比随机均匀分割复杂得多。划分过程是问题导向的：分割应该模拟模型在推理时所要面对的场景。

一个典型的例子是对时间相关的数据进行预测。比如潜在顾客评分模型倾向于服务新顾客（系列[文章一](https://libertydream.github.io/2020/02/23/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%88%B0%E5%BA%95%E6%98%AF%E5%9C%A8%E8%AF%B4%E4%BB%80%E4%B9%88/)中的例子），随机划分数据集会得到存有偏见的错误估计，因为我们期望的是对新生成的顾客评分进行排序，模型应该在旧人群评分上进行训练，而在新评分上进行验证。

其他问题还有[数据泄露](https://mlinproduction.com/data-leakage/)，隐式[反馈循环](https://medium.com/@rchang/getting-better-at-machine-learning-16b4dd913a1f)，以及数据集并不能代表所有人等等。最后，即使数据集代表性很好，保留数据集上表现优异也并不意味着模型能改进产品或进一步实现商业诉求。仅仅依靠集成性能标准对保留数据进行验证是远远不够的。