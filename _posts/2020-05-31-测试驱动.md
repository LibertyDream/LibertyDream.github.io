---
layout:		post
title:  	测试驱动的机器学习开发
subtitle:   模型部署系列之七
date:       2020-05-31
author:     一轩明月
header-img: img/post-bg-future.jpg
catalog:    true
tags:
    - production
excerpt:    类似于传统软件，模型需要经过完备测试才能用于生产。测试驱动的软体构建技术称为测试驱动开发，离线环境下的重点是保留数据测试，特殊样本测试以及重要亚群测试，加上测试模型部署才能闭环，实现 CI/CD
---

> 编译自：Test-Driven Machine Learning Development，[Luigi](https://mlinproduction.com/author/luigi/)

机器学习部署系列的上一篇文章介绍了模型注册怎样做到了一石多鸟，包括存储模型谱系，模型版本和配置信息。比如，可以用模型注册系统查询某个序列化模型存在哪里。注册系统同时还能指明给定模型的 _开发阶段_，可以查询了解训练完成的模型的哪个版本处于开发阶段，又是哪个版本运行时负责生成预测。类似于新软件的部署过程，新近训练过的模型在升至 _筹划_ 乃至 _生产_  就绪阶段前，都要从 _开发_  起步。

但怎样判断时机，升华模型走完整轮周期？还是站在巨人的肩膀上从软件开发中偷师借鉴点东西吧。传统软件开发中，代码必须经过各式各样的测试才能上生产线。比如[单元测试](https://martinfowler.com/articles/practical-test-pyramid.html#UnitTests)负责检测特定代码单元，而[集成测试](https://martinfowler.com/articles/practical-test-pyramid.html#IntegrationTests)则要测试你的 app 和外围其他应用的集成度。

机器学习模型当然也要经过测试才能上线，这很重要。只有经过充分测试才能有信心模型会如期运作，但该语境下的“测试”到底意味着什么呢？本文会给出模型测试的定义并介绍几种离线环境的模型测验方法。这些测试策略可使我们实现模型部署闭环，将 CI/CD 应用到机器学习上。

### 测试驱动开发

如 Martin Fowler 所言，[测试驱动开发（Test-Driven Development，TDD）](https://martinfowler.com/bliki/TestDrivenDevelopment.html)是一种软件建构技术，通过写测试来指导软件开发。该过程就简单的三个步骤，不断重复直到项目完成。

1. 给想添加的功能写测试用例
2. 编写功能代码直到测试通过
3. 重构所有代码优化结构

举个简单例子，假设你想写个方法接收一个数字列表，将数字的平方数列表返回

```python
def square_list(numbers):
    '''
    接收数值列表，对数值求平方后以列表方式返回
    '''
    pass
```

实现功能前，要先写测试，_假定功能实现正确_ 则其应当返回预期结果。`square_list` 方法的测试样例包括：

- 1）如果参数 `numbers` 并非列表要抛出异常
- 2）返回列表应当和 `numbers` 等长
- 3）返回列表中索引 `j` 处的数等于参数 `numbers` 中 `j` 处数的平方

这些只是可写测试的一小部分。比如，测试三假定 `numbers` 中的每一项都是数字，结合上下文，当然可以写个测试核对该条件是否成立。

申明并写好测试用例后，就可以着手实现 `square_list` 方法。当方法通过了所有测试，就可以说代码实现了预期功能，同时也有了一组测试能防止基础代码退化。这也正是测试驱动开发的主要优点：这项技术生产的不仅仅是代码，而是经过良好测试的代码。

### 测试机器学习模型

和常规软件一样，机器学习模型部署前必须经过验证，通过检验或测试才能保证高质量交付预测结果。预测性能差劲的模型可能会给用户和组织带来灾难性后果，鉴于低劣的歌曲推荐系统会使听众不满，自动驾驶系统中的物体探测器如果不精确则有可能招致死亡。显然部署模型上线前应当竭尽所能提防此类错误出现。

如前文所述，TDD 主要就是在实现应用功能前先写好测试用例。但写用例首先要假定你知道什么要测试，这（一般）在确定性系统中很直观，如 `square_list` 的例子所展示的那样。但不像传统软件，机器学习模型是不确定的，这在 [ML 测试评分](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf)一文中说的清楚：

> ML 系统测试相较检验人工编码系统来的更复杂，因为 ML 系统行为高度依赖数据和模型，而这些却不能先验地明确指定

所以要怎么测试我们的模型呢？要怎样规避机器学习的随机性，即无法预知会碰到什么未知数据的事实？来看一下撰写机器学习模型测试用例的三种方式，尤其要看下利于实现模型部署自动化的这些测试用例。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-07-09_test-pyramid.png)

#### 1. 保留数据集上的性能

首先要验证的是模型在保留数据集上的预测能力，并与之前定好的最小可接受阈值进行比对。在[产品规划阶段](https://www.jeremyjordan.me/ml-requirements/)，产品和数据科学团队应合伙商讨确定性能标准和最低性能门槛。该标准/阈值的组合决定了一个模型在部署上线前所应达到的最次水平。

尽管这是最广为人知且最容易实现的模型测试，还是有几点要注意。标准/阈值组合一旦确定，数据划分策略也就随之而定了。和数据科学训练营与在线教程教授你的不同，适宜地将数据集划分成训练集、验证集和测试集要比随机均匀分割复杂得多。划分过程是问题导向的：数据划分应该尽量接近模型在实际推理时所要面对的场景。

一个典型的例子是对带有时间属性的数据进行预测。比如潜在顾客评分模型倾向于服务新顾客（[系列文章一](https://libertydream.github.io/2020/02/23/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%88%B0%E5%BA%95%E6%98%AF%E5%9C%A8%E8%AF%B4%E4%BB%80%E4%B9%88/)中的例子），随机划分数据集得到的预测结果会存有偏见，因为我们期望的是对新近产生的顾客得分进行排序，模型应该在相对较旧的客户群体评分上进行训练，而在较新人群上进行验证。

其他问题还有[数据泄露](https://mlinproduction.com/data-leakage/)，隐式[反馈循环](https://medium.com/@rchang/getting-better-at-machine-learning-16b4dd913a1f)，以及数据集并不能代表所有人等等。最后，即使数据集代表性很好，保留数据集上表现优异也并不意味着模型能改进产品或进一步实现商业诉求。仅仅依靠集成性能标准对保留数据进行验证是远远不够的。

#### 2. 特定样本性能

保留数据集上模型的表现完全看不出对特定样本的效果。有时会有些样本我们希望模型 _总会_ 输出特定结果。

回到潜在客户评分模型，假定我们就职的 A 公司是一家 B2B SaaS 性质的企业，专卖昂贵的企业软件。销售团队对高质量客户会有哪些特征通常有一套有效经验，这些潜在客户“画像”不是 ML 模型——只是些启发式线索或对特定输入特征的简单条件判断语句。A 公司的销售团队相信起步阶段的创业公司无法负担得起我们的产品，所以与其在创立时间少于 6 个月的公司上浪费客户经理的时间，销售队伍偏爱将他们添加到自动邮件推广序列中慢慢“培养”。

此时，任何已上线模型都该将存续时间少于 6 个月的公司归为没资质客户。假如模型能访问到公司年限并将其作为输入特征，就可以构建一组测试断言期望行为，但要怎么做呢？

一种方式是查看历史数据并从先前样本中抽出匹配画像的。向 CRM 查询哪些公司年限少于 6 个月并提取结果，不管何时会训练新模型，都要对这些测试用例进行预测并断言结果总是缺乏资质。[《构建机器学习赋能的应用》](https://amzn.to/33AQ0Ue)一书中对此类测试的重要性评价道：

> 我们也要对特定输入进行测试，这有助于监测到新模型预测质量的衰退，并保证所使用的任意模型对这些样本总会得到期望结果。新模型集成性能表现较好时，很难注意到特定类型输入得到的结果可能变糟。这些测试能更容易地披露此类问题。

#### 3. 关键亚群性能

评估模型在验证集上的性能，会将一系列预测中包含的信息压缩到想准确率或 RMSE 这样的单个数字中。这种集成可能会产生误导性结果，例如，模型在特定数据切片上的表现可能会与在全体数据上的表现大相径庭。

在重要数据亚群上保证模型不犯系统性失误十分重要，这些亚群可通过性别、年龄或种族这样的人口统计数据进行划分。亦或采用实用性定义，比如按潜在客户来源分成自发搜索、 Facebook 广告和 Linkedin 广告等。即使模型在总体数据集上取得了能接受的最次性能，如果模型对关键亚群极其低效甚至无效，一旦部署上线你可能就会陷入麻烦之中。

还是潜在客户评分的例子，假设模型集成表现绝佳但对来自移动广告的客户表现却不甚理想。从过往经验来看，只有少数客户来自移动广告，所以集成时这种差劲表现被洗刷掉了。但假如市场团队决定增加移动广告投入，这自然会使来自移动平台的客户增多，那么我们可就陷入棘手境地了。市场运作之前，模型对移动端客户表现欠佳但好在数量较少，现在模型可就会对更大一部分用户失效了。这对基于评分进行决策的下游任务/活动无疑会是场灾难，而且可能到局面难以挽回前我们都诊断不出是哪里错了。

要预防此类问题的发生，自然要在模型上线前检查模型对重要亚群的性能表现，然后按评估标准考核模型在全体数据和重要切片上的性能。在潜在客户评分的例子中，这意味着要计算验证集整体的准确率以及各客户源上的准确率。如此一来，在模型评估阶段就发现性能短板，而不是在线上运行时影响到用户。

Andrej Karpathy 在 2020 年 2 月 [the ScaledML Conference](https://www.youtube.com/watch?v=hx7BXih7zx8) 上的报告可以作为不错的案例。报告中 Karpathy 介绍了特斯拉的 AI 团队怎样手工编写专项测试，这些测试在模型部署到车辆上之前必须通过。比如检测停止标志的时候，团队为严重阻挡、人为持有、建筑区内等场景中的停止标志独立构建了测试。

### 总结

本文介绍了几种机器学习模型的测试方法，项目要采用哪种验证过程取决于应用复杂度（模型是否自动化运作？），模型出错的代价（音乐推荐 vs 医疗干预推荐），以及组织自身的资源限制（是否有基建专家？）

接着我们探讨了离线环境下的模型测试问题。离线测试用于在历史数据上验证模型，此时我们拥有实际标签/答案。但离线测试并不是我们能（或应该）做的唯一类型的验证。要建立模型和预期效果间的因果关系，在线测试必不可少，比如增加用户参与度或提高转换率。当然，这需要额外写些代码和相关基础设施的支持。