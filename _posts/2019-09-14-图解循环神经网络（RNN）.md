---
layout:     post
title:      图解循环神经网络（RNN）
subtitle:   理解直觉
date:       2019-09-14
author:     一轩明月
header-img: img/post-bg-unix-linux.jpg
catalog: 	 true
tags:
    - NLP
    - RNN
---

# 图解循环神经网络（RNN）

> 文章编译自：
>
> https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9

如果你在使用智能手机或是经常上网，大概率你已经使用过 RNN 程序了。循环神经网络（Recurrent neural network，RNN） 作为一种深度学习模型广泛用于语音识别，语言翻译，股票预测甚至解析图片中的文字内容。

本文旨在为初学深度学习的人们直观了解循环神经网络（下文统称 RNN）及其背后思想提供便利。本文会尽量回避数学公式，而是通过动画演示与文字注解尽力揭示 RNN 蕴含的思想，这也是我当初学习轨迹的再现。希望看完本文后你能很好的理解 RNN 并获得啊哈时刻。

## 序列数据

RNN 这种神经网络很擅长处理序列数据（sequence data）。什么意思呢？我们先来做个思想实验。假设你在给一颗正在快速移动的球拍快照。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-13_ball.png)

你想要预测这颗球的移动方向，只有上图中的信息的情况下你会怎么做呢？你可能会给出一个猜测，但所有猜测都将是随机的，也就是瞎猜。不知道球从哪里来，就没有足够数据预言它会到哪去。

而当你连续拍下了球移动时的许多快照，你就有了足够的信息能作出更好的预测。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-13_ball_move.gif)

这就是序列，一个接一个的特定顺序。此时你可以看出这颗球在向右运动。

序列有很多形式，音频天然具有序列属性。你可以将一段音频频谱切成几段输入到 RNN 模型当中

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-13_audio.png)

文本是另一种形式的序列，可以将文本拆解成字符序列或是单词序列。

## 顺序记忆

RNN 确实在处理序列数据并给出预测上有一套，但是怎样做到的呢？我的答案是顺序记忆（sequential memory）。这是什么意思呢？

便于理解背后思想，请你现在在脑中背一下字母表。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_alpha.png)

很容易是不是？如果你学习时学习的顺序如此那对你来说不会是什么难事。

现在试着倒着背一遍。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_alpha_backward.png)

除非你刻意练习过，这一次你会困难许多。让我们继续，这次从中间一个字母 F 开始

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_char_f.png)

在回忆 f 之后的头几个字母时会有些困难，而后你的大脑会捕获模式，剩下的字母可以自然地“流淌”出来。

这样就有了一个合理解释，为什么背诵字母表时难时易。学习字母表时我们就是按照序列学习的。顺序记忆是大脑的一种快速识别序列模式的机制。

## 循环神经网络

一个自然而然的问题是，顺序记忆这种机制在 RNN 中是怎么体现的呢？让我们看一下典型的前馈神经网络，有输入层、隐层和输出层。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_ffnn.png)

怎样让前馈神经网络能用先前的信息影响后续内容呢？给它加个循环传递早先的信息怎如何？

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_rnn.png)

这也正是 RNN 所做的事情。RNN 包含一个循环机制，像高速路一样将这一步的信息传递给下一步。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_rnn.gif)

传递的信息称之为隐态（hidden state），表示之前的输入。让我们通过一个例子来熟悉一下 RNN 工作流程。

比方说我们现在想要构建一个对话机器人，这在最近很流行。我们的机器人能够识别用户所输入内容的意图

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_classify_intent_of_text.gif)

要解决这个问题，首先我们要用 RNN 对文本编码。然后将 RNN 的输出传递给一个前馈神经网络进行意图分类。

假设用户输入的是 “what time is it?”。首先我们将语句序列分解成独立的单词。RNN是顺序工作的，所以每次我们传入一个单词。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_tokenization.gif)

首先我们传入“What"。RNN 对 ”What“进行编码并得到一个输出。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_what.gif)

下一步，我们传入”time“和上一步的隐态。RNN 现在就有了 ”What“ 和 ”time“ 两者的信息。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_time.gif)

重复这一过程直至最后一步。可以看到运算到了最后一步时， RNN 已经对所有单词都进行了编码

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_last_step.gif)

因为最后一步的输出是在考量了整个序列信息后得出的，我们将其作为最终输出送入前馈网络部分进行意图分类。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_intent_classify.gif)

下面的 Python 代码展示了上述控制逻辑

```python
rnn = RNN()
ff = FeedForwardNN()
hidden_state = [0.0, 0.0, 0.0, 0.0]

for word in inputs:
    output, hidden_state = rnn(word, hidden_state)
    
prediction = ff(output)
```

首先初始化网络层与隐态。隐态的维度与大小视你的循环神经网络的维度与大小而定。之后遍历输入，将单词与隐态送入 RNN。RNN 更新输出与隐态，递归直至处理完所有单词。最后将输出送入前馈层得到分类预测的结果。就是这样，通过 for 循环控制 RNN 正向传导。

## 消失的梯度

你可能已经注意到了隐态中的颜色分布比例。这是为了说明 RNN 中被称为短文记忆的问题。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_final_state.png)

短文记忆是由声名狼藉的梯度消失问题导致的，后者也广泛存在于其它网络架构中。随着 RNN 处理步骤增多，保留先前步骤的信息越来越难。如你所见，”what“和”time“的信息在最后一步几乎不存在了。短文记忆和梯度消失源自反向传播，一种训练与优化神经网络的算法。为了理解这一点，我们先看一下反向传播对深层前馈神经网络的影响。

训练神经网络大体分为三步。首先，前向传播得到一个预测结果。之后，借助损失函数比较预测值与真实值。损失函数给出一个误差值，表示当前网络的效果有多差。最后，使用误差值进行反向传播，这要计算网络中每一个结点的梯度。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_train_network.gif)

梯度是网络要学习的，用于调整网络内部权重的值。梯度越大，调整幅度越大，反之亦然。这就是问题所在。进行反向传播时，每个结点计算梯度时都会参考在它之前各层的梯度。所以如果对它前面各层的调整很小，对当前层的调整会更小。

这会导致反向传播时梯度会指数级缩小。极小的梯度导致内部权重几乎不怎么改变，也就使得前面的网络层学不到什么东西。这就是梯度消失问题

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_gradient_shrink.gif)

我们来看一下这在 RNN 内是怎样一种情形。RNN 中的每一步可以视作一层，要训练网络，就要用到时进反向传播（back-propagation through time）。梯度值随着时步（time step）变化指数级收缩。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-14_shrink_time_step.gif)

要调整内部权重就要学习梯度，小梯度意味着小调整。这导致前面的层基本不学习。

因为梯度消失，RNN 无法学习大范围的依赖关系。这意味着单词”what“和”time“在预测用户意图时不会被考虑。网络必须用”is it ?“做出最优预测。这对人类来讲都太难了。所以无法学习早先时步内容导致网络的短文记忆问题。

## 其实还好

总结一下，RNN  适于处理序列数据并进行预测，只是有短文记忆问题。如果想要学习长文依赖关系，可以使用进阶版本比如 LSTM 或是 GRU。因为张量计算更少，RNN 具有训练速度快、资源消耗少的优点。