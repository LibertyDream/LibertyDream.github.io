---
layout:     post
title:      GRU 与 LSTM 图解
subtitle:   不要数学，要白话
date:       2019-09-21
author:     一轩明月
header-img: img/post-bg-2015.jpg
catalog: 	 true
tags:
    - NLP
    - LSTM
excerpt:    RNN 在长距离上的梯度消失问题导致其在长文本分析上表现不佳。使用门机制的 LSTM 尝试通过“存储”相关重要信息破解长距离依赖问题，GRU 进一步简化了门机制。本文会先介绍 GRU 与 LSTM 的思想，而后阐述二者的内在机制。
---

> 文章编译自：
>
> https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21

本文会先介绍 GRU 与 LSTM 的思想，而后阐述 LSTM 与 GRU 取得良好表现的内在机制。

### RNN 的缺陷

短文记忆是循环神经网络（RNN）自带的问题之一。当文本过长，后面的文本很难从前文获取有效信息。所以在处理比较长的文本进行预测时，特别要小心 RNN 可能会漏掉文章开头的信息。

在反向传播时 RNN 有梯度消失问题。梯度常用于更新神经网络权重。梯度消失是指随着反向传播的推进，梯度缩减的越来越厉害。当梯度特别小的时候，对学习过程就没什么帮助了。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_gradient_update_rule.png)

因此，RNN 里梯度更新小的层意味着学习停止。越在前面的层这一现象越明显，而越不学习，在后续学习时 RNN 越少考量其价值，以至于“忘了”它们的存在，短文记忆问题由此产生。关于 RNN 的通俗介绍可以参见我的另一篇[文章](https://libertydream.github.io/2019/09/14/图解循环神经网络-RNN/)

### LSTM 和 GRU 的解决办法

LSTM 与 GRU 因短文记忆问题而生。它们都通过门机制调节信息流，不让后面的文本”忘记“前面的内容。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_gate_internal_mechanisms.png)

这些门可以识别哪些内容需要保留或是删除，这样就能在长文本序列间传递相关信息帮助预测了。在 BERT 出现前几乎所有性能优异的 RNN 模型都是依托这两种模型实现的。不论是语音识别，语音合成还是文本生成中都能见到它们的身影，甚至还可以用它们生成视频标题。

好了，既然知晓了 LSTM 和 GRU 的神奇，接下来我会用图形化的方式介绍二者背后的思想与机理，同时尽量回避数学公式。

### 思想阐述

让我们从一个思想实验开始，假设你想买罐麦片，正在浏览买家评论，看看麦片好坏，决定要不要在线买。![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_customer_review.png)

当你阅读评论时，你的大脑潜意识下只会选择记些关键信息。你会关注"amazing", "perfectly balanced breakfast"这些定性内容。你不会太在意"this", "gave", "all"这些词。如果朋友隔天问起这款麦片评价怎样，你不会记得每一个词，你可能只记得一些关键点，比如"will definitely be buying again"。不出意外的话，其他词会从你的记忆里消失。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_customer_review.gif)

这也正是 LSTM 和 GRU 所做的事，它们只保留对预测有帮助的信息，其他无关信息统统忘掉。例子中你记住的内容会让你判断这款麦片不错。

#### 回顾一下 RNN

要理解 LSTM 和 GRU 是怎么做到选择性记忆的，我们先回顾一下普通的循环神经网络。标准 RNN 通常是先把单词转换成机器可读的输入向量，然后逐个处理文本序列里的单词。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_comman_rnn.gif)

处理过程中，前一个单元输出的隐层状态作为输入之一传递给下一个单元。隐状态就像是神经网络的记忆，把持着神经网络已经浏览过的信息。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_pass_hidden_state.gif)

我们来看一下隐状态是怎样计算的。首先，前一个单元的隐状态和本单元的输入结合成一个向量。这个向量拥有当下和之前的输入信息。之后将该向量传入 tanh 激活函数，函数输出就是新的隐状态或者说网络记忆。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_rnn_unit_cal.gif)

### tanh 激活函数

tanh 是双曲正切函数，双曲函数之一。这里被用作激活函数调节神经网络内部单元间传递的值，即司掌状态与输出。tanh 函数会把值压缩到 -1 到 1 的区间内。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_tanh.gif)

向量在网络内部传递时，会在各种数学处理下变换许多次。试想一下如果没有这些处理，一个向量经过每个处理单元时都乘以三，你会看到一些值会变得无比大以至于其它值显得微不足道。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_trans_without_tanh.gif)

tanh 函数保证所有值都被约束在 -1 到 1 之间，从而控制了神经网络的输出。你可以看到同样的向量在 tanh 管制下怎样在界限内变化的。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_trans_with_tanh.gif)

这就是 RNN 了，只是一些简单计算操作的重复就可以取得不错效果，当然计算背景得是短序列类型的。相比于各种 RNN 演化得到的变体，比如 LSTM 和 GRU，普通 RNN 对计算资源需求并不大。

### LSTM

长短文记忆模型（ Long Short-Term Memory，LSTM）有着与循环神经网络相似的控制流程，前向传播的时候一边处理数据一边传递信息。差别在于计算处理单元的内部构造。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_LSTM_cell.gif)

这些操作使 LSTM 能保留或是忘记信息。乍一看这些操作有些让人头晕，我们一步一步来。

#### 核心概念

LSTM 的核心在于单元状态和各种门。单元状态像高速公路一样沿着序列链条传递相关信息，你可以将其视作网络的“记忆”。理论上单元状态可以全程携带相关信息处理序列。这样早先得到的信息也能给后面步骤以帮助，一定程度上解决了短文记忆问题。而就在单元状态流动的同时，信息通过各种门添加到状态中，或是从状态中删除。门是决定信息去留的不同神经网络。门可以在训练时学习哪些信息是有用的要保留，哪些要忘记。

#### sigmod 函数

门机制包含 sigmod 激活函数。sigmod 和 tanh 类似，都是双曲函数。只是不同于后者，sigmod 会将值限定在 0 到 1 之间，这有助于更新或抛弃数据，因为任何数乘以 0 得 0，意味着这个值消失或被丢弃了。同理，任何数乘 1 还是本身，这个值就被保留或“记住”了。网络会学习哪些数据无足轻重要放弃，哪些很重要要记住。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_sigmod.gif)

三种门把持着 LSTM 单元间的信息流：遗忘门，输入门以及输出门

#### 遗忘门

遗忘门决定信息是否要被丢弃或是保留。上一步的隐状态和当前的输入组合成新向量喂给 sigmod 函数，得到 0 到 1 间的一个输出。接近 0 丢弃，靠近 1 保留。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_forget_gate.gif)

#### 输入门

要更新单元状态，就需要输入门。首先，将上一步的隐状态和当前的输入组合成新向量传入 sigmoid 函数，压缩到 0 到 1 之间决定哪些数据比较重要。0 代表不重要，1 代表重要。同时还要把新向量传给一个 tanh 函数，将数值继续限定在 -1 到 1 之间。随后将两个激活函数的结果相乘。sigmod 的输出决定 tanh 输出中的哪些值应该保留。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_input_gate.gif)

#### 单元状态

现在我们已经有足够的信息更新单元状态了。首先上一步的单元状态向量与遗忘门得到的向量作点积。如果乘积接近于 0 就从单元状态内删除这些值。之后将运算结果与输入门得到的向量相加更新单元状态。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_cell_state.gif)

#### 输出门

输出门决定输出的隐状态。隐状态包含之前步骤的输入信息，也用于做预测。首先，将前一步的隐状态与当前输入组合成新的向量传入 sigmod 函数。之后将更新后的单元状态传入 tanh 激活。将两个函数的输出相乘决定新的隐状态应当携带哪些信息。新的单元状态和隐状态作为输入之一传给下一步。

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_output_gate.gif)

总结一下，遗忘门决定哪些数据和早先内容相关。输入门决定当前哪些信息值得加入。输出门决定下一阶段的隐状态。

LSTM 伪代码如下：

```python
def LSTMCELL(prev_ct, prev_ht, input):
    combine = prev_ht + input
    ft = forget_layer(combine)
    candidate = candidate_layer(combine)
    it = input_layer(combine)
    Ct = prev_ct * ft + candidate * it
    ot = output_layer(combine)
    ht = ot * tanh(Ct)
    return ht, Ct

ct = [0, 0, 0]
ht = [0, 0, 0]

for input in inputs:
    ct, ht = LSTMCELL(ct, ht, input)
```

1. 首先，上一步的隐状态和当前输入级联，这个操作我们定义成  `combine`
2. 将 `combine` 送入遗忘层。这一层会去掉无关数据
3. 使用 `combine` 在候选层取得候选列表。列表记录了所有可能会加入到单元状态中的值
4.  `combine` 送入输入层。本层决定候选列表内的哪些数据要加入到新单元状态内
5. 计算遗忘层，候选层和输入层的输出向量，结合之前的单元状态计算新的单元状态
6. 计算输出
7. 将输出与新单元状态作点积得到新隐态

LSTM 网络的控制流就是这样一些张量运算和一个循环。你可以使用隐状态进行预测。结合所有这些机制，LSTM 能在处理序列的时候选择保留或删除数据

### GRU

我们已经知道了 LSTM 的机理，现在快速看一下 GRU。GRU 是更新一代的循环神经网络，和 LSTM 很像。GRU 摆脱了单元状态而使用隐状态传递信息。GRU 只有两个门，重置门和更新门

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-21_GRU.png)

- 更新门

更新门的作用类似于 LSTM 中的遗忘门和输入门，用以决定哪些信息丢弃哪些保留。

- 重置门

重置门用来决定有多少旧信息要被丢弃。

这就是 GRU，有着更少的张量运算，所以通常会比 LSTM 训练时间稍短些。不能特别肯定地说二者哪一个更好。研究员和工程师们通常两个都会试试看，选择表现更好的那个。