---

layout:		post
title:  	元学习：学会快速学习
subtitle:   
date:       2021-03-28
author:     一轩明月
header-img: img/post-bg-hacker.jpg
catalog:    true
tags:
    - math
    - optimization
excerpt:    meta-learning
---

> 编译自：Meta-Learning: Learning to Learn Fast， [Lilian Weng](https://lilianweng.github.io/lil-log/)

### 元问题定义

#### 朴素视角

 $$\mathcal{D}$$,
$$
\theta^* = \arg\min_\theta \mathbb{E}_{\mathcal{D}\sim p(\mathcal{D})} [\mathcal{L}_\theta(\mathcal{D})]
$$
 $$\mathcal{D}$$   $$S$$  t $$B$$  $$\mathcal{D}=\langle S, B\rangle$$. 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_few-shot-classification.png)



#### 测试训练方式一致

 $$\mathcal{D}$$  $$\mathcal{D} = \{(\mathbf{x}_i, y_i)\}$$  $$\mathcal{L}^\text{label}$$. $$f_\theta$$  $$\theta$$ r $$\mathbf{x}$$, $$P_\theta(y\vert\mathbf{x})$$. 

 $$B \subset \mathcal{D}$$:
$$
\begin{aligned}
\theta^* &= {\arg\max}_{\theta} \mathbb{E}_{(\mathbf{x}, y)\in \mathcal{D}}[P_\theta(y \vert \mathbf{x})] &\\
\theta^* &= {\arg\max}_{\theta} \mathbb{E}_{B\subset \mathcal{D}}[\sum_{(\mathbf{x}, y)\in B}P_\theta(y \vert \mathbf{x})] & \scriptstyle{\text{; trained with mini-batches.}}
\end{aligned}
$$
 $$L\subset\mathcal{L}^\text{label}$$.

 $$S^L \subset \mathcal{D}$$   $$B^L \subset \mathcal{D}$$. $$L$$, $$y \in L, \forall (x, y) \in S^L, B^L$$.

 $$B^L$$

 $$(S^L, B^L)$$ 
$$
\theta = \arg\max_\theta \color{red}{E_{L\subset\mathcal{L}}[} E_{\color{red}{S^L \subset\mathcal{D}, }B^L \subset\mathcal{D}} [\sum_{(x, y)\in B^L} P_\theta(x, y\color{red}{, S^L})] \color{red}{]}
$$

#### 学习器和元学习器

 $$f_\theta$$ 

 $$g_\phi$$ 

 $$S$$, $$\theta' = g_\phi(\theta, S)$$.
$$
\mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})]]
$$

#### 一般方法

|                                                      | Model-based                 | Metric-based                                                 | Optimization-based                              |
| ---------------------------------------------------- | --------------------------- | ------------------------------------------------------------ | ----------------------------------------------- |
| **Key idea**                                         | RNN; memory                 | Metric learning                                              | Gradient descent                                |
| **How $$P_\theta(y \vert \mathbf{x})$$ is modeled?** | $$f_\theta(\mathbf{x}, S)$$ | $$\sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i$$ (*) | $$P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$$ |

(*) $$k_\theta$$  $$\mathbf{x}_i$$ and $$\mathbf{x}$$.

### 基于标准的

$$k_\theta$$, 
$$
P_\theta(y \vert \mathbf{x}, S) = \sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i
$$

#### 孪生卷积网络

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_siamese-conv-net.png)

 $$f_\theta$$ 

 $$\vert f_\theta(\mathbf{x}_i) - f_\theta(\mathbf{x}_j) \vert$$.

 $$p$$
$$
\begin{aligned}
p(\mathbf{x}_i, \mathbf{x}_j) &= \sigma(\mathbf{W}\vert f_\theta(\mathbf{x}_i) - f_\theta(\mathbf{x}_j) \vert) \\
\mathcal{L}(B) &= \sum_{(\mathbf{x}_i, \mathbf{x}_j, y_i, y_j)\in B} \mathbf{1}_{y_i=y_j}\log p(\mathbf{x}_i, \mathbf{x}_j) + (1-\mathbf{1}_{y_i=y_j})\log (1-p(\mathbf{x}_i, \mathbf{x}_j))
\end{aligned}
$$
 $$B$$  $$S$$  $$\mathbf{x}$$,
$$
\hat{c}_S(\mathbf{x}) = c(\arg\max_{\mathbf{x}_i \in S} P(\mathbf{x}, \mathbf{x}_i))
$$
 $$c(\mathbf{x})$$  $$\mathbf{x}$$ and $$\hat{c}(.)$$ 

#### 匹配网络

 $$S=\{x_i, y_i\}_{i=1}^k$$  $$y$$  $$\mathbf{x}$$ l $$a(\mathbf{x}, \mathbf{x}_i)$$  $$\mathbf{x}$$ and $$\mathbf{x}_i$$.

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_matching-networks.png)

$$
c_S(\mathbf{x}) = P(y \vert \mathbf{x}, S) = \sum_{i=1}^k a(\mathbf{x}, \mathbf{x}_i) y_i
\text{, where }S=\{(\mathbf{x}_i, y_i)\}_{i=1}^k
$$
$$f$$ and $$g$$,  $$\text{cosine}(.)$$,
$$
a(\mathbf{x}, \mathbf{x}_i) = \frac{\exp(\text{cosine}(f(\mathbf{x}), g(\mathbf{x}_i))}{\sum_{j=1}^k\exp(\text{cosine}(f(\mathbf{x}), g(\mathbf{x}_j))}
$$

##### -- 简易 embedding

 $$f=g$$. 

##### -- 全语境 embedding

$$g_\theta(\mathbf{x}_i, S)$$   $$\mathbf{x}_i$$   $$S$$.

$$f_\theta(\mathbf{x}, S)$$  $$\mathbf{x}$$  $$S$$.

 $$f'(\mathbf{x})$$.
$$
\begin{aligned}
\hat{\mathbf{h}}_t, \mathbf{c}_t &= \text{LSTM}(f'(\mathbf{x}), [\mathbf{h}_{t-1}, \mathbf{r}_{t-1}], \mathbf{c}_{t-1}) \\
\mathbf{h}_t &= \hat{\mathbf{h}}_t + f'(\mathbf{x}) \\
\mathbf{r}_{t-1} &= \sum_{i=1}^k a(\mathbf{h}_{t-1}, g(\mathbf{x}_i)) g(\mathbf{x}_i) \\
a(\mathbf{h}_{t-1}, g(\mathbf{x}_i)) &= \text{softmax}(\mathbf{h}_{t-1}^\top g(\mathbf{x}_i)) = \frac{\exp(\mathbf{h}_{t-1}^\top g(\mathbf{x}_i))}{\sum_{j=1}^k \exp(\mathbf{h}_{t-1}^\top g(\mathbf{x}_j))}
\end{aligned}
$$
 $$f(\mathbf{x}, S)=\mathbf{h}_K$$ 
$$
\theta^* = \arg\max_\theta \mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_\theta(y\vert\mathbf{x}, S^L)]]
$$

#### 关联网络

 $$\mathbf{x}_i$$ and $$\mathbf{x}_j$$, is $$r_{ij} = g_\phi([\mathbf{x}_i, \mathbf{x}_j])$$  $$[.,.]$$ 

$$\mathcal{L}(B) = \sum_{(\mathbf{x}_i, \mathbf{x}_j, y_i, y_j)\in B} (r_{ij} - \mathbf{1}_{y_i=y_j})^2$$.

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_relation-network.png)

#### 原型网络

 $$f_\theta$$  $$M$$-  $$c \in \mathcal{C}$$,
$$
\mathbf{v}_c = \frac{1}{|S_c|} \sum_{(\mathbf{x}_i, y_i) \in S_c} f_\theta(\mathbf{x}_i)
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_prototypical-networks.png)

t $$\mathbf{x}$$ 
$$
P(y=c\vert\mathbf{x})=\text{softmax}(-d_\varphi(f_\theta(\mathbf{x}), \mathbf{v}_c)) = \frac{\exp(-d_\varphi(f_\theta(\mathbf{x}), \mathbf{v}_c))}{\sum_{c' \in \mathcal{C}}\exp(-d_\varphi(f_\theta(\mathbf{x}), \mathbf{v}_{c'}))}
$$
 $$d_\varphi$$  $$\varphi$$ $$\mathcal{L}(\theta) = -\log P_\theta(y=c\vert\mathbf{x})$$.

### 基于模型的

 $$P_\theta(y\vert\mathbf{x})$$. 

#### 记忆强化网络

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_NTM.png)

 $$\mathbf{M}_t$$  $$N \times M$$,

##### -- 元学习的 MANN

 $$y_t$$, $$(\mathbf{x}_{t+1}, y_t)$$:

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_mann-meta-learning.png)

##### -- 元学习的寻址机制

 $$\mathbf{k}_t$$  $$\mathbf{x}$$.  $$\mathbf{w}_t^r$$  $$\mathbf{r}_t$$ 
$$
\mathbf{r}_i = \sum_{i=1}^N w_t^r(i)\mathbf{M}_t(i)
\text{, where } w_t^r(i) = \text{softmax}(\frac{\mathbf{k}_t \cdot \mathbf{M}_t(i)}{\|\mathbf{k}_t\| \cdot \|\mathbf{M}_t(i)\|})
$$
 $$M_t$$ $$M_t(i)$$ 

 $$\mathbf{w}^u_t$$ , $$\gamma \mathbf{w}^u_{t-1}$$, $$\gamma$$ 

r $$\alpha$$.

 $$\mathbf{w}^{lu}$$   $$\mathbf{w}_t^u$$, 
$$
\begin{aligned}
\mathbf{w}_t^u &= \gamma \mathbf{w}_{t-1}^u + \mathbf{w}_t^r + \mathbf{w}_t^w \\
\mathbf{w}_t^r &= \text{softmax}(\text{cosine}(\mathbf{k}_t, \mathbf{M}_t(i))) \\
\mathbf{w}_t^w &= \sigma(\alpha)\mathbf{w}_{t-1}^r + (1-\sigma(\alpha))\mathbf{w}^{lu}_{t-1}\\
\mathbf{w}_t^{lu} &= \mathbf{1}_{w_t^u(i) \leq m(\mathbf{w}_t^u, n)}
\text{, where }m(\mathbf{w}_t^u, n)\text{ is the }n\text{-th smallest element in vector }\mathbf{w}_t^u\text{.}
\end{aligned}
$$
 $$\mathbf{w}_t^{lu}$$,
$$
\mathbf{M}_t(i) = \mathbf{M}_{t-1}(i) + w_t^w(i)\mathbf{k}_t, \forall i
$$

#### 元网络

##### -- 敏捷权重

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_combine-slow-fast-weights.png)

 $$\bigoplus$$ 

##### -- 模型构成

 $$f_\theta$$  $$\theta$$, 

 $$g_\phi$$  $$\phi$$, 

 $$f$$ and $$g$$ 

$$F_w$$: $$\theta^+$$  $$f$$.  $$f$$'s

$$G_v$$: $$v$$  $$\phi^+$$  $$g$$ 

 $$S=\{\mathbf{x}'_i, y'_i\}_{i=1}^K$$ $$U=\{\mathbf{x}_i, y_i\}_{i=1}^L$$. , $$(\theta, \phi, w, v)$$.

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_meta-network.png)

##### -- 训练过程

 $$S$$, $$(\mathbf{x}'_i, y'_i)$$ and $$(\mathbf{x}'_j, y_j)$$. Let $$\mathbf{x}_{(t,1)}=\mathbf{x}'_i$$ and $$\mathbf{x}_{(t,2)}=\mathbf{x}'_j$$.

 $$t = 1, \dots, K$$:

$$\mathcal{L}^\text{emb}_t = \mathbf{1}_{y'_i=y'_j} \log P_t + (1 - \mathbf{1}_{y'_i=y'_j})\log(1 - P_t)\text{, where }P_t = \sigma(\mathbf{W}\vert f_\theta(\mathbf{x}_{(t,1)}) - f_\theta(\mathbf{x}_{(t,2)})\vert)$$

$$\theta^+ = F_w(\nabla_\theta \mathcal{L}^\text{emb}_1, \dots, \mathcal{L}^\text{emb}_T)$$

 $$S$$ 

for $$i=1, \dots, K$$:

 $$P(\hat{y}_i \vert \mathbf{x}_i) = g_\phi(\mathbf{x}_i)$$  

$$\mathcal{L}^\text{task}_i = y'_i \log g_\phi(\mathbf{x}'_i) + (1- y'_i) \log (1 - g_\phi(\mathbf{x}'_i))$$

$$\phi_i^+ = G_v(\nabla_\phi\mathcal{L}^\text{task}_i)$$

 $$\phi^+_i$$ $$i$$-  $$\mathbf{M}$$.

: $$r'_i = f_{\theta, \theta^+}(\mathbf{x}'_i)$$

 $$r'_i$$  $$i$$- $$\mathbf{R}$$. 

 $$U=\{\mathbf{x}_i, y_i\}_{i=1}^L$$. 

 $$\mathcal{L}_\text{train}=0$$:

 $$j=1, \dots, L$$:

$$r_j = f_{\theta, \theta^+}(\mathbf{x}_j)$$

 $$\mathbf{R}$$.
$$
\begin{aligned}
 a_j &= \text{cosine}(\mathbf{R}, r_j) = [\frac{r'_1\cdot r_j}{\|r'_1\|\cdot\|r_j\|}, \dots, \frac{r'_N\cdot r_j}{\|r'_N\|\cdot\|r_j\|}]\\
 \phi^+_j &= \text{softmax}(a_j)^\top \mathbf{M}
 \end{aligned}
$$
 $$\mathcal{L}_\text{train} \leftarrow \mathcal{L}_\text{train} + \mathcal{L}^\text{task}(g_{\phi, \phi^+}(\mathbf{x}_i), y_i) $$

 $$(\theta, \phi, w, v)$$ using $$\mathcal{L}_\text{train}$$.

### 基于优化的

#### LSTM 元学习器

 $$M_\theta$$  $$\theta$$,  $$R_\Theta$$  $$\Theta$$,   $$\mathcal{L}$$.

##### -- 为何用 LSTM

 $$\alpha_t$$ 
$$
\theta_t = \theta_{t-1} - \alpha_t \nabla_{\theta_{t-1}}\mathcal{L}_t
$$
 $$f_t=1$$,   $$i_t = \alpha_t$$,   $$c_t = \theta_t$$,   $$\tilde{c}_t = -\nabla_{\theta_{t-1}}\mathcal{L}_t$$:
$$
\begin{aligned}
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t\\
    &= \theta_{t-1} - \alpha_t\nabla_{\theta_{t-1}}\mathcal{L}_t
\end{aligned}
$$
$$f_t=1$$ and $$i_t=\alpha_t$$
$$
\begin{aligned}
f_t &= \sigma(\mathbf{W}_f \cdot [\nabla_{\theta_{t-1}}\mathcal{L}_t, \mathcal{L}_t, \theta_{t-1}, f_{t-1}] + \mathbf{b}_f) & \scriptstyle{\text{; how much to forget the old value of parameters.}}\\
i_t &= \sigma(\mathbf{W}_i \cdot [\nabla_{\theta_{t-1}}\mathcal{L}_t, \mathcal{L}_t, \theta_{t-1}, i_{t-1}] + \mathbf{b}_i) & \scriptstyle{\text{; corresponding to the learning rate at time step t.}}\\
\tilde{\theta}_t &= -\nabla_{\theta_{t-1}}\mathcal{L}_t &\\
\theta_t &= f_t \odot \theta_{t-1} + i_t \odot \tilde{\theta}_t &\\
\end{aligned}
$$

##### -- 模型设置

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_lstm-meta-learner.png)

 $$M_\theta$$  $$R_\Theta$$ 

 $$\mathcal{D} = (\mathcal{D}_\text{train}, \mathcal{D}_\text{test}) \in \hat{\mathcal{D}}_\text{meta-train}$$ $$\mathcal{D}_\text{train}$$  $$\theta$$ for $$T$$   $$\theta_T$$  $$\mathcal{D}_\text{test}$$.

 $$\mathcal{L}_t$$  $$\nabla_{\theta_{t-1}} \mathcal{L}_t$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_train-meta-learner.png)

#### MAML

 $$f_\theta$$  $$\theta$$.  $$\tau_i$$ a t $$(\mathcal{D}^{(i)}_\text{train}, \mathcal{D}^{(i)}_\text{test})$$,
$$
\theta'_i = \theta - \alpha \nabla_\theta\mathcal{L}^{(0)}_{\tau_i}(f_\theta)
$$
 $$\mathcal{L}^{(0)}$$ 

![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_maml.png)

 $$\theta^*$$ s $$\mathcal{L}^{(1)}$$,  $$\mathcal{L}^{(0)}$$ and $$\mathcal{L}^{(1)}$$ 
$$
\begin{aligned}
\theta^* 
&= \arg\min_\theta \sum_{\tau_i \sim p(\tau)} \mathcal{L}_{\tau_i}^{(1)} (f_{\theta'_i}) = \arg\min_\theta \sum_{\tau_i \sim p(\tau)} \mathcal{L}_{\tau_i}^{(1)} (f_{\theta - \alpha\nabla_\theta \mathcal{L}_{\tau_i}^{(0)}(f_\theta)}) & \\
\theta &\leftarrow \theta - \beta \nabla_{\theta} \sum_{\tau_i \sim p(\tau)} \mathcal{L}_{\tau_i}^{(1)} (f_{\theta - \alpha\nabla_\theta \mathcal{L}_{\tau_i}^{(0)}(f_\theta)}) & \scriptstyle{\text{; updating rule}}
\end{aligned}
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_maml-algo.png)

##### -- 先序 MAML

 $$k$$ $$k\geq1$$. $$\theta_\text{meta}$$:
$$
\begin{aligned}
\theta_0 &= \theta_\text{meta}\\
\theta_1 &= \theta_0 - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_0)\\
\theta_2 &= \theta_1 - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_1)\\
&\dots\\
\theta_k &= \theta_{k-1} - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_{k-1})
\end{aligned}
$$

$$
\begin{aligned}
\theta_\text{meta} &\leftarrow \theta_\text{meta} - \beta g_\text{MAML} & \scriptstyle{\text{; update for meta-objective}} \\[2mm]
\text{where } g_\text{MAML}
&= \nabla_{\theta} \mathcal{L}^{(1)}(\theta_k) &\\[2mm]
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot (\nabla_{\theta_{k-1}} \theta_k) \dots (\nabla_{\theta_0} \theta_1) \cdot (\nabla_{\theta} \theta_0) & \scriptstyle{\text{; following the chain rule}} \\
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \Big( \prod_{i=1}^k \nabla_{\theta_{i-1}} \theta_i \Big) \cdot I &  \\
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k \nabla_{\theta_{i-1}} (\theta_{i-1} - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_{i-1})) &  \\
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k (I - \alpha\nabla_{\theta_{i-1}}(\nabla_\theta\mathcal{L}^{(0)}(\theta_{i-1}))) &
\end{aligned}
$$

$$
g_\text{MAML} = \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k (I - \alpha \color{red}{\nabla_{\theta_{i-1}}(\nabla_\theta\mathcal{L}^{(0)}(\theta_{i-1}))})
$$

$$
g_\text{FOMAML} = \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k)
$$

#### 爬虫

$$\text{SGD}(\mathcal{L}_{\tau_i}, \theta, k)$$   $$\mathcal{L}_{\tau_i}$$  $$\theta$$  $$(\theta - W)/\alpha$$,   $$\alpha$$ 



![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_reptile-algo.png)

 $$\text{SGD}(\mathbb{E}
_\tau[\mathcal{L}_{\tau}], \theta, k)$$ diverge from $$\mathbb{E}_\tau [\text{SGD}(\mathcal{L}_{\tau}, \theta, k)]$$ when k > 1.

##### -- 最优化假设

 $$\tau \sim p(\tau)$$ , $$\mathcal{W}_{\tau}^*$$.  $$f_\theta$$   $$\mathcal{W}_{\tau}^*$$. 
$$
\theta^* = \arg\min_\theta \mathbb{E}_{\tau \sim p(\tau)} [\frac{1}{2} \text{dist}(\theta, \mathcal{W}_\tau^*)^2]
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_reptile-optim.png)

 $$\text{dist}(.)$$  $$\theta$$ $$\mathcal{W}_\tau^*$$  $$\theta$$  $$W_{\tau}^*(\theta)$$  $$\theta$$:
$$
\text{dist}(\theta, \mathcal{W}_{\tau}^*) = \text{dist}(\theta, W_{\tau}^*(\theta)) \text{, where }W_{\tau}^*(\theta) = \arg\min_{W\in\mathcal{W}_{\tau}^*} \text{dist}(\theta, W)
$$

$$
\begin{aligned}
\nabla_\theta[\frac{1}{2}\text{dist}(\theta, \mathcal{W}_{\tau_i}^*)^2]
&= \nabla_\theta[\frac{1}{2}\text{dist}(\theta, W_{\tau_i}^*(\theta))^2] & \\
&= \nabla_\theta[\frac{1}{2}(\theta - W_{\tau_i}^*(\theta))^2] & \\
&= \theta - W_{\tau_i}^*(\theta) & \scriptstyle{\text{; See notes.}}
\end{aligned}
$$
 Θ 2(Θ − p)
$$
\theta = \theta - \alpha \nabla_\theta[\frac{1}{2} \text{dist}(\theta, \mathcal{W}_{\tau_i}^*)^2] = \theta - \alpha(\theta - W_{\tau_i}^*(\theta)) = (1-\alpha)\theta + \alpha W_{\tau_i}^*(\theta)
$$
 $$W_{\tau_i}^*(\theta)$$  $$\text{SGD}(\mathcal{L}_\tau, \theta, k)$$.

##### 爬虫 vs FOMAML

 $$\text{SGD}(.)$$. , $$\mathcal{L}^{(0)}$$ and $$\mathcal{L}^{(1)}$$ $$g^{(i)}_j = \nabla_{\theta} \mathcal{L}^{(i)}(\theta_j)$$ and $$H^{(i)}_j = \nabla^2_{\theta} \mathcal{L}^{(i)}(\theta_j)$$.
$$
\begin{aligned}
\theta_0 &= \theta_\text{meta}\\
\theta_1 &= \theta_0 - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_0)= \theta_0 - \alpha g^{(0)}_0 \\
\theta_2 &= \theta_1 - \alpha\nabla_\theta\mathcal{L}^{(1)}(\theta_1) = \theta_0 - \alpha g^{(0)}_0 - \alpha g^{(1)}_1
\end{aligned}
$$

$$
\begin{aligned}
g_\text{FOMAML} &= \nabla_{\theta_1} \mathcal{L}^{(1)}(\theta_1) = g^{(1)}_1 \\
g_\text{MAML} &= \nabla_{\theta_1} \mathcal{L}^{(1)}(\theta_1) \cdot (I - \alpha\nabla^2_{\theta} \mathcal{L}^{(0)}(\theta_0)) = g^{(1)}_1 - \alpha H^{(0)}_0 g^{(1)}_1
\end{aligned}
$$

$$
g_\text{Reptile} = (\theta_0 - \theta_2) / \alpha = g^{(0)}_0 + g^{(1)}_1
$$
![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2020-03-15_reptile_vs_FOMAML.png)
$$
\begin{aligned}
g_\text{FOMAML} &= g^{(1)}_1 \\
g_\text{MAML} &= g^{(1)}_1 - \alpha H^{(0)}_0 g^{(1)}_1 \\
g_\text{Reptile} &= g^{(0)}_0 + g^{(1)}_1
\end{aligned}
$$
 $$g^{(1)}_1$$   $$f(x)$$   $$a$$ i
$$
f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \dots = \sum_{i=0}^\infty \frac{f^{(i)}(a)}{i!}(x-a)^i
$$
 $$\nabla_{\theta}\mathcal{L}^{(1)}(.)$$   $$\theta_0$$   $$g_1^{(1)}$$   $$\theta_0$$ 
$$
\begin{aligned}
g_1^{(1)} &= \nabla_{\theta}\mathcal{L}^{(1)}(\theta_1) \\
&= \nabla_{\theta}\mathcal{L}^{(1)}(\theta_0) + \nabla^2_\theta\mathcal{L}^{(1)}(\theta_0)(\theta_1 - \theta_0) + \frac{1}{2}\nabla^3_\theta\mathcal{L}^{(1)}(\theta_0)(\theta_1 - \theta_0)^2 + \dots & \\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + \frac{\alpha^2}{2}\nabla^3_\theta\mathcal{L}^{(1)}(\theta_0) (g_0^{(0)})^2 + \dots & \scriptstyle{\text{; because }\theta_1-\theta_0=-\alpha g_0^{(0)}} \\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)
\end{aligned}
$$


 $$g_1^{(1)}$$
$$
\begin{aligned}
g_\text{FOMAML} &= g^{(1)}_1 = g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)\\
g_\text{MAML} &= g^{(1)}_1 - \alpha H^{(0)}_0 g^{(1)}_1 \\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2) - \alpha H^{(0)}_0 (g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2))\\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} - \alpha H^{(0)}_0 g_0^{(1)} + \alpha^2 \alpha H^{(0)}_0 H^{(1)}_0 g_0^{(0)} + O(\alpha^2)\\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} - \alpha H^{(0)}_0 g_0^{(1)} + O(\alpha^2)
\end{aligned}
$$

$$
\begin{aligned}
g_\text{Reptile} 
&= g^{(0)}_0 + g^{(1)}_1 \\
&= g^{(0)}_0 + g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)
\end{aligned}
$$

$$
\begin{aligned}
g_\text{FOMAML} &= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)\\
g_\text{MAML} &= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} - \alpha H^{(0)}_0 g_0^{(1)} + O(\alpha^2)\\
g_\text{Reptile}  &= g^{(0)}_0 + g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)
\end{aligned}
$$


 $$\mathbb{E}_{\tau,0,1}$$   $$\tau$$.

$$A = \mathbb{E}_{\tau,0,1} [g_0^{(0)}] = \mathbb{E}_{\tau,0,1} [g_0^{(1)}]$$;   $$A$$.

$$B = \mathbb{E}_{\tau,0,1} [H^{(1)}_0 g_0^{(0)}] = \frac{1}{2}\mathbb{E}_{\tau,0,1} [H^{(1)}_0 g_0^{(0)} + H^{(0)}_0 g_0^{(1)}] = \frac{1}{2}\mathbb{E}_{\tau,0,1} [\nabla_\theta(g^{(0)}_0 g_0^{(1)})]$$;  $$B$$.
$$
\begin{aligned}
\mathbb{E}_{\tau,1,2}[g_\text{FOMAML}] &= A - \alpha B + O(\alpha^2)\\
\mathbb{E}_{\tau,1,2}[g_\text{MAML}] &= A - 2\alpha B + O(\alpha^2)\\
\mathbb{E}_{\tau,1,2}[g_\text{Reptile}]  &= 2A - \alpha B + O(\alpha^2)
\end{aligned}
$$
 $$O(\alpha^2)$$ 